---------- Training loss 122.729 updated ! and save the model! (step:6) ----------
---------- Training loss 39.722 updated ! and save the model! (step:12) ----------
---------- Training loss 37.540 updated ! and save the model! (step:24) ----------
---------- Training loss 33.022 updated ! and save the model! (step:30) ----------
---------- Training loss 29.654 updated ! and save the model! (step:36) ----------
---------- Training loss 28.943 updated ! and save the model! (step:42) ----------
---------- Training loss 28.609 updated ! and save the model! (step:60) ----------
---------- Training loss 27.560 updated ! and save the model! (step:66) ----------
---------- Training loss 23.479 updated ! and save the model! (step:78) ----------
---------- Training loss 22.281 updated ! and save the model! (step:102) ----------
---------- Training loss 22.087 updated ! and save the model! (step:114) ----------
---------- Training loss 20.734 updated ! and save the model! (step:120) ----------
---------- Training loss 19.483 updated ! and save the model! (step:144) ----------
iteration : 200 loss : 28.747 NLL : -28.264 KLD : 0.483 
---------- Training loss 16.102 updated ! and save the model! (step:222) ----------
iteration : 400 loss : 25.002 NLL : -24.936 KLD : 0.065 
iteration : 600 loss : 23.525 NLL : -23.059 KLD : 0.467 
iteration : 800 loss : 25.020 NLL : -24.713 KLD : 0.306 
iteration : 1000 loss : 24.479 NLL : -23.672 KLD : 0.807 
iteration : 1200 loss : 22.564 NLL : -22.428 KLD : 0.135 
iteration : 1400 loss : 27.942 NLL : -27.742 KLD : 0.201 
iteration : 1600 loss : 16.992 NLL : -16.809 KLD : 0.183 
iteration : 1800 loss : 23.184 NLL : -23.149 KLD : 0.036 
iteration : 2000 loss : 22.124 NLL : -21.941 KLD : 0.182 
iteration : 2200 loss : 28.975 NLL : -28.396 KLD : 0.579 
iteration : 2400 loss : 25.294 NLL : -24.781 KLD : 0.513 
iteration : 2600 loss : 27.518 NLL : -27.434 KLD : 0.084 
iteration : 2800 loss : 27.733 NLL : -27.630 KLD : 0.103 
---------- Training loss 15.653 updated ! and save the model! (step:2898) ----------
iteration : 3000 loss : 23.000 NLL : -22.447 KLD : 0.553 
iteration : 3200 loss : 26.195 NLL : -26.068 KLD : 0.127 
iteration : 3400 loss : 19.607 NLL : -19.565 KLD : 0.042 
iteration : 3600 loss : 23.427 NLL : -23.257 KLD : 0.170 
iteration : 3800 loss : 27.596 NLL : -27.234 KLD : 0.362 
iteration : 4000 loss : 26.705 NLL : -26.671 KLD : 0.034 
iteration : 4200 loss : 21.916 NLL : -21.499 KLD : 0.418 
iteration : 4400 loss : 14.578 NLL : -14.470 KLD : 0.109 
iteration : 4600 loss : 25.739 NLL : -25.647 KLD : 0.093 
iteration : 4800 loss : 22.667 NLL : -22.616 KLD : 0.052 
iteration : 5000 loss : 20.906 NLL : -20.764 KLD : 0.143 
iteration : 5200 loss : 22.977 NLL : -22.525 KLD : 0.452 
iteration : 5400 loss : 20.732 NLL : -20.469 KLD : 0.263 
iteration : 5600 loss : 22.305 NLL : -22.245 KLD : 0.060 
---------- Training loss 14.964 updated ! and save the model! (step:5796) ----------
iteration : 5800 loss : 13.701 NLL : -13.568 KLD : 0.133 
iteration : 6000 loss : 17.934 NLL : -17.802 KLD : 0.131 
iteration : 6200 loss : 21.907 NLL : -21.882 KLD : 0.025 
iteration : 6400 loss : 23.052 NLL : -22.767 KLD : 0.285 
iteration : 6600 loss : 12.784 NLL : -12.114 KLD : 0.671 
iteration : 6800 loss : 11.872 NLL : -11.498 KLD : 0.374 
iteration : 7000 loss : 20.363 NLL : -20.274 KLD : 0.089 
iteration : 7200 loss : 18.932 NLL : -18.719 KLD : 0.213 
iteration : 7400 loss : 24.539 NLL : -24.248 KLD : 0.291 
iteration : 7600 loss : 24.031 NLL : -23.928 KLD : 0.103 
iteration : 7800 loss : 22.437 NLL : -22.272 KLD : 0.165 
iteration : 8000 loss : 16.575 NLL : -16.064 KLD : 0.511 
iteration : 8200 loss : 24.202 NLL : -24.015 KLD : 0.187 
iteration : 8400 loss : 16.377 NLL : -16.254 KLD : 0.123 
iteration : 8600 loss : 13.254 NLL : -13.118 KLD : 0.136 
iteration : 8800 loss : 14.525 NLL : -14.418 KLD : 0.107 
iteration : 9000 loss : 9.228 NLL : -9.032 KLD : 0.196 
iteration : 9200 loss : 19.058 NLL : -18.910 KLD : 0.148 
iteration : 9400 loss : 21.756 NLL : -21.572 KLD : 0.184 
iteration : 9600 loss : 24.676 NLL : -24.625 KLD : 0.051 
iteration : 9800 loss : 14.468 NLL : -14.368 KLD : 0.100 
iteration : 10000 loss : 13.723 NLL : -13.660 KLD : 0.063 
iteration : 10200 loss : 24.805 NLL : -24.209 KLD : 0.596 
iteration : 10400 loss : 10.042 NLL : -9.914 KLD : 0.128 
iteration : 10600 loss : 12.870 NLL : -12.426 KLD : 0.445 
---------- Training loss 14.883 updated ! and save the model! (step:10650) ----------
iteration : 10800 loss : 21.594 NLL : -21.517 KLD : 0.077 
iteration : 11000 loss : 15.485 NLL : -14.879 KLD : 0.606 
iteration : 11200 loss : 21.445 NLL : -21.003 KLD : 0.442 
iteration : 11400 loss : 21.451 NLL : -21.354 KLD : 0.097 
---------- Training loss 14.260 updated ! and save the model! (step:11526) ----------
iteration : 11600 loss : 18.750 NLL : -18.656 KLD : 0.094 
iteration : 11800 loss : 19.578 NLL : -19.402 KLD : 0.176 
iteration : 12000 loss : 23.602 NLL : -23.474 KLD : 0.128 
iteration : 12200 loss : 12.618 NLL : -12.290 KLD : 0.328 
iteration : 12400 loss : 22.640 NLL : -22.566 KLD : 0.074 
iteration : 12600 loss : 9.868 NLL : -9.379 KLD : 0.489 
iteration : 12800 loss : 12.590 NLL : -12.407 KLD : 0.184 
iteration : 13000 loss : 19.047 NLL : -19.004 KLD : 0.043 
iteration : 13200 loss : 22.314 NLL : -22.274 KLD : 0.039 
iteration : 13400 loss : 20.578 NLL : -20.543 KLD : 0.035 
iteration : 13600 loss : 17.023 NLL : -16.974 KLD : 0.049 
iteration : 13800 loss : 23.799 NLL : -23.360 KLD : 0.439 
iteration : 14000 loss : 23.134 NLL : -23.062 KLD : 0.072 
iteration : 14200 loss : 13.613 NLL : -13.463 KLD : 0.150 
iteration : 14400 loss : 14.984 NLL : -14.949 KLD : 0.035 
---------- Training loss 14.211 updated ! and save the model! (step:14418) ----------
iteration : 14600 loss : 19.391 NLL : -19.352 KLD : 0.039 
---------- Training loss 12.734 updated ! and save the model! (step:14652) ----------
iteration : 14800 loss : 17.963 NLL : -17.736 KLD : 0.227 
iteration : 15000 loss : 20.648 NLL : -20.586 KLD : 0.062 
iteration : 15200 loss : 20.939 NLL : -20.899 KLD : 0.040 
iteration : 15400 loss : 16.061 NLL : -15.960 KLD : 0.101 
iteration : 15600 loss : 11.679 NLL : -11.584 KLD : 0.095 
iteration : 15800 loss : 20.222 NLL : -20.173 KLD : 0.049 
iteration : 16000 loss : 14.445 NLL : -14.401 KLD : 0.043 
iteration : 16200 loss : 20.635 NLL : -20.301 KLD : 0.334 
iteration : 16400 loss : 18.927 NLL : -18.881 KLD : 0.046 
iteration : 16600 loss : 12.818 NLL : -12.635 KLD : 0.183 
iteration : 16800 loss : 15.478 NLL : -15.419 KLD : 0.060 
iteration : 17000 loss : 17.420 NLL : -17.325 KLD : 0.095 
iteration : 17200 loss : 11.735 NLL : -11.678 KLD : 0.057 
iteration : 17400 loss : 16.622 NLL : -16.551 KLD : 0.071 
iteration : 17600 loss : 21.673 NLL : -21.591 KLD : 0.082 
iteration : 17800 loss : 15.841 NLL : -15.640 KLD : 0.200 
iteration : 18000 loss : 20.771 NLL : -20.729 KLD : 0.042 
iteration : 18200 loss : 17.463 NLL : -17.433 KLD : 0.031 
iteration : 18400 loss : 17.791 NLL : -17.722 KLD : 0.068 
iteration : 18600 loss : 18.264 NLL : -18.152 KLD : 0.112 
iteration : 18800 loss : 14.953 NLL : -14.919 KLD : 0.034 
iteration : 19000 loss : 11.551 NLL : -11.498 KLD : 0.053 
iteration : 19200 loss : 16.118 NLL : -16.003 KLD : 0.116 
---------- Training loss 12.665 updated ! and save the model! (step:19344) ----------
iteration : 19400 loss : 11.715 NLL : -11.659 KLD : 0.056 
---------- Training loss 11.117 updated ! and save the model! (step:19470) ----------
iteration : 19600 loss : 19.864 NLL : -19.787 KLD : 0.077 
iteration : 19800 loss : 11.576 NLL : -11.302 KLD : 0.274 
iteration : 20000 loss : 11.438 NLL : -11.390 KLD : 0.048 
iteration : 20200 loss : 15.833 NLL : -15.700 KLD : 0.133 
iteration : 20400 loss : 9.032 NLL : -8.823 KLD : 0.209 
iteration : 20600 loss : 18.591 NLL : -18.436 KLD : 0.155 
iteration : 20800 loss : 17.629 NLL : -17.603 KLD : 0.026 
iteration : 21000 loss : 13.586 NLL : -13.524 KLD : 0.061 
iteration : 21200 loss : 12.841 NLL : -12.724 KLD : 0.117 
---------- Training loss 10.528 updated ! and save the model! (step:21210) ----------
iteration : 21400 loss : 19.172 NLL : -18.714 KLD : 0.458 
iteration : 21600 loss : 18.467 NLL : -18.162 KLD : 0.305 
iteration : 21800 loss : 17.569 NLL : -17.535 KLD : 0.034 
iteration : 22000 loss : 17.843 NLL : -17.773 KLD : 0.070 
iteration : 22200 loss : 15.880 NLL : -15.829 KLD : 0.051 
iteration : 22400 loss : 14.648 NLL : -14.592 KLD : 0.056 
iteration : 22600 loss : 11.393 NLL : -11.297 KLD : 0.097 
iteration : 22800 loss : 13.387 NLL : -13.305 KLD : 0.082 
iteration : 23000 loss : 11.559 NLL : -11.314 KLD : 0.245 
iteration : 23200 loss : 17.697 NLL : -17.582 KLD : 0.114 
iteration : 23400 loss : 9.883 NLL : -9.820 KLD : 0.062 
iteration : 23600 loss : 15.299 NLL : -15.273 KLD : 0.026 
iteration : 23800 loss : 22.317 NLL : -22.191 KLD : 0.126 
iteration : 24000 loss : 16.266 NLL : -16.229 KLD : 0.037 
iteration : 24200 loss : 9.691 NLL : -9.601 KLD : 0.090 
iteration : 24400 loss : 13.784 NLL : -13.326 KLD : 0.459 
iteration : 24600 loss : 17.464 NLL : -17.321 KLD : 0.143 
iteration : 24800 loss : 20.293 NLL : -19.975 KLD : 0.318 
iteration : 25000 loss : 16.979 NLL : -16.940 KLD : 0.038 
iteration : 25200 loss : 18.538 NLL : -18.501 KLD : 0.037 
iteration : 25400 loss : 19.738 NLL : -19.690 KLD : 0.048 
iteration : 25600 loss : 20.343 NLL : -20.067 KLD : 0.275 
iteration : 25800 loss : 18.413 NLL : -18.342 KLD : 0.071 
iteration : 26000 loss : 15.392 NLL : -15.268 KLD : 0.124 
iteration : 26200 loss : 16.735 NLL : -16.464 KLD : 0.271 
iteration : 26400 loss : 15.576 NLL : -15.551 KLD : 0.025 
iteration : 26600 loss : 16.632 NLL : -16.464 KLD : 0.167 
iteration : 26800 loss : 20.110 NLL : -19.711 KLD : 0.399 
iteration : 27000 loss : 12.861 NLL : -12.448 KLD : 0.413 
iteration : 27200 loss : 12.999 NLL : -12.970 KLD : 0.029 
iteration : 27400 loss : 23.449 NLL : -23.091 KLD : 0.358 
iteration : 27600 loss : 15.311 NLL : -15.256 KLD : 0.054 
iteration : 27800 loss : 16.644 NLL : -16.605 KLD : 0.039 
iteration : 28000 loss : 19.085 NLL : -18.417 KLD : 0.668 
iteration : 28200 loss : 15.976 NLL : -15.934 KLD : 0.041 
iteration : 28400 loss : 9.062 NLL : -8.821 KLD : 0.242 
iteration : 28600 loss : 16.076 NLL : -15.989 KLD : 0.086 
iteration : 28800 loss : 17.130 NLL : -17.092 KLD : 0.038 
iteration : 29000 loss : 13.391 NLL : -13.349 KLD : 0.042 
iteration : 29200 loss : 19.339 NLL : -19.302 KLD : 0.037 
iteration : 29400 loss : 17.075 NLL : -17.044 KLD : 0.032 
iteration : 29600 loss : 13.502 NLL : -13.447 KLD : 0.055 
iteration : 29800 loss : 14.974 NLL : -14.846 KLD : 0.128 
iteration : 30000 loss : 16.096 NLL : -15.813 KLD : 0.283 
---------- Training loss 13.822 updated ! and save the model! (step:30000) ----------
---------- Training loss 12.720 updated ! and save the model! (step:30006) ----------
---------- Training loss 11.449 updated ! and save the model! (step:30120) ----------
iteration : 30200 loss : 19.228 NLL : -19.049 KLD : 0.179 
iteration : 30400 loss : 17.648 NLL : -17.601 KLD : 0.047 
iteration : 30600 loss : 14.386 NLL : -14.356 KLD : 0.031 
iteration : 30800 loss : 16.617 NLL : -16.409 KLD : 0.208 
---------- Training loss 10.361 updated ! and save the model! (step:30930) ----------
iteration : 31000 loss : 11.177 NLL : -11.085 KLD : 0.092 
iteration : 31200 loss : 15.144 NLL : -15.107 KLD : 0.036 
iteration : 31400 loss : 18.145 NLL : -18.011 KLD : 0.134 
iteration : 31600 loss : 10.038 NLL : -9.851 KLD : 0.186 
iteration : 31800 loss : 6.307 NLL : -6.082 KLD : 0.225 
iteration : 32000 loss : 16.989 NLL : -16.853 KLD : 0.136 
iteration : 32200 loss : 19.862 NLL : -19.132 KLD : 0.731 
iteration : 32400 loss : 16.332 NLL : -16.257 KLD : 0.076 
iteration : 32600 loss : 17.446 NLL : -17.349 KLD : 0.097 
iteration : 32800 loss : 11.031 NLL : -10.768 KLD : 0.263 
iteration : 33000 loss : 15.021 NLL : -14.762 KLD : 0.259 
iteration : 33200 loss : 17.970 NLL : -17.848 KLD : 0.122 
iteration : 33400 loss : 8.424 NLL : -8.138 KLD : 0.285 
iteration : 33600 loss : 14.854 NLL : -14.557 KLD : 0.296 
iteration : 33800 loss : 16.400 NLL : -16.299 KLD : 0.101 
iteration : 34000 loss : 15.122 NLL : -14.934 KLD : 0.188 
iteration : 34200 loss : 15.876 NLL : -15.840 KLD : 0.035 
iteration : 34400 loss : 9.022 NLL : -8.579 KLD : 0.443 
iteration : 34600 loss : 15.369 NLL : -15.226 KLD : 0.143 
iteration : 34800 loss : 17.266 NLL : -16.964 KLD : 0.302 
---------- Training loss 10.291 updated ! and save the model! (step:34968) ----------
iteration : 35000 loss : 15.799 NLL : -15.737 KLD : 0.062 
iteration : 35200 loss : 11.898 NLL : -11.872 KLD : 0.026 
iteration : 35400 loss : 17.959 NLL : -17.803 KLD : 0.156 
iteration : 35600 loss : 15.190 NLL : -15.086 KLD : 0.104 
iteration : 35800 loss : 15.011 NLL : -14.959 KLD : 0.052 
iteration : 36000 loss : 13.210 NLL : -13.019 KLD : 0.192 
iteration : 36200 loss : 15.136 NLL : -15.066 KLD : 0.070 
---------- Training loss 9.949 updated ! and save the model! (step:36390) ----------
iteration : 36400 loss : 10.390 NLL : -9.998 KLD : 0.393 
iteration : 36600 loss : 13.399 NLL : -13.161 KLD : 0.239 
iteration : 36800 loss : 15.763 NLL : -15.735 KLD : 0.027 
iteration : 37000 loss : 12.708 NLL : -12.682 KLD : 0.026 
iteration : 37200 loss : 15.958 NLL : -15.931 KLD : 0.027 
iteration : 37400 loss : 9.891 NLL : -9.675 KLD : 0.217 
iteration : 37600 loss : 11.861 NLL : -11.507 KLD : 0.354 
iteration : 37800 loss : 16.858 NLL : -16.549 KLD : 0.310 
iteration : 38000 loss : 8.612 NLL : -8.298 KLD : 0.314 
iteration : 38200 loss : 14.465 NLL : -14.050 KLD : 0.416 
iteration : 38400 loss : 12.615 NLL : -12.550 KLD : 0.064 
iteration : 38600 loss : 11.498 NLL : -11.145 KLD : 0.353 
iteration : 38800 loss : 14.800 NLL : -14.728 KLD : 0.072 
iteration : 39000 loss : 16.625 NLL : -16.552 KLD : 0.073 
iteration : 39200 loss : 16.947 NLL : -16.839 KLD : 0.109 
iteration : 39400 loss : 17.701 NLL : -17.536 KLD : 0.165 
iteration : 39600 loss : 11.063 NLL : -10.899 KLD : 0.163 
iteration : 39800 loss : 16.479 NLL : -16.193 KLD : 0.286 
iteration : 40000 loss : 13.191 NLL : -13.034 KLD : 0.157 
iteration : 40200 loss : 15.035 NLL : -15.013 KLD : 0.022 
iteration : 40400 loss : 8.856 NLL : -8.714 KLD : 0.142 
iteration : 40600 loss : 16.389 NLL : -16.242 KLD : 0.147 
iteration : 40800 loss : 14.706 NLL : -14.323 KLD : 0.382 
iteration : 41000 loss : 9.217 NLL : -8.942 KLD : 0.275 
iteration : 41200 loss : 10.718 NLL : -10.648 KLD : 0.070 
iteration : 41400 loss : 14.829 NLL : -14.748 KLD : 0.081 
iteration : 41600 loss : 15.235 NLL : -15.180 KLD : 0.055 
iteration : 41800 loss : 16.158 NLL : -15.952 KLD : 0.206 
iteration : 42000 loss : 13.189 NLL : -13.166 KLD : 0.023 
iteration : 42200 loss : 9.133 NLL : -9.055 KLD : 0.078 
iteration : 42400 loss : 11.981 NLL : -11.709 KLD : 0.272 
iteration : 42600 loss : 11.167 NLL : -10.623 KLD : 0.544 
iteration : 42800 loss : 19.247 NLL : -18.993 KLD : 0.254 
iteration : 43000 loss : 12.550 NLL : -12.474 KLD : 0.077 
iteration : 43200 loss : 9.417 NLL : -9.303 KLD : 0.114 
iteration : 43400 loss : 14.975 NLL : -14.894 KLD : 0.081 
iteration : 43600 loss : 14.182 NLL : -13.701 KLD : 0.481 
iteration : 43800 loss : 16.918 NLL : -16.869 KLD : 0.049 
iteration : 44000 loss : 14.266 NLL : -14.156 KLD : 0.110 
iteration : 44200 loss : 21.115 NLL : -21.007 KLD : 0.108 
iteration : 44400 loss : 14.856 NLL : -14.673 KLD : 0.182 
iteration : 44600 loss : 17.810 NLL : -17.759 KLD : 0.052 
iteration : 44800 loss : 12.724 NLL : -12.661 KLD : 0.063 
iteration : 45000 loss : 11.569 NLL : -11.523 KLD : 0.046 
iteration : 45200 loss : 8.084 NLL : -7.976 KLD : 0.108 
iteration : 45400 loss : 15.541 NLL : -15.284 KLD : 0.257 
iteration : 45600 loss : 15.858 NLL : -15.724 KLD : 0.134 
iteration : 45800 loss : 14.865 NLL : -14.738 KLD : 0.126 
---------- Training loss 9.761 updated ! and save the model! (step:45804) ----------
iteration : 46000 loss : 16.216 NLL : -15.119 KLD : 1.097 
iteration : 46200 loss : 8.212 NLL : -8.039 KLD : 0.173 
iteration : 46400 loss : 9.126 NLL : -9.084 KLD : 0.042 
iteration : 46600 loss : 16.212 NLL : -16.169 KLD : 0.043 
iteration : 46800 loss : 14.574 NLL : -14.533 KLD : 0.041 
iteration : 47000 loss : 12.911 NLL : -12.887 KLD : 0.025 
iteration : 47200 loss : 14.112 NLL : -14.055 KLD : 0.056 
---------- Training loss 9.283 updated ! and save the model! (step:47376) ----------
iteration : 47400 loss : 15.633 NLL : -15.227 KLD : 0.405 
iteration : 47600 loss : 14.123 NLL : -14.065 KLD : 0.057 
iteration : 47800 loss : 9.570 NLL : -9.506 KLD : 0.064 
---------- Training loss 9.273 updated ! and save the model! (step:47886) ----------
iteration : 48000 loss : 8.244 NLL : -8.198 KLD : 0.046 
iteration : 48200 loss : 7.146 NLL : -7.004 KLD : 0.142 
iteration : 48400 loss : 15.600 NLL : -15.551 KLD : 0.049 
iteration : 48600 loss : 14.900 NLL : -14.770 KLD : 0.131 
iteration : 48800 loss : 15.089 NLL : -15.031 KLD : 0.058 
iteration : 49000 loss : 11.610 NLL : -11.562 KLD : 0.048 
iteration : 49200 loss : 6.960 NLL : -6.685 KLD : 0.275 
iteration : 49400 loss : 14.838 NLL : -14.691 KLD : 0.146 
---------- Training loss 9.259 updated ! and save the model! (step:49416) ----------
iteration : 49600 loss : 13.710 NLL : -13.659 KLD : 0.051 
iteration : 49800 loss : 14.005 NLL : -13.917 KLD : 0.087 
iteration : 50000 loss : 15.435 NLL : -15.380 KLD : 0.055 
iteration : 50200 loss : 15.204 NLL : -15.162 KLD : 0.042 
iteration : 50400 loss : 11.801 NLL : -11.500 KLD : 0.301 
iteration : 50600 loss : 14.627 NLL : -14.033 KLD : 0.594 
iteration : 50800 loss : 15.301 NLL : -15.246 KLD : 0.055 
iteration : 51000 loss : 11.547 NLL : -11.463 KLD : 0.084 
iteration : 51200 loss : 11.072 NLL : -11.022 KLD : 0.050 
iteration : 51400 loss : 7.619 NLL : -7.526 KLD : 0.092 
iteration : 51600 loss : 14.133 NLL : -14.108 KLD : 0.025 
iteration : 51800 loss : 14.737 NLL : -14.663 KLD : 0.073 
iteration : 52000 loss : 12.154 NLL : -11.797 KLD : 0.357 
iteration : 52200 loss : 15.404 NLL : -15.327 KLD : 0.077 
---------- Training loss 8.435 updated ! and save the model! (step:52242) ----------
iteration : 52400 loss : 12.238 NLL : -12.028 KLD : 0.210 
iteration : 52600 loss : 13.804 NLL : -13.737 KLD : 0.067 
iteration : 52800 loss : 12.898 NLL : -12.776 KLD : 0.122 
---------- Training loss 8.376 updated ! and save the model! (step:52830) ----------
iteration : 53000 loss : 5.811 NLL : -5.688 KLD : 0.123 
iteration : 53200 loss : 15.865 NLL : -15.678 KLD : 0.187 
iteration : 53400 loss : 9.195 NLL : -9.123 KLD : 0.072 
iteration : 53600 loss : 12.754 NLL : -12.684 KLD : 0.070 
iteration : 53800 loss : 12.802 NLL : -12.335 KLD : 0.467 
iteration : 54000 loss : 16.831 NLL : -16.781 KLD : 0.050 
iteration : 54200 loss : 14.720 NLL : -14.663 KLD : 0.057 
iteration : 54400 loss : 13.907 NLL : -13.832 KLD : 0.075 
iteration : 54600 loss : 12.568 NLL : -12.464 KLD : 0.104 
iteration : 54800 loss : 13.067 NLL : -13.043 KLD : 0.024 
iteration : 55000 loss : 7.951 NLL : -7.805 KLD : 0.146 
iteration : 55200 loss : 8.992 NLL : -8.843 KLD : 0.149 
iteration : 55400 loss : 13.653 NLL : -13.601 KLD : 0.052 
iteration : 55600 loss : 22.316 NLL : -21.171 KLD : 1.146 
iteration : 55800 loss : 18.894 NLL : -18.524 KLD : 0.370 
iteration : 56000 loss : 10.054 NLL : -9.852 KLD : 0.203 
iteration : 56200 loss : 12.773 NLL : -12.735 KLD : 0.037 
iteration : 56400 loss : 16.999 NLL : -16.927 KLD : 0.072 
iteration : 56600 loss : 13.388 NLL : -13.356 KLD : 0.031 
iteration : 56800 loss : 16.312 NLL : -15.695 KLD : 0.617 
iteration : 57000 loss : 13.559 NLL : -13.472 KLD : 0.087 
iteration : 57200 loss : 12.997 NLL : -12.936 KLD : 0.062 
iteration : 57400 loss : 12.865 NLL : -12.826 KLD : 0.039 
iteration : 57600 loss : 13.372 NLL : -13.315 KLD : 0.057 
iteration : 57800 loss : 12.450 NLL : -12.318 KLD : 0.133 
iteration : 58000 loss : 15.844 NLL : -15.565 KLD : 0.280 
iteration : 58200 loss : 13.366 NLL : -13.190 KLD : 0.176 
iteration : 58400 loss : 12.053 NLL : -12.020 KLD : 0.033 
iteration : 58600 loss : 18.643 NLL : -18.483 KLD : 0.161 
iteration : 58800 loss : 14.878 NLL : -14.836 KLD : 0.042 
iteration : 59000 loss : 12.907 NLL : -12.856 KLD : 0.050 
iteration : 59200 loss : 16.264 NLL : -16.171 KLD : 0.093 
iteration : 59400 loss : 16.142 NLL : -16.037 KLD : 0.105 
iteration : 59600 loss : 10.779 NLL : -10.727 KLD : 0.052 
iteration : 59800 loss : 11.554 NLL : -11.479 KLD : 0.075 
iteration : 60000 loss : 12.268 NLL : -12.233 KLD : 0.035 
---------- Training loss 11.909 updated ! and save the model! (step:60000) ----------
---------- Training loss 10.344 updated ! and save the model! (step:60024) ----------
---------- Training loss 10.255 updated ! and save the model! (step:60030) ----------
---------- Training loss 9.533 updated ! and save the model! (step:60096) ----------
iteration : 60200 loss : 14.202 NLL : -14.118 KLD : 0.084 
iteration : 60400 loss : 11.757 NLL : -11.709 KLD : 0.048 
iteration : 60600 loss : 15.484 NLL : -15.447 KLD : 0.038 
---------- Training loss 9.466 updated ! and save the model! (step:60762) ----------
iteration : 60800 loss : 11.471 NLL : -11.380 KLD : 0.091 
iteration : 61000 loss : 15.436 NLL : -15.300 KLD : 0.136 
iteration : 61200 loss : 14.521 NLL : -14.484 KLD : 0.036 
iteration : 61400 loss : 11.182 NLL : -11.029 KLD : 0.154 
iteration : 61600 loss : 15.688 NLL : -15.385 KLD : 0.303 
---------- Training loss 9.424 updated ! and save the model! (step:61668) ----------
iteration : 61800 loss : 12.802 NLL : -12.491 KLD : 0.310 
iteration : 62000 loss : 12.321 NLL : -12.049 KLD : 0.272 
iteration : 62200 loss : 27.018 NLL : -26.822 KLD : 0.197 
iteration : 62400 loss : 13.430 NLL : -13.394 KLD : 0.036 
iteration : 62600 loss : 11.530 NLL : -11.470 KLD : 0.060 
iteration : 62800 loss : 11.426 NLL : -11.237 KLD : 0.189 
iteration : 63000 loss : 13.777 NLL : -13.614 KLD : 0.163 
iteration : 63200 loss : 16.266 NLL : -16.193 KLD : 0.074 
---------- Training loss 8.991 updated ! and save the model! (step:63360) ----------
iteration : 63400 loss : 10.578 NLL : -10.544 KLD : 0.034 
iteration : 63600 loss : 8.064 NLL : -7.867 KLD : 0.197 
iteration : 63800 loss : 12.762 NLL : -12.616 KLD : 0.145 
iteration : 64000 loss : 9.406 NLL : -9.294 KLD : 0.112 
iteration : 64200 loss : 12.850 NLL : -12.452 KLD : 0.398 
iteration : 64400 loss : 13.356 NLL : -13.338 KLD : 0.018 
iteration : 64600 loss : 13.025 NLL : -12.993 KLD : 0.032 
iteration : 64800 loss : 9.714 NLL : -9.632 KLD : 0.083 
iteration : 65000 loss : 14.502 NLL : -14.230 KLD : 0.272 
iteration : 65200 loss : 15.061 NLL : -14.826 KLD : 0.236 
iteration : 65400 loss : 14.524 NLL : -14.455 KLD : 0.069 
iteration : 65600 loss : 16.884 NLL : -16.198 KLD : 0.686 
iteration : 65800 loss : 13.452 NLL : -13.343 KLD : 0.109 
iteration : 66000 loss : 13.815 NLL : -13.643 KLD : 0.172 
iteration : 66200 loss : 13.341 NLL : -13.219 KLD : 0.122 
iteration : 66400 loss : 6.102 NLL : -5.937 KLD : 0.165 
iteration : 66600 loss : 7.289 NLL : -6.974 KLD : 0.314 
iteration : 66800 loss : 11.096 NLL : -11.025 KLD : 0.071 
iteration : 67000 loss : 13.518 NLL : -13.417 KLD : 0.101 
iteration : 67200 loss : 7.132 NLL : -6.598 KLD : 0.534 
iteration : 67400 loss : 13.486 NLL : -13.215 KLD : 0.271 
iteration : 67600 loss : 12.032 NLL : -12.001 KLD : 0.031 
iteration : 67800 loss : 11.349 NLL : -11.331 KLD : 0.019 
iteration : 68000 loss : 15.995 NLL : -15.910 KLD : 0.085 
iteration : 68200 loss : 12.550 NLL : -12.385 KLD : 0.165 
iteration : 68400 loss : 12.361 NLL : -12.297 KLD : 0.064 
iteration : 68600 loss : 13.514 NLL : -13.435 KLD : 0.079 
---------- Training loss 8.669 updated ! and save the model! (step:68730) ----------
iteration : 68800 loss : 8.779 NLL : -8.444 KLD : 0.335 
iteration : 69000 loss : 9.443 NLL : -9.357 KLD : 0.085 
iteration : 69200 loss : 13.025 NLL : -12.835 KLD : 0.190 
iteration : 69400 loss : 12.911 NLL : -12.852 KLD : 0.059 
iteration : 69600 loss : 10.647 NLL : -10.552 KLD : 0.096 
iteration : 69800 loss : 14.818 NLL : -14.782 KLD : 0.035 
iteration : 70000 loss : 5.763 NLL : -5.536 KLD : 0.227 
iteration : 70200 loss : 8.217 NLL : -7.954 KLD : 0.264 
iteration : 70400 loss : 15.135 NLL : -14.963 KLD : 0.171 
iteration : 70600 loss : 4.006 NLL : -3.779 KLD : 0.227 
iteration : 70800 loss : 14.342 NLL : -14.263 KLD : 0.079 
iteration : 71000 loss : 14.052 NLL : -13.253 KLD : 0.799 
iteration : 71200 loss : 13.755 NLL : -13.490 KLD : 0.265 
iteration : 71400 loss : 13.310 NLL : -13.059 KLD : 0.251 
iteration : 71600 loss : 12.941 NLL : -12.897 KLD : 0.044 
iteration : 71800 loss : 9.113 NLL : -8.777 KLD : 0.336 
iteration : 72000 loss : 14.580 NLL : -14.166 KLD : 0.414 
iteration : 72200 loss : 12.692 NLL : -12.659 KLD : 0.033 
iteration : 72400 loss : 8.728 NLL : -8.614 KLD : 0.114 
iteration : 72600 loss : 14.666 NLL : -14.572 KLD : 0.093 
iteration : 72800 loss : 14.378 NLL : -14.191 KLD : 0.187 
iteration : 73000 loss : 12.495 NLL : -12.448 KLD : 0.046 
iteration : 73200 loss : 8.501 NLL : -7.992 KLD : 0.509 
iteration : 73400 loss : 11.839 NLL : -11.499 KLD : 0.340 
iteration : 73600 loss : 11.131 NLL : -10.837 KLD : 0.294 
iteration : 73800 loss : 9.459 NLL : -9.040 KLD : 0.418 
iteration : 74000 loss : 16.452 NLL : -16.319 KLD : 0.133 
iteration : 74200 loss : 10.036 NLL : -9.861 KLD : 0.175 
iteration : 74400 loss : 16.183 NLL : -16.055 KLD : 0.128 
iteration : 74600 loss : 6.939 NLL : -6.825 KLD : 0.114 
iteration : 74800 loss : 14.372 NLL : -14.293 KLD : 0.079 
iteration : 75000 loss : 11.427 NLL : -11.385 KLD : 0.042 
iteration : 75200 loss : 14.179 NLL : -14.134 KLD : 0.045 
---------- Training loss 8.226 updated ! and save the model! (step:75264) ----------
iteration : 75400 loss : 13.026 NLL : -12.957 KLD : 0.069 
iteration : 75600 loss : 13.678 NLL : -13.609 KLD : 0.068 
iteration : 75800 loss : 11.162 NLL : -10.794 KLD : 0.368 
iteration : 76000 loss : 12.631 NLL : -12.454 KLD : 0.177 
iteration : 76200 loss : 12.689 NLL : -12.494 KLD : 0.195 
iteration : 76400 loss : 8.976 NLL : -8.872 KLD : 0.104 
iteration : 76600 loss : 12.929 NLL : -12.318 KLD : 0.611 
iteration : 76800 loss : 13.684 NLL : -13.235 KLD : 0.449 
iteration : 77000 loss : 12.992 NLL : -12.930 KLD : 0.062 
iteration : 77200 loss : 6.725 NLL : -6.642 KLD : 0.084 
iteration : 77400 loss : 12.172 NLL : -12.023 KLD : 0.150 
iteration : 77600 loss : 13.806 NLL : -13.544 KLD : 0.263 
iteration : 77800 loss : 11.284 NLL : -10.693 KLD : 0.591 
iteration : 78000 loss : 10.651 NLL : -10.502 KLD : 0.148 
iteration : 78200 loss : 13.378 NLL : -13.332 KLD : 0.046 
iteration : 78400 loss : 7.392 NLL : -7.344 KLD : 0.048 
iteration : 78600 loss : 11.111 NLL : -11.052 KLD : 0.058 
iteration : 78800 loss : 11.647 NLL : -11.580 KLD : 0.067 
iteration : 79000 loss : 12.281 NLL : -12.159 KLD : 0.122 
iteration : 79200 loss : 12.791 NLL : -12.756 KLD : 0.035 
iteration : 79400 loss : 10.270 NLL : -10.099 KLD : 0.171 
iteration : 79600 loss : 11.183 NLL : -11.149 KLD : 0.033 
iteration : 79800 loss : 15.895 NLL : -15.570 KLD : 0.325 
iteration : 80000 loss : 6.388 NLL : -6.034 KLD : 0.354 
iteration : 80200 loss : 12.385 NLL : -12.326 KLD : 0.059 
iteration : 80400 loss : 12.252 NLL : -12.155 KLD : 0.098 
iteration : 80600 loss : 7.795 NLL : -7.742 KLD : 0.053 
iteration : 80800 loss : 16.580 NLL : -15.680 KLD : 0.900 
iteration : 81000 loss : 11.502 NLL : -11.212 KLD : 0.290 
iteration : 81200 loss : 12.676 NLL : -12.509 KLD : 0.167 
---------- Training loss 8.011 updated ! and save the model! (step:81378) ----------
iteration : 81400 loss : 8.699 NLL : -8.677 KLD : 0.022 
iteration : 81600 loss : 11.041 NLL : -10.990 KLD : 0.050 
iteration : 81800 loss : 14.417 NLL : -14.174 KLD : 0.243 
iteration : 82000 loss : 13.688 NLL : -13.489 KLD : 0.199 
iteration : 82200 loss : 10.879 NLL : -10.818 KLD : 0.061 
iteration : 82400 loss : 13.866 NLL : -13.828 KLD : 0.037 
iteration : 82600 loss : 12.719 NLL : -12.608 KLD : 0.111 
iteration : 82800 loss : 11.763 NLL : -11.677 KLD : 0.086 
iteration : 83000 loss : 11.038 NLL : -10.980 KLD : 0.058 
iteration : 83200 loss : 14.628 NLL : -14.368 KLD : 0.260 
iteration : 83400 loss : 21.587 NLL : -21.248 KLD : 0.338 
iteration : 83600 loss : 17.075 NLL : -16.912 KLD : 0.162 
iteration : 83800 loss : 10.376 NLL : -10.231 KLD : 0.145 
iteration : 84000 loss : 8.270 NLL : -7.953 KLD : 0.318 
iteration : 84200 loss : 14.049 NLL : -13.844 KLD : 0.205 
iteration : 84400 loss : 12.618 NLL : -12.556 KLD : 0.062 
iteration : 84600 loss : 12.084 NLL : -11.944 KLD : 0.139 
iteration : 84800 loss : 9.640 NLL : -9.398 KLD : 0.242 
iteration : 85000 loss : 11.195 NLL : -11.144 KLD : 0.051 
iteration : 85200 loss : 10.348 NLL : -10.327 KLD : 0.021 
iteration : 85400 loss : 10.450 NLL : -10.412 KLD : 0.039 
iteration : 85600 loss : 5.521 NLL : -5.237 KLD : 0.284 
iteration : 85800 loss : 10.992 NLL : -10.780 KLD : 0.212 
iteration : 86000 loss : 9.540 NLL : -9.484 KLD : 0.056 
iteration : 86200 loss : 10.246 NLL : -10.118 KLD : 0.128 
iteration : 86400 loss : 13.370 NLL : -13.288 KLD : 0.081 
iteration : 86600 loss : 7.621 NLL : -7.363 KLD : 0.258 
iteration : 86800 loss : 10.585 NLL : -10.479 KLD : 0.106 
iteration : 87000 loss : 11.243 NLL : -11.207 KLD : 0.036 
iteration : 87200 loss : 13.007 NLL : -12.968 KLD : 0.039 
iteration : 87400 loss : 16.294 NLL : -15.867 KLD : 0.427 
iteration : 87600 loss : 12.537 NLL : -12.491 KLD : 0.046 
iteration : 87800 loss : 15.096 NLL : -15.063 KLD : 0.033 
iteration : 88000 loss : 6.781 NLL : -6.490 KLD : 0.291 
iteration : 88200 loss : 10.860 NLL : -10.837 KLD : 0.023 
iteration : 88400 loss : 13.790 NLL : -13.415 KLD : 0.375 
iteration : 88600 loss : 12.120 NLL : -12.079 KLD : 0.041 
iteration : 88800 loss : 12.995 NLL : -12.922 KLD : 0.072 
iteration : 89000 loss : 11.322 NLL : -11.236 KLD : 0.086 
iteration : 89200 loss : 11.519 NLL : -11.449 KLD : 0.069 
iteration : 89400 loss : 12.393 NLL : -12.360 KLD : 0.034 
iteration : 89600 loss : 11.309 NLL : -11.260 KLD : 0.049 
iteration : 89800 loss : 10.205 NLL : -10.178 KLD : 0.028 
iteration : 90000 loss : 11.959 NLL : -11.885 KLD : 0.074 
---------- Training loss 11.281 updated ! and save the model! (step:90000) ----------
---------- Training loss 10.644 updated ! and save the model! (step:90006) ----------
---------- Training loss 10.375 updated ! and save the model! (step:90036) ----------
---------- Training loss 10.242 updated ! and save the model! (step:90048) ----------
---------- Training loss 9.473 updated ! and save the model! (step:90138) ----------
iteration : 90200 loss : 10.475 NLL : -10.313 KLD : 0.163 
---------- Training loss 8.589 updated ! and save the model! (step:90258) ----------
iteration : 90400 loss : 8.573 NLL : -8.437 KLD : 0.136 
---------- Training loss 8.012 updated ! and save the model! (step:90558) ----------
iteration : 90600 loss : 13.505 NLL : -13.426 KLD : 0.079 
iteration : 90800 loss : 16.321 NLL : -15.993 KLD : 0.328 
iteration : 91000 loss : 14.527 NLL : -14.458 KLD : 0.068 
iteration : 91200 loss : 13.161 NLL : -13.137 KLD : 0.024 
---------- Training loss 7.068 updated ! and save the model! (step:91314) ----------
iteration : 91400 loss : 12.392 NLL : -12.338 KLD : 0.054 
iteration : 91600 loss : 9.175 NLL : -9.054 KLD : 0.122 
iteration : 91800 loss : 8.139 NLL : -7.993 KLD : 0.146 
iteration : 92000 loss : 12.596 NLL : -12.561 KLD : 0.035 
iteration : 92200 loss : 13.263 NLL : -13.189 KLD : 0.074 
iteration : 92400 loss : 10.698 NLL : -10.636 KLD : 0.063 
iteration : 92600 loss : 13.793 NLL : -13.717 KLD : 0.076 
iteration : 92800 loss : 7.249 NLL : -7.132 KLD : 0.116 
iteration : 93000 loss : 12.102 NLL : -12.024 KLD : 0.078 
iteration : 93200 loss : 9.654 NLL : -9.624 KLD : 0.030 
iteration : 93400 loss : 11.779 NLL : -11.693 KLD : 0.085 
iteration : 93600 loss : 11.386 NLL : -11.332 KLD : 0.054 
iteration : 93800 loss : 16.169 NLL : -15.704 KLD : 0.465 
iteration : 94000 loss : 4.655 NLL : -4.219 KLD : 0.437 
iteration : 94200 loss : 12.979 NLL : -12.884 KLD : 0.095 
iteration : 94400 loss : 13.791 NLL : -13.669 KLD : 0.122 
iteration : 94600 loss : 11.573 NLL : -11.148 KLD : 0.425 
iteration : 94800 loss : 9.101 NLL : -9.034 KLD : 0.068 
iteration : 95000 loss : 9.923 NLL : -9.900 KLD : 0.023 
iteration : 95200 loss : 12.835 NLL : -12.669 KLD : 0.166 
iteration : 95400 loss : 4.606 NLL : -4.314 KLD : 0.292 
iteration : 95600 loss : 13.524 NLL : -13.466 KLD : 0.058 
iteration : 95800 loss : 10.975 NLL : -10.933 KLD : 0.042 
iteration : 96000 loss : 10.436 NLL : -10.063 KLD : 0.373 
iteration : 96200 loss : 12.492 NLL : -12.416 KLD : 0.076 
iteration : 96400 loss : 13.612 NLL : -13.509 KLD : 0.103 
iteration : 96600 loss : 9.401 NLL : -9.168 KLD : 0.233 
iteration : 96800 loss : 10.033 NLL : -9.950 KLD : 0.083 
iteration : 97000 loss : 14.285 NLL : -13.609 KLD : 0.676 
iteration : 97200 loss : 15.155 NLL : -13.897 KLD : 1.258 
iteration : 97400 loss : 16.131 NLL : -16.093 KLD : 0.038 
iteration : 97600 loss : 13.450 NLL : -13.272 KLD : 0.177 
iteration : 97800 loss : 9.496 NLL : -9.364 KLD : 0.132 
iteration : 98000 loss : 9.238 NLL : -8.918 KLD : 0.320 
iteration : 98200 loss : 7.967 NLL : -7.635 KLD : 0.332 
iteration : 98400 loss : 10.156 NLL : -10.120 KLD : 0.036 
iteration : 98600 loss : 9.953 NLL : -9.540 KLD : 0.413 
iteration : 98800 loss : 11.721 NLL : -11.703 KLD : 0.018 
iteration : 99000 loss : 14.674 NLL : -14.577 KLD : 0.096 
iteration : 99200 loss : 10.929 NLL : -10.726 KLD : 0.203 
iteration : 99400 loss : 11.819 NLL : -11.772 KLD : 0.048 
iteration : 99600 loss : 12.963 NLL : -12.896 KLD : 0.067 
iteration : 99800 loss : 10.968 NLL : -10.620 KLD : 0.348 
iteration : 100000 loss : 10.736 NLL : -10.690 KLD : 0.045 
iteration : 100200 loss : 12.121 NLL : -12.047 KLD : 0.074 
iteration : 100400 loss : 11.644 NLL : -11.562 KLD : 0.082 
iteration : 100600 loss : 8.465 NLL : -8.427 KLD : 0.039 
iteration : 100800 loss : 12.777 NLL : -12.735 KLD : 0.042 
iteration : 101000 loss : 20.566 NLL : -20.425 KLD : 0.141 
iteration : 101200 loss : 20.917 NLL : -20.841 KLD : 0.076 
iteration : 101400 loss : 9.781 NLL : -9.201 KLD : 0.580 
iteration : 101600 loss : 15.166 NLL : -14.994 KLD : 0.173 
iteration : 101800 loss : 11.884 NLL : -11.449 KLD : 0.435 
iteration : 102000 loss : 11.031 NLL : -10.877 KLD : 0.154 
iteration : 102200 loss : 14.301 NLL : -14.266 KLD : 0.035 
iteration : 102400 loss : 9.635 NLL : -9.458 KLD : 0.177 
iteration : 102600 loss : 11.765 NLL : -11.718 KLD : 0.047 
iteration : 102800 loss : 9.498 NLL : -9.234 KLD : 0.264 
iteration : 103000 loss : 14.213 NLL : -13.877 KLD : 0.336 
iteration : 103200 loss : 5.393 NLL : -5.350 KLD : 0.043 
iteration : 103400 loss : 13.063 NLL : -12.871 KLD : 0.192 
iteration : 103600 loss : 9.255 NLL : -9.167 KLD : 0.088 
iteration : 103800 loss : 9.639 NLL : -9.491 KLD : 0.148 
iteration : 104000 loss : 8.361 NLL : -8.269 KLD : 0.092 
iteration : 104200 loss : 13.839 NLL : -13.755 KLD : 0.084 
iteration : 104400 loss : 10.649 NLL : -10.067 KLD : 0.582 
iteration : 104600 loss : 9.733 NLL : -9.668 KLD : 0.065 
iteration : 104800 loss : 11.693 NLL : -11.668 KLD : 0.026 
iteration : 105000 loss : 12.088 NLL : -12.003 KLD : 0.085 
iteration : 105200 loss : 10.412 NLL : -10.270 KLD : 0.143 
iteration : 105400 loss : 11.186 NLL : -11.112 KLD : 0.074 
iteration : 105600 loss : 10.511 NLL : -10.449 KLD : 0.062 
iteration : 105800 loss : 12.614 NLL : -12.431 KLD : 0.183 
iteration : 106000 loss : 6.392 NLL : -6.000 KLD : 0.391 
iteration : 106200 loss : 8.621 NLL : -8.584 KLD : 0.037 
iteration : 106400 loss : 11.031 NLL : -10.989 KLD : 0.042 
iteration : 106600 loss : 11.187 NLL : -10.742 KLD : 0.445 
iteration : 106800 loss : 11.022 NLL : -10.982 KLD : 0.040 
iteration : 107000 loss : 12.461 NLL : -12.392 KLD : 0.069 
iteration : 107200 loss : 8.803 NLL : -8.763 KLD : 0.040 
iteration : 107400 loss : 8.435 NLL : -8.342 KLD : 0.093 
iteration : 107600 loss : 7.678 NLL : -7.486 KLD : 0.193 
iteration : 107800 loss : 16.817 NLL : -16.630 KLD : 0.187 
iteration : 108000 loss : 9.917 NLL : -9.470 KLD : 0.447 
iteration : 108200 loss : 10.702 NLL : -10.448 KLD : 0.254 
iteration : 108400 loss : 13.519 NLL : -13.383 KLD : 0.135 
iteration : 108600 loss : 12.516 NLL : -12.179 KLD : 0.337 
iteration : 108800 loss : 12.434 NLL : -12.357 KLD : 0.077 
iteration : 109000 loss : 13.507 NLL : -13.445 KLD : 0.062 
iteration : 109200 loss : 12.400 NLL : -11.985 KLD : 0.414 
iteration : 109400 loss : 12.635 NLL : -12.545 KLD : 0.090 
iteration : 109600 loss : 11.285 NLL : -10.601 KLD : 0.684 
iteration : 109800 loss : 14.073 NLL : -13.980 KLD : 0.092 
iteration : 110000 loss : 14.356 NLL : -14.062 KLD : 0.294 
iteration : 110200 loss : 10.099 NLL : -10.045 KLD : 0.054 
iteration : 110400 loss : 12.535 NLL : -12.441 KLD : 0.094 
iteration : 110600 loss : 13.490 NLL : -13.408 KLD : 0.081 
iteration : 110800 loss : 11.046 NLL : -10.994 KLD : 0.053 
iteration : 111000 loss : 12.278 NLL : -12.121 KLD : 0.157 
iteration : 111200 loss : 11.483 NLL : -11.416 KLD : 0.067 
iteration : 111400 loss : 11.573 NLL : -11.275 KLD : 0.298 
iteration : 111600 loss : 12.276 NLL : -12.227 KLD : 0.049 
iteration : 111800 loss : 14.327 NLL : -14.196 KLD : 0.130 
iteration : 112000 loss : 12.858 NLL : -12.716 KLD : 0.142 
iteration : 112200 loss : 10.407 NLL : -10.360 KLD : 0.047 
iteration : 112400 loss : 11.194 NLL : -10.842 KLD : 0.352 
iteration : 112600 loss : 12.089 NLL : -12.061 KLD : 0.028 
iteration : 112800 loss : 11.579 NLL : -11.314 KLD : 0.265 
iteration : 113000 loss : 11.678 NLL : -11.628 KLD : 0.050 
iteration : 113200 loss : 9.857 NLL : -9.686 KLD : 0.171 
iteration : 113400 loss : 9.505 NLL : -9.367 KLD : 0.137 
iteration : 113600 loss : 8.047 NLL : -7.653 KLD : 0.395 
iteration : 113800 loss : 13.421 NLL : -13.288 KLD : 0.133 
iteration : 114000 loss : 12.052 NLL : -11.970 KLD : 0.082 
iteration : 114200 loss : 8.277 NLL : -8.195 KLD : 0.082 
iteration : 114400 loss : 10.877 NLL : -10.825 KLD : 0.053 
iteration : 114600 loss : 7.214 NLL : -7.088 KLD : 0.126 
iteration : 114800 loss : 6.153 NLL : -6.099 KLD : 0.054 
iteration : 115000 loss : 11.576 NLL : -11.544 KLD : 0.032 
iteration : 115200 loss : 6.378 NLL : -6.333 KLD : 0.045 
iteration : 115400 loss : 10.786 NLL : -10.417 KLD : 0.370 
iteration : 115600 loss : 9.587 NLL : -9.541 KLD : 0.046 
iteration : 115800 loss : 10.769 NLL : -10.724 KLD : 0.045 
iteration : 116000 loss : 9.085 NLL : -8.999 KLD : 0.086 
iteration : 116200 loss : 12.140 NLL : -12.045 KLD : 0.095 
iteration : 116400 loss : 11.457 NLL : -11.299 KLD : 0.158 
iteration : 116600 loss : 6.884 NLL : -6.739 KLD : 0.146 
iteration : 116800 loss : 12.846 NLL : -12.249 KLD : 0.598 
iteration : 117000 loss : 12.095 NLL : -11.985 KLD : 0.110 
iteration : 117200 loss : 11.425 NLL : -11.411 KLD : 0.014 
iteration : 117400 loss : 11.223 NLL : -11.073 KLD : 0.150 
iteration : 117600 loss : 8.403 NLL : -8.057 KLD : 0.346 
iteration : 117800 loss : 9.294 NLL : -9.184 KLD : 0.111 
iteration : 118000 loss : 13.348 NLL : -13.205 KLD : 0.143 
iteration : 118200 loss : 14.255 NLL : -14.090 KLD : 0.164 
iteration : 118400 loss : 14.030 NLL : -13.925 KLD : 0.105 
iteration : 118600 loss : 8.823 NLL : -8.760 KLD : 0.063 
iteration : 118800 loss : 5.646 NLL : -5.337 KLD : 0.309 
iteration : 119000 loss : 10.858 NLL : -10.659 KLD : 0.199 
iteration : 119200 loss : 9.885 NLL : -9.843 KLD : 0.043 
iteration : 119400 loss : 10.609 NLL : -10.385 KLD : 0.224 
iteration : 119600 loss : 9.406 NLL : -9.218 KLD : 0.189 
iteration : 119800 loss : 11.106 NLL : -10.947 KLD : 0.159 
iteration : 120000 loss : 10.835 NLL : -10.795 KLD : 0.041 
---------- Training loss 10.957 updated ! and save the model! (step:120000) ----------
---------- Training loss 10.613 updated ! and save the model! (step:120012) ----------
---------- Training loss 9.412 updated ! and save the model! (step:120048) ----------
---------- Training loss 8.691 updated ! and save the model! (step:120132) ----------
iteration : 120200 loss : 15.532 NLL : -14.836 KLD : 0.697 
---------- Training loss 8.489 updated ! and save the model! (step:120366) ----------
iteration : 120400 loss : 10.347 NLL : -10.297 KLD : 0.050 
iteration : 120600 loss : 10.646 NLL : -10.580 KLD : 0.066 
---------- Training loss 8.377 updated ! and save the model! (step:120654) ----------
---------- Training loss 8.003 updated ! and save the model! (step:120744) ----------
iteration : 120800 loss : 7.643 NLL : -7.598 KLD : 0.045 
---------- Training loss 7.746 updated ! and save the model! (step:120852) ----------
---------- Training loss 7.374 updated ! and save the model! (step:120870) ----------
iteration : 121000 loss : 12.793 NLL : -12.696 KLD : 0.097 
iteration : 121200 loss : 11.923 NLL : -11.815 KLD : 0.108 
iteration : 121400 loss : 8.451 NLL : -8.309 KLD : 0.142 
iteration : 121600 loss : 5.918 NLL : -5.559 KLD : 0.358 
iteration : 121800 loss : 12.733 NLL : -12.650 KLD : 0.083 
iteration : 122000 loss : 10.781 NLL : -10.726 KLD : 0.055 
iteration : 122200 loss : 15.210 NLL : -15.138 KLD : 0.072 
iteration : 122400 loss : 6.819 NLL : -6.478 KLD : 0.341 
iteration : 122600 loss : 11.286 NLL : -11.027 KLD : 0.259 
iteration : 122800 loss : 12.365 NLL : -12.174 KLD : 0.191 
iteration : 123000 loss : 15.218 NLL : -15.172 KLD : 0.046 
iteration : 123200 loss : 16.504 NLL : -16.334 KLD : 0.170 
iteration : 123400 loss : 5.462 NLL : -5.285 KLD : 0.177 
iteration : 123600 loss : 12.468 NLL : -12.391 KLD : 0.077 
iteration : 123800 loss : 9.573 NLL : -9.407 KLD : 0.165 
iteration : 124000 loss : 6.248 NLL : -6.129 KLD : 0.120 
iteration : 124200 loss : 8.602 NLL : -8.481 KLD : 0.122 
iteration : 124400 loss : 12.210 NLL : -12.156 KLD : 0.055 
iteration : 124600 loss : 4.223 NLL : -3.902 KLD : 0.321 
iteration : 124800 loss : 7.184 NLL : -7.000 KLD : 0.184 
iteration : 125000 loss : 10.943 NLL : -10.911 KLD : 0.032 
iteration : 125200 loss : 8.570 NLL : -8.372 KLD : 0.197 
---------- Training loss 6.887 updated ! and save the model! (step:125280) ----------
iteration : 125400 loss : 12.116 NLL : -11.955 KLD : 0.161 
iteration : 125600 loss : 5.700 NLL : -5.498 KLD : 0.202 
iteration : 125800 loss : 12.874 NLL : -12.232 KLD : 0.642 
iteration : 126000 loss : 10.281 NLL : -10.208 KLD : 0.073 
iteration : 126200 loss : 12.948 NLL : -12.774 KLD : 0.174 
iteration : 126400 loss : 14.066 NLL : -13.649 KLD : 0.417 
iteration : 126600 loss : 16.969 NLL : -15.873 KLD : 1.096 
iteration : 126800 loss : 10.126 NLL : -10.093 KLD : 0.033 
iteration : 127000 loss : 10.533 NLL : -10.330 KLD : 0.203 
iteration : 127200 loss : 10.117 NLL : -10.070 KLD : 0.047 
iteration : 127400 loss : 8.565 NLL : -8.546 KLD : 0.018 
iteration : 127600 loss : 9.928 NLL : -9.845 KLD : 0.084 
iteration : 127800 loss : 13.997 NLL : -13.913 KLD : 0.084 
iteration : 128000 loss : 14.502 NLL : -14.451 KLD : 0.051 
iteration : 128200 loss : 13.705 NLL : -13.666 KLD : 0.039 
iteration : 128400 loss : 12.734 NLL : -12.712 KLD : 0.021 
iteration : 128600 loss : 7.066 NLL : -6.587 KLD : 0.478 
iteration : 128800 loss : 11.005 NLL : -10.945 KLD : 0.060 
iteration : 129000 loss : 12.873 NLL : -12.513 KLD : 0.360 
iteration : 129200 loss : 10.043 NLL : -9.836 KLD : 0.207 
iteration : 129400 loss : 12.298 NLL : -11.861 KLD : 0.437 
iteration : 129600 loss : 6.663 NLL : -6.602 KLD : 0.061 
iteration : 129800 loss : 7.520 NLL : -7.423 KLD : 0.096 
iteration : 130000 loss : 7.853 NLL : -7.598 KLD : 0.255 
iteration : 130200 loss : 12.185 NLL : -12.137 KLD : 0.049 
iteration : 130400 loss : 8.679 NLL : -8.289 KLD : 0.391 
iteration : 130600 loss : 11.834 NLL : -11.740 KLD : 0.094 
iteration : 130800 loss : 10.576 NLL : -10.342 KLD : 0.234 
iteration : 131000 loss : 13.387 NLL : -13.295 KLD : 0.092 
iteration : 131200 loss : 7.867 NLL : -7.645 KLD : 0.222 
iteration : 131400 loss : 12.975 NLL : -12.847 KLD : 0.128 
---------- Training loss 6.858 updated ! and save the model! (step:131490) ----------
iteration : 131600 loss : 14.503 NLL : -14.454 KLD : 0.050 
iteration : 131800 loss : 11.798 NLL : -11.712 KLD : 0.086 
iteration : 132000 loss : 5.481 NLL : -5.392 KLD : 0.089 
iteration : 132200 loss : 10.535 NLL : -10.435 KLD : 0.100 
iteration : 132400 loss : 11.133 NLL : -10.781 KLD : 0.352 
iteration : 132600 loss : 7.875 NLL : -7.802 KLD : 0.074 
iteration : 132800 loss : 6.292 NLL : -5.903 KLD : 0.390 
iteration : 133000 loss : 8.614 NLL : -8.538 KLD : 0.076 
iteration : 133200 loss : 9.207 NLL : -9.121 KLD : 0.086 
iteration : 133400 loss : 9.869 NLL : -9.828 KLD : 0.041 
iteration : 133600 loss : 13.643 NLL : -12.385 KLD : 1.258 
iteration : 133800 loss : 15.314 NLL : -15.220 KLD : 0.095 
iteration : 134000 loss : 9.293 NLL : -9.222 KLD : 0.072 
iteration : 134200 loss : 11.113 NLL : -11.088 KLD : 0.025 
iteration : 134400 loss : 7.092 NLL : -6.807 KLD : 0.285 
iteration : 134600 loss : 5.299 NLL : -5.098 KLD : 0.200 
iteration : 134800 loss : 10.874 NLL : -10.582 KLD : 0.291 
iteration : 135000 loss : 14.156 NLL : -13.806 KLD : 0.350 
iteration : 135200 loss : 11.450 NLL : -11.197 KLD : 0.253 
iteration : 135400 loss : 15.073 NLL : -14.834 KLD : 0.239 
iteration : 135600 loss : 5.146 NLL : -4.913 KLD : 0.233 
iteration : 135800 loss : 5.815 NLL : -5.350 KLD : 0.465 
iteration : 136000 loss : 9.567 NLL : -9.439 KLD : 0.128 
iteration : 136200 loss : 10.943 NLL : -10.496 KLD : 0.447 
iteration : 136400 loss : 9.496 NLL : -9.441 KLD : 0.055 
iteration : 136600 loss : 7.486 NLL : -7.383 KLD : 0.103 
iteration : 136800 loss : 12.202 NLL : -11.697 KLD : 0.504 
iteration : 137000 loss : 12.943 NLL : -12.826 KLD : 0.117 
iteration : 137200 loss : 9.918 NLL : -9.761 KLD : 0.157 
iteration : 137400 loss : 12.615 NLL : -12.495 KLD : 0.120 
iteration : 137600 loss : 10.819 NLL : -10.782 KLD : 0.037 
iteration : 137800 loss : 4.493 NLL : -4.080 KLD : 0.413 
iteration : 138000 loss : 10.400 NLL : -10.369 KLD : 0.031 
iteration : 138200 loss : 9.762 NLL : -9.575 KLD : 0.187 
iteration : 138400 loss : 7.596 NLL : -7.524 KLD : 0.072 
iteration : 138600 loss : 10.475 NLL : -10.368 KLD : 0.108 
iteration : 138800 loss : 11.511 NLL : -11.342 KLD : 0.169 
iteration : 139000 loss : 9.641 NLL : -9.594 KLD : 0.047 
iteration : 139200 loss : 12.988 NLL : -12.915 KLD : 0.072 
iteration : 139400 loss : 15.427 NLL : -15.291 KLD : 0.136 
iteration : 139600 loss : 9.605 NLL : -9.541 KLD : 0.064 
iteration : 139800 loss : 11.162 NLL : -11.131 KLD : 0.031 
iteration : 140000 loss : 5.628 NLL : -5.574 KLD : 0.054 
iteration : 140200 loss : 8.040 NLL : -7.946 KLD : 0.093 
iteration : 140400 loss : 9.766 NLL : -9.515 KLD : 0.250 
iteration : 140600 loss : 12.856 NLL : -12.836 KLD : 0.020 
iteration : 140800 loss : 10.897 NLL : -10.781 KLD : 0.117 
iteration : 141000 loss : 13.867 NLL : -13.711 KLD : 0.156 
iteration : 141200 loss : 7.450 NLL : -7.387 KLD : 0.063 
iteration : 141400 loss : 9.520 NLL : -9.048 KLD : 0.472 
iteration : 141600 loss : 8.534 NLL : -8.416 KLD : 0.118 
iteration : 141800 loss : 7.466 NLL : -7.399 KLD : 0.067 
iteration : 142000 loss : 5.955 NLL : -5.801 KLD : 0.154 
iteration : 142200 loss : 10.344 NLL : -10.274 KLD : 0.070 
iteration : 142400 loss : 3.412 NLL : -2.992 KLD : 0.420 
iteration : 142600 loss : 10.613 NLL : -10.467 KLD : 0.145 
iteration : 142800 loss : 10.949 NLL : -10.890 KLD : 0.059 
iteration : 143000 loss : 8.989 NLL : -8.738 KLD : 0.250 
iteration : 143200 loss : 12.446 NLL : -12.162 KLD : 0.284 
iteration : 143400 loss : 10.467 NLL : -9.998 KLD : 0.469 
iteration : 143600 loss : 5.534 NLL : -5.194 KLD : 0.339 
iteration : 143800 loss : 12.656 NLL : -12.514 KLD : 0.142 
iteration : 144000 loss : 11.616 NLL : -11.532 KLD : 0.084 
iteration : 144200 loss : 10.564 NLL : -10.531 KLD : 0.033 
iteration : 144400 loss : 14.101 NLL : -13.731 KLD : 0.370 
iteration : 144600 loss : 9.525 NLL : -9.485 KLD : 0.040 
iteration : 144800 loss : 7.662 NLL : -7.396 KLD : 0.266 
iteration : 145000 loss : 10.734 NLL : -10.504 KLD : 0.230 
iteration : 145200 loss : 8.876 NLL : -8.814 KLD : 0.061 
---------- Training loss 6.656 updated ! and save the model! (step:145338) ----------
iteration : 145400 loss : 5.324 NLL : -4.802 KLD : 0.522 
iteration : 145600 loss : 29.736 NLL : -28.317 KLD : 1.419 
iteration : 145800 loss : 15.957 NLL : -15.848 KLD : 0.109 
iteration : 146000 loss : 7.760 NLL : -6.959 KLD : 0.801 
iteration : 146200 loss : 8.463 NLL : -8.015 KLD : 0.449 
iteration : 146400 loss : 12.845 NLL : -12.774 KLD : 0.071 
iteration : 146600 loss : 11.242 NLL : -11.152 KLD : 0.090 
iteration : 146800 loss : 8.515 NLL : -8.442 KLD : 0.073 
iteration : 147000 loss : 7.926 NLL : -7.732 KLD : 0.194 
iteration : 147200 loss : 10.068 NLL : -10.013 KLD : 0.055 
iteration : 147400 loss : 6.592 NLL : -6.154 KLD : 0.437 
iteration : 147600 loss : 11.206 NLL : -11.157 KLD : 0.050 
iteration : 147800 loss : 8.851 NLL : -8.710 KLD : 0.141 
iteration : 148000 loss : 11.538 NLL : -11.347 KLD : 0.191 
iteration : 148200 loss : 9.729 NLL : -9.425 KLD : 0.305 
iteration : 148400 loss : 13.355 NLL : -13.302 KLD : 0.052 
iteration : 148600 loss : 10.335 NLL : -10.269 KLD : 0.067 
iteration : 148800 loss : 12.278 NLL : -12.222 KLD : 0.057 
iteration : 149000 loss : 4.530 NLL : -4.360 KLD : 0.170 
iteration : 149200 loss : 10.983 NLL : -10.490 KLD : 0.493 
iteration : 149400 loss : 6.794 NLL : -6.589 KLD : 0.205 
iteration : 149600 loss : 13.808 NLL : -13.383 KLD : 0.426 
iteration : 149800 loss : 12.284 NLL : -12.248 KLD : 0.036 
iteration : 150000 loss : 6.061 NLL : -5.988 KLD : 0.072 
---------- Training loss 9.252 updated ! and save the model! (step:150000) ----------
---------- Training loss 7.898 updated ! and save the model! (step:150030) ----------
iteration : 150200 loss : 10.817 NLL : -10.437 KLD : 0.380 
iteration : 150400 loss : 11.272 NLL : -11.238 KLD : 0.034 
iteration : 150600 loss : 6.084 NLL : -5.816 KLD : 0.268 
iteration : 150800 loss : 8.053 NLL : -7.997 KLD : 0.056 
---------- Training loss 7.545 updated ! and save the model! (step:150858) ----------
---------- Training loss 7.246 updated ! and save the model! (step:150960) ----------
iteration : 151000 loss : 11.461 NLL : -11.316 KLD : 0.146 
---------- Training loss 6.878 updated ! and save the model! (step:151194) ----------
iteration : 151200 loss : 7.117 NLL : -7.074 KLD : 0.043 
iteration : 151400 loss : 8.921 NLL : -8.813 KLD : 0.109 
iteration : 151600 loss : 12.310 NLL : -12.238 KLD : 0.072 
iteration : 151800 loss : 10.552 NLL : -10.499 KLD : 0.053 
iteration : 152000 loss : 6.084 NLL : -5.802 KLD : 0.283 
iteration : 152200 loss : 16.138 NLL : -15.695 KLD : 0.443 
iteration : 152400 loss : 9.541 NLL : -9.501 KLD : 0.040 
iteration : 152600 loss : 9.799 NLL : -9.245 KLD : 0.554 
iteration : 152800 loss : 9.801 NLL : -9.778 KLD : 0.024 
iteration : 153000 loss : 9.207 NLL : -9.161 KLD : 0.046 
iteration : 153200 loss : 9.842 NLL : -9.802 KLD : 0.040 
iteration : 153400 loss : 4.318 NLL : -3.974 KLD : 0.344 
iteration : 153600 loss : 9.582 NLL : -9.516 KLD : 0.066 
iteration : 153800 loss : 11.972 NLL : -11.951 KLD : 0.022 
iteration : 154000 loss : 7.357 NLL : -7.250 KLD : 0.107 
iteration : 154200 loss : 6.902 NLL : -6.757 KLD : 0.144 
iteration : 154400 loss : 6.821 NLL : -6.546 KLD : 0.275 
iteration : 154600 loss : 11.599 NLL : -11.277 KLD : 0.322 
iteration : 154800 loss : 9.341 NLL : -8.530 KLD : 0.811 
iteration : 155000 loss : 7.890 NLL : -7.775 KLD : 0.115 
iteration : 155200 loss : 7.811 NLL : -7.683 KLD : 0.127 
iteration : 155400 loss : 6.774 NLL : -6.393 KLD : 0.382 
iteration : 155600 loss : 13.313 NLL : -13.227 KLD : 0.086 
iteration : 155800 loss : 11.408 NLL : -11.120 KLD : 0.289 
iteration : 156000 loss : 12.974 NLL : -12.902 KLD : 0.072 
iteration : 156200 loss : 7.186 NLL : -7.144 KLD : 0.042 
iteration : 156400 loss : 5.583 NLL : -5.333 KLD : 0.249 
iteration : 156600 loss : 7.121 NLL : -6.978 KLD : 0.142 
iteration : 156800 loss : 12.331 NLL : -12.092 KLD : 0.238 
iteration : 157000 loss : 15.416 NLL : -14.937 KLD : 0.480 
iteration : 157200 loss : 12.740 NLL : -12.597 KLD : 0.143 
iteration : 157400 loss : 11.849 NLL : -11.791 KLD : 0.058 
---------- Training loss 6.506 updated ! and save the model! (step:157452) ----------
iteration : 157600 loss : 12.964 NLL : -12.856 KLD : 0.108 
iteration : 157800 loss : 6.879 NLL : -6.290 KLD : 0.590 
iteration : 158000 loss : 5.342 NLL : -5.074 KLD : 0.269 
iteration : 158200 loss : 9.853 NLL : -9.740 KLD : 0.113 
iteration : 158400 loss : 10.604 NLL : -10.574 KLD : 0.029 
iteration : 158600 loss : 13.315 NLL : -13.252 KLD : 0.063 
iteration : 158800 loss : 9.041 NLL : -9.002 KLD : 0.039 
iteration : 159000 loss : 10.540 NLL : -10.428 KLD : 0.112 
iteration : 159200 loss : 8.138 NLL : -7.881 KLD : 0.257 
iteration : 159400 loss : 9.270 NLL : -9.131 KLD : 0.139 
iteration : 159600 loss : 8.935 NLL : -8.762 KLD : 0.173 
iteration : 159800 loss : 5.414 NLL : -4.921 KLD : 0.492 
iteration : 160000 loss : 10.225 NLL : -10.108 KLD : 0.116 
iteration : 160200 loss : 11.025 NLL : -10.781 KLD : 0.244 
iteration : 160400 loss : 12.216 NLL : -12.191 KLD : 0.025 
iteration : 160600 loss : 10.894 NLL : -10.824 KLD : 0.070 
iteration : 160800 loss : 6.188 NLL : -6.036 KLD : 0.152 
iteration : 161000 loss : 6.035 NLL : -5.920 KLD : 0.115 
iteration : 161200 loss : 9.990 NLL : -9.914 KLD : 0.076 
iteration : 161400 loss : 6.336 NLL : -5.870 KLD : 0.466 
---------- Training loss 6.388 updated ! and save the model! (step:161460) ----------
iteration : 161600 loss : 7.303 NLL : -7.250 KLD : 0.053 
iteration : 161800 loss : 7.243 NLL : -7.132 KLD : 0.111 
iteration : 162000 loss : 17.479 NLL : -17.117 KLD : 0.363 
iteration : 162200 loss : 7.409 NLL : -6.949 KLD : 0.460 
iteration : 162400 loss : 9.392 NLL : -9.250 KLD : 0.142 
iteration : 162600 loss : 7.239 NLL : -7.192 KLD : 0.047 
iteration : 162800 loss : 5.785 NLL : -5.683 KLD : 0.101 
iteration : 163000 loss : 5.296 NLL : -5.191 KLD : 0.105 
iteration : 163200 loss : 11.544 NLL : -11.471 KLD : 0.073 
iteration : 163400 loss : 8.131 NLL : -8.066 KLD : 0.065 
iteration : 163600 loss : 12.485 NLL : -12.144 KLD : 0.340 
iteration : 163800 loss : 4.919 NLL : -4.770 KLD : 0.149 
iteration : 164000 loss : 9.099 NLL : -9.034 KLD : 0.064 
iteration : 164200 loss : 7.712 NLL : -7.636 KLD : 0.075 
iteration : 164400 loss : 9.398 NLL : -9.314 KLD : 0.084 
iteration : 164600 loss : 12.122 NLL : -11.802 KLD : 0.320 
iteration : 164800 loss : 9.038 NLL : -8.791 KLD : 0.247 
iteration : 165000 loss : 8.446 NLL : -8.148 KLD : 0.299 
iteration : 165200 loss : 12.643 NLL : -12.575 KLD : 0.068 
iteration : 165400 loss : 9.838 NLL : -9.786 KLD : 0.051 
iteration : 165600 loss : 9.102 NLL : -9.076 KLD : 0.026 
iteration : 165800 loss : 8.897 NLL : -8.819 KLD : 0.078 
iteration : 166000 loss : 7.993 NLL : -7.843 KLD : 0.150 
iteration : 166200 loss : 11.819 NLL : -11.510 KLD : 0.309 
iteration : 166400 loss : 10.225 NLL : -10.162 KLD : 0.063 
iteration : 166600 loss : 12.918 NLL : -12.627 KLD : 0.291 
iteration : 166800 loss : 10.470 NLL : -10.045 KLD : 0.425 
iteration : 167000 loss : 9.933 NLL : -9.769 KLD : 0.164 
iteration : 167200 loss : 12.029 NLL : -11.996 KLD : 0.033 
iteration : 167400 loss : 9.804 NLL : -9.607 KLD : 0.198 
iteration : 167600 loss : 11.085 NLL : -10.924 KLD : 0.160 
iteration : 167800 loss : 9.971 NLL : -9.835 KLD : 0.136 
iteration : 168000 loss : 12.230 NLL : -12.115 KLD : 0.115 
iteration : 168200 loss : 11.492 NLL : -11.471 KLD : 0.021 
iteration : 168400 loss : 14.066 NLL : -13.915 KLD : 0.151 
iteration : 168600 loss : 6.969 NLL : -6.826 KLD : 0.143 
iteration : 168800 loss : 12.637 NLL : -12.456 KLD : 0.180 
iteration : 169000 loss : 5.298 NLL : -5.034 KLD : 0.264 
iteration : 169200 loss : 8.667 NLL : -8.597 KLD : 0.070 
iteration : 169400 loss : 9.666 NLL : -9.598 KLD : 0.069 
iteration : 169600 loss : 11.778 NLL : -11.395 KLD : 0.384 
iteration : 169800 loss : 13.792 NLL : -13.513 KLD : 0.278 
iteration : 170000 loss : 10.051 NLL : -9.903 KLD : 0.148 
iteration : 170200 loss : 11.972 NLL : -11.844 KLD : 0.128 
iteration : 170400 loss : 6.976 NLL : -6.919 KLD : 0.057 
iteration : 170600 loss : 11.940 NLL : -11.602 KLD : 0.338 
iteration : 170800 loss : 9.008 NLL : -8.969 KLD : 0.039 
iteration : 171000 loss : 4.185 NLL : -3.830 KLD : 0.355 
iteration : 171200 loss : 8.198 NLL : -8.178 KLD : 0.020 
iteration : 171400 loss : 9.820 NLL : -9.582 KLD : 0.238 
iteration : 171600 loss : 11.601 NLL : -11.504 KLD : 0.096 
iteration : 171800 loss : 8.873 NLL : -8.832 KLD : 0.042 
iteration : 172000 loss : 6.447 NLL : -6.367 KLD : 0.079 
iteration : 172200 loss : 11.106 NLL : -10.831 KLD : 0.275 
iteration : 172400 loss : 10.548 NLL : -10.476 KLD : 0.072 
iteration : 172600 loss : 7.483 NLL : -7.439 KLD : 0.044 
iteration : 172800 loss : 6.453 NLL : -6.229 KLD : 0.223 
iteration : 173000 loss : 9.564 NLL : -9.335 KLD : 0.229 
iteration : 173200 loss : 9.038 NLL : -8.971 KLD : 0.067 
iteration : 173400 loss : 8.142 NLL : -8.127 KLD : 0.015 
iteration : 173600 loss : 8.590 NLL : -8.500 KLD : 0.091 
iteration : 173800 loss : 6.987 NLL : -6.693 KLD : 0.294 
iteration : 174000 loss : 8.226 NLL : -7.926 KLD : 0.300 
iteration : 174200 loss : 24.245 NLL : -24.068 KLD : 0.176 
iteration : 174400 loss : 11.728 NLL : -11.187 KLD : 0.541 
iteration : 174600 loss : 8.796 NLL : -8.497 KLD : 0.299 
iteration : 174800 loss : 17.168 NLL : -17.026 KLD : 0.142 
iteration : 175000 loss : 10.067 NLL : -9.762 KLD : 0.305 
iteration : 175200 loss : 10.331 NLL : -10.103 KLD : 0.227 /home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])

iteration : 175400 loss : 11.913 NLL : -11.774 KLD : 0.139 
iteration : 175600 loss : 11.589 NLL : -10.805 KLD : 0.784 
iteration : 175800 loss : 13.589 NLL : -13.494 KLD : 0.095 
iteration : 176000 loss : 6.597 NLL : -6.168 KLD : 0.429 
iteration : 176200 loss : 5.244 NLL : -5.104 KLD : 0.140 
iteration : 176400 loss : 13.341 NLL : -12.857 KLD : 0.484 
iteration : 176600 loss : 13.918 NLL : -13.498 KLD : 0.421 
iteration : 176800 loss : 12.100 NLL : -12.053 KLD : 0.047 
iteration : 177000 loss : 13.204 NLL : -13.140 KLD : 0.065 
iteration : 177200 loss : 9.204 NLL : -8.931 KLD : 0.273 
iteration : 177400 loss : 12.156 NLL : -12.079 KLD : 0.077 
iteration : 177600 loss : 8.207 NLL : -8.133 KLD : 0.074 
iteration : 177800 loss : 14.148 NLL : -14.046 KLD : 0.102 
iteration : 178000 loss : 10.161 NLL : -10.030 KLD : 0.131 
iteration : 178200 loss : 12.690 NLL : -12.651 KLD : 0.039 
iteration : 178400 loss : 10.376 NLL : -10.330 KLD : 0.045 
iteration : 178600 loss : 10.165 NLL : -9.910 KLD : 0.255 
iteration : 178800 loss : 9.725 NLL : -9.683 KLD : 0.042 
iteration : 179000 loss : 10.147 NLL : -10.105 KLD : 0.042 
iteration : 179200 loss : 8.410 NLL : -8.350 KLD : 0.059 
iteration : 179400 loss : 9.516 NLL : -8.618 KLD : 0.899 
iteration : 179600 loss : 12.709 NLL : -12.567 KLD : 0.141 
iteration : 179800 loss : 9.827 NLL : -9.709 KLD : 0.118 
iteration : 180000 loss : 11.105 NLL : -11.019 KLD : 0.086 
---------- Training loss 9.320 updated ! and save the model! (step:180000) ----------
---------- Training loss 9.269 updated ! and save the model! (step:180018) ----------
---------- Training loss 8.470 updated ! and save the model! (step:180054) ----------
---------- Training loss 7.648 updated ! and save the model! (step:180072) ----------
iteration : 180200 loss : 11.288 NLL : -10.972 KLD : 0.316 
iteration : 180400 loss : 10.974 NLL : -10.935 KLD : 0.039 
iteration : 180600 loss : 9.831 NLL : -9.257 KLD : 0.574 
iteration : 180800 loss : 12.588 NLL : -12.230 KLD : 0.358 
iteration : 181000 loss : 8.512 NLL : -8.388 KLD : 0.124 
iteration : 181200 loss : 11.939 NLL : -11.769 KLD : 0.169 
iteration : 181400 loss : 10.366 NLL : -10.249 KLD : 0.118 
iteration : 181600 loss : 6.875 NLL : -6.778 KLD : 0.098 
iteration : 181800 loss : 11.083 NLL : -11.001 KLD : 0.082 
iteration : 182000 loss : 7.292 NLL : -6.883 KLD : 0.409 
iteration : 182200 loss : 13.387 NLL : -13.032 KLD : 0.355 
iteration : 182400 loss : 8.539 NLL : -8.088 KLD : 0.450 
iteration : 182600 loss : 6.479 NLL : -5.893 KLD : 0.585 
iteration : 182800 loss : 12.648 NLL : -12.060 KLD : 0.588 
iteration : 183000 loss : 11.959 NLL : -11.911 KLD : 0.048 
---------- Training loss 7.601 updated ! and save the model! (step:183048) ----------
iteration : 183200 loss : 3.150 NLL : -2.615 KLD : 0.534 
iteration : 183400 loss : 9.932 NLL : -9.249 KLD : 0.683 
iteration : 183600 loss : 13.626 NLL : -13.546 KLD : 0.080 
iteration : 183800 loss : 11.200 NLL : -11.161 KLD : 0.039 
iteration : 184000 loss : 4.740 NLL : -4.504 KLD : 0.237 
---------- Training loss 7.261 updated ! and save the model! (step:184002) ----------
iteration : 184200 loss : 6.247 NLL : -6.202 KLD : 0.046 
iteration : 184400 loss : 7.989 NLL : -7.902 KLD : 0.087 
iteration : 184600 loss : 9.641 NLL : -9.551 KLD : 0.090 
iteration : 184800 loss : 9.689 NLL : -9.432 KLD : 0.257 
iteration : 185000 loss : 4.523 NLL : -4.169 KLD : 0.354 
iteration : 185200 loss : 10.838 NLL : -10.810 KLD : 0.027 
iteration : 185400 loss : 7.943 NLL : -7.854 KLD : 0.090 
---------- Training loss 6.863 updated ! and save the model! (step:185430) ----------
iteration : 185600 loss : 4.478 NLL : -3.848 KLD : 0.630 
iteration : 185800 loss : 5.780 NLL : -5.087 KLD : 0.693 
iteration : 186000 loss : 8.648 NLL : -8.621 KLD : 0.027 
iteration : 186200 loss : 11.167 NLL : -11.134 KLD : 0.033 
iteration : 186400 loss : 10.935 NLL : -10.886 KLD : 0.049 
iteration : 186600 loss : 6.361 NLL : -6.299 KLD : 0.063 
iteration : 186800 loss : 5.586 NLL : -5.510 KLD : 0.075 
iteration : 187000 loss : 8.593 NLL : -8.546 KLD : 0.047 
iteration : 187200 loss : 16.812 NLL : -16.367 KLD : 0.445 
iteration : 187400 loss : 10.753 NLL : -10.668 KLD : 0.085 
iteration : 187600 loss : 9.646 NLL : -9.555 KLD : 0.091 
iteration : 187800 loss : 11.936 NLL : -11.888 KLD : 0.048 
iteration : 188000 loss : 10.866 NLL : -10.742 KLD : 0.124 
iteration : 188200 loss : 8.152 NLL : -8.102 KLD : 0.050 
iteration : 188400 loss : 13.088 NLL : -12.576 KLD : 0.513 
iteration : 188600 loss : 13.765 NLL : -13.266 KLD : 0.499 
iteration : 188800 loss : 13.602 NLL : -13.510 KLD : 0.092 
iteration : 189000 loss : 11.130 NLL : -10.905 KLD : 0.225 
iteration : 189200 loss : 13.228 NLL : -13.084 KLD : 0.145 
iteration : 189400 loss : 8.710 NLL : -8.505 KLD : 0.205 
iteration : 189600 loss : 6.162 NLL : -6.096 KLD : 0.066 
iteration : 189800 loss : 11.658 NLL : -11.425 KLD : 0.233 
iteration : 190000 loss : 10.384 NLL : -9.605 KLD : 0.779 
iteration : 190200 loss : 9.328 NLL : -9.255 KLD : 0.073 
iteration : 190400 loss : 11.191 NLL : -11.110 KLD : 0.081 
iteration : 190600 loss : 13.470 NLL : -12.953 KLD : 0.517 
iteration : 190800 loss : 14.295 NLL : -13.500 KLD : 0.795 
iteration : 191000 loss : 9.953 NLL : -9.747 KLD : 0.206 
iteration : 191200 loss : 9.894 NLL : -9.826 KLD : 0.068 
iteration : 191400 loss : 11.327 NLL : -10.879 KLD : 0.447 
iteration : 191600 loss : 12.311 NLL : -11.948 KLD : 0.363 
iteration : 191800 loss : 9.765 NLL : -9.647 KLD : 0.118 
iteration : 192000 loss : 11.266 NLL : -11.173 KLD : 0.093 
iteration : 192200 loss : 11.399 NLL : -11.120 KLD : 0.279 
iteration : 192400 loss : 10.535 NLL : -10.256 KLD : 0.279 
iteration : 192600 loss : 6.876 NLL : -6.803 KLD : 0.073 
iteration : 192800 loss : 5.912 NLL : -5.749 KLD : 0.163 
iteration : 193000 loss : 4.577 NLL : -4.403 KLD : 0.174 
iteration : 193200 loss : 5.438 NLL : -5.336 KLD : 0.101 
iteration : 193400 loss : 13.339 NLL : -13.006 KLD : 0.333 
iteration : 193600 loss : 10.716 NLL : -10.620 KLD : 0.096 
iteration : 193800 loss : 12.135 NLL : -12.052 KLD : 0.083 
iteration : 194000 loss : 6.035 NLL : -5.733 KLD : 0.302 
iteration : 194200 loss : 10.505 NLL : -10.199 KLD : 0.306 
iteration : 194400 loss : 22.155 NLL : -21.939 KLD : 0.217 
iteration : 194600 loss : 9.265 NLL : -8.909 KLD : 0.357 
iteration : 194800 loss : 12.616 NLL : -12.542 KLD : 0.074 
iteration : 195000 loss : 7.182 NLL : -7.145 KLD : 0.037 
iteration : 195200 loss : 11.645 NLL : -11.590 KLD : 0.056 
iteration : 195400 loss : 5.131 NLL : -5.038 KLD : 0.093 
iteration : 195600 loss : 9.273 NLL : -9.255 KLD : 0.018 
iteration : 195800 loss : 7.417 NLL : -7.352 KLD : 0.065 
iteration : 196000 loss : 12.888 NLL : -12.798 KLD : 0.090 
iteration : 196200 loss : 12.220 NLL : -11.978 KLD : 0.242 
iteration : 196400 loss : 13.449 NLL : -13.354 KLD : 0.095 
iteration : 196600 loss : 6.061 NLL : -5.892 KLD : 0.169 
iteration : 196800 loss : 9.226 NLL : -9.065 KLD : 0.161 
iteration : 197000 loss : 8.334 NLL : -8.289 KLD : 0.045 
iteration : 197200 loss : 10.153 NLL : -9.986 KLD : 0.168 
iteration : 197400 loss : 9.682 NLL : -8.941 KLD : 0.741 
iteration : 197600 loss : 12.610 NLL : -12.388 KLD : 0.221 
iteration : 197800 loss : 11.842 NLL : -11.750 KLD : 0.092 
iteration : 198000 loss : 6.316 NLL : -5.984 KLD : 0.332 
iteration : 198200 loss : 11.800 NLL : -11.379 KLD : 0.422 
iteration : 198400 loss : 5.967 NLL : -5.871 KLD : 0.096 
iteration : 198600 loss : 8.769 NLL : -8.652 KLD : 0.118 
iteration : 198800 loss : 6.499 NLL : -6.197 KLD : 0.302 
iteration : 199000 loss : 12.392 NLL : -12.339 KLD : 0.054 
iteration : 199200 loss : 4.833 NLL : -4.654 KLD : 0.180 
iteration : 199400 loss : 8.831 NLL : -8.759 KLD : 0.072 
iteration : 199600 loss : 6.643 NLL : -6.476 KLD : 0.168 
iteration : 199800 loss : 10.022 NLL : -9.953 KLD : 0.069 
iteration : 200000 loss : 11.323 NLL : -10.976 KLD : 0.347 
---------- Save the model! (step:None) ----------
