---------- Training loss 105.663 updated ! and save the model! (step:6) ----------
---------- Training loss 51.531 updated ! and save the model! (step:12) ----------
---------- Training loss 46.179 updated ! and save the model! (step:18) ----------
---------- Training loss 45.282 updated ! and save the model! (step:24) ----------
---------- Training loss 38.839 updated ! and save the model! (step:30) ----------
---------- Training loss 32.507 updated ! and save the model! (step:36) ----------
---------- Training loss 32.310 updated ! and save the model! (step:42) ----------
---------- Training loss 29.993 updated ! and save the model! (step:48) ----------
---------- Training loss 25.770 updated ! and save the model! (step:54) ----------
---------- Training loss 25.631 updated ! and save the model! (step:132) ----------
iteration : 200 loss : 27.656 NLL : -20.133 KLD : 0.004 KLD_attention : 7.519 
iteration : 400 loss : 26.703 NLL : -18.882 KLD : 0.001 KLD_attention : 7.820 
---------- Training loss 24.343 updated ! and save the model! (step:486) ----------
iteration : 600 loss : 28.252 NLL : -20.127 KLD : 0.000 KLD_attention : 8.125 
---------- Training loss 24.078 updated ! and save the model! (step:726) ----------
iteration : 800 loss : 25.936 NLL : -17.440 KLD : 0.000 KLD_attention : 8.496 
iteration : 1000 loss : 35.892 NLL : -27.884 KLD : 0.000 KLD_attention : 8.008 
---------- Training loss 22.560 updated ! and save the model! (step:1098) ----------
iteration : 1200 loss : 24.963 NLL : -16.864 KLD : 0.000 KLD_attention : 8.098 
---------- Training loss 21.495 updated ! and save the model! (step:1386) ----------
iteration : 1400 loss : 29.968 NLL : -21.626 KLD : 0.000 KLD_attention : 8.343 
iteration : 1600 loss : 31.961 NLL : -23.900 KLD : 0.000 KLD_attention : 8.061 
iteration : 1800 loss : 30.190 NLL : -22.114 KLD : 0.000 KLD_attention : 8.075 
iteration : 2000 loss : 23.605 NLL : -15.376 KLD : 0.000 KLD_attention : 8.228 
iteration : 2200 loss : 25.483 NLL : -16.943 KLD : 0.001 KLD_attention : 8.539 
---------- Training loss 20.909 updated ! and save the model! (step:2262) ----------
iteration : 2400 loss : 30.172 NLL : -21.878 KLD : 0.000 KLD_attention : 8.294 
iteration : 2600 loss : 20.795 NLL : -12.523 KLD : 0.000 KLD_attention : 8.271 
iteration : 2800 loss : 25.178 NLL : -16.689 KLD : 0.000 KLD_attention : 8.489 
---------- Training loss 19.771 updated ! and save the model! (step:2808) ----------
iteration : 3000 loss : 24.500 NLL : -16.111 KLD : 0.000 KLD_attention : 8.389 
iteration : 3200 loss : 35.892 NLL : -27.244 KLD : 0.000 KLD_attention : 8.648 
iteration : 3400 loss : 26.175 NLL : -17.843 KLD : 0.000 KLD_attention : 8.332 
iteration : 3600 loss : 21.459 NLL : -13.257 KLD : 0.000 KLD_attention : 8.202 
iteration : 3800 loss : 26.483 NLL : -18.232 KLD : 0.000 KLD_attention : 8.250 
---------- Training loss 19.065 updated ! and save the model! (step:3906) ----------
iteration : 4000 loss : 16.515 NLL : -8.296 KLD : 0.000 KLD_attention : 8.219 
iteration : 4200 loss : 22.197 NLL : -13.730 KLD : 0.000 KLD_attention : 8.467 
---------- Training loss 18.391 updated ! and save the model! (step:4350) ----------
iteration : 4400 loss : 22.760 NLL : -14.418 KLD : 0.000 KLD_attention : 8.342 
iteration : 4600 loss : 26.065 NLL : -17.886 KLD : 0.000 KLD_attention : 8.179 
iteration : 4800 loss : 22.441 NLL : -14.026 KLD : 0.000 KLD_attention : 8.415 
iteration : 5000 loss : 22.250 NLL : -13.828 KLD : 0.000 KLD_attention : 8.422 
iteration : 5200 loss : 18.561 NLL : -10.161 KLD : 0.000 KLD_attention : 8.401 
iteration : 5400 loss : 18.696 NLL : -10.292 KLD : 0.000 KLD_attention : 8.404 
---------- Training loss 17.440 updated ! and save the model! (step:5592) ----------
iteration : 5600 loss : 16.211 NLL : -7.775 KLD : 0.000 KLD_attention : 8.436 
iteration : 5800 loss : 25.316 NLL : -16.990 KLD : 0.000 KLD_attention : 8.326 
iteration : 6000 loss : 19.300 NLL : -11.121 KLD : 0.000 KLD_attention : 8.178 
iteration : 6200 loss : 26.745 NLL : -18.196 KLD : 0.000 KLD_attention : 8.549 
iteration : 6400 loss : 23.313 NLL : -14.880 KLD : 0.000 KLD_attention : 8.433 
iteration : 6600 loss : 18.090 NLL : -9.949 KLD : 0.000 KLD_attention : 8.141 
iteration : 6800 loss : 27.992 NLL : -19.723 KLD : 0.000 KLD_attention : 8.270 
---------- Training loss 16.477 updated ! and save the model! (step:6894) ----------
iteration : 7000 loss : 21.257 NLL : -12.991 KLD : 0.000 KLD_attention : 8.266 
---------- Training loss 16.219 updated ! and save the model! (step:7098) ----------
iteration : 7200 loss : 23.380 NLL : -15.267 KLD : 0.000 KLD_attention : 8.113 
iteration : 7400 loss : 22.120 NLL : -13.700 KLD : 0.000 KLD_attention : 8.420 
iteration : 7600 loss : 15.806 NLL : -7.428 KLD : 0.000 KLD_attention : 8.378 
iteration : 7800 loss : 26.125 NLL : -18.011 KLD : 0.000 KLD_attention : 8.114 
iteration : 8000 loss : 27.236 NLL : -18.973 KLD : 0.000 KLD_attention : 8.263 
iteration : 8200 loss : 25.946 NLL : -17.375 KLD : 0.000 KLD_attention : 8.571 
---------- Training loss 15.914 updated ! and save the model! (step:8334) ----------
iteration : 8400 loss : 28.573 NLL : -20.058 KLD : 0.000 KLD_attention : 8.515 
iteration : 8600 loss : 22.868 NLL : -14.636 KLD : 0.000 KLD_attention : 8.232 
iteration : 8800 loss : 25.173 NLL : -16.810 KLD : 0.000 KLD_attention : 8.363 
iteration : 9000 loss : 19.984 NLL : -11.550 KLD : 0.000 KLD_attention : 8.434 
iteration : 9200 loss : 30.878 NLL : -22.635 KLD : 0.000 KLD_attention : 8.242 
iteration : 9400 loss : 17.176 NLL : -9.039 KLD : 0.000 KLD_attention : 8.138 
iteration : 9600 loss : 29.108 NLL : -20.918 KLD : 0.000 KLD_attention : 8.190 
iteration : 9800 loss : 14.217 NLL : -5.916 KLD : 0.000 KLD_attention : 8.301 
iteration : 10000 loss : 25.358 NLL : -16.992 KLD : 0.000 KLD_attention : 8.365 
iteration : 10200 loss : 25.326 NLL : -16.991 KLD : 0.000 KLD_attention : 8.335 
iteration : 10400 loss : 23.712 NLL : -15.322 KLD : 0.000 KLD_attention : 8.391 
iteration : 10600 loss : 19.578 NLL : -11.348 KLD : 0.000 KLD_attention : 8.229 
iteration : 10800 loss : 18.649 NLL : -10.412 KLD : 0.000 KLD_attention : 8.237 
iteration : 11000 loss : 25.865 NLL : -17.577 KLD : 0.000 KLD_attention : 8.288 
iteration : 11200 loss : 15.491 NLL : -7.085 KLD : 0.000 KLD_attention : 8.406 
iteration : 11400 loss : 23.820 NLL : -15.470 KLD : 0.000 KLD_attention : 8.350 
iteration : 11600 loss : 15.459 NLL : -7.197 KLD : 0.000 KLD_attention : 8.263 
iteration : 11800 loss : 27.547 NLL : -19.382 KLD : 0.000 KLD_attention : 8.165 
iteration : 12000 loss : 24.473 NLL : -16.196 KLD : 0.000 KLD_attention : 8.278 
iteration : 12200 loss : 21.575 NLL : -13.270 KLD : 0.000 KLD_attention : 8.305 
iteration : 12400 loss : 20.635 NLL : -12.551 KLD : 0.000 KLD_attention : 8.084 
iteration : 12600 loss : 22.821 NLL : -14.628 KLD : 0.000 KLD_attention : 8.193 
iteration : 12800 loss : 23.522 NLL : -15.194 KLD : 0.000 KLD_attention : 8.328 
iteration : 13000 loss : 31.275 NLL : -23.225 KLD : 0.000 KLD_attention : 8.050 
iteration : 13200 loss : 26.651 NLL : -18.428 KLD : 0.000 KLD_attention : 8.223 
iteration : 13400 loss : 19.062 NLL : -10.684 KLD : 0.000 KLD_attention : 8.379 
iteration : 13600 loss : 16.110 NLL : -8.036 KLD : 0.000 KLD_attention : 8.074 
---------- Training loss 14.569 updated ! and save the model! (step:13716) ----------
iteration : 13800 loss : 20.942 NLL : -12.589 KLD : 0.000 KLD_attention : 8.353 
iteration : 14000 loss : 24.080 NLL : -15.708 KLD : 0.000 KLD_attention : 8.373 
iteration : 14200 loss : 25.116 NLL : -16.970 KLD : 0.000 KLD_attention : 8.146 
iteration : 14400 loss : 24.056 NLL : -15.877 KLD : 0.000 KLD_attention : 8.179 
iteration : 14600 loss : 18.318 NLL : -10.129 KLD : 0.000 KLD_attention : 8.190 
iteration : 14800 loss : 17.551 NLL : -9.111 KLD : 0.000 KLD_attention : 8.440 
iteration : 15000 loss : 24.406 NLL : -16.219 KLD : 0.000 KLD_attention : 8.186 
iteration : 15200 loss : 15.379 NLL : -7.318 KLD : 0.000 KLD_attention : 8.060 
iteration : 15400 loss : 22.939 NLL : -14.752 KLD : 0.000 KLD_attention : 8.187 
iteration : 15600 loss : 17.858 NLL : -9.511 KLD : 0.000 KLD_attention : 8.347 
iteration : 15800 loss : 16.480 NLL : -8.332 KLD : 0.000 KLD_attention : 8.148 
iteration : 16000 loss : 22.684 NLL : -14.317 KLD : 0.000 KLD_attention : 8.367 
iteration : 16200 loss : 20.171 NLL : -11.816 KLD : 0.000 KLD_attention : 8.355 
iteration : 16400 loss : 16.327 NLL : -8.246 KLD : 0.000 KLD_attention : 8.081 
iteration : 16600 loss : 23.214 NLL : -15.031 KLD : 0.000 KLD_attention : 8.183 
iteration : 16800 loss : 23.824 NLL : -15.556 KLD : 0.000 KLD_attention : 8.267 
iteration : 17000 loss : 20.960 NLL : -12.913 KLD : 0.000 KLD_attention : 8.046 
iteration : 17200 loss : 17.955 NLL : -9.609 KLD : 0.000 KLD_attention : 8.347 
iteration : 17400 loss : 15.628 NLL : -7.498 KLD : 0.001 KLD_attention : 8.129 
iteration : 17600 loss : 21.199 NLL : -12.741 KLD : 0.000 KLD_attention : 8.458 
iteration : 17800 loss : 21.522 NLL : -13.315 KLD : 0.001 KLD_attention : 8.207 
iteration : 18000 loss : 14.312 NLL : -6.358 KLD : 0.000 KLD_attention : 7.954 
iteration : 18200 loss : 26.413 NLL : -18.207 KLD : 0.000 KLD_attention : 8.206 
iteration : 18400 loss : 17.298 NLL : -8.955 KLD : 0.000 KLD_attention : 8.343 
iteration : 18600 loss : 30.701 NLL : -22.632 KLD : 0.000 KLD_attention : 8.068 
iteration : 18800 loss : 17.118 NLL : -9.081 KLD : 0.000 KLD_attention : 8.037 
iteration : 19000 loss : 21.211 NLL : -13.066 KLD : 0.000 KLD_attention : 8.145 
iteration : 19200 loss : 23.260 NLL : -14.834 KLD : 0.000 KLD_attention : 8.426 
iteration : 19400 loss : 20.132 NLL : -12.130 KLD : 0.001 KLD_attention : 8.001 
iteration : 19600 loss : 34.492 NLL : -26.250 KLD : 0.004 KLD_attention : 8.238 
iteration : 19800 loss : 30.455 NLL : -22.389 KLD : 0.000 KLD_attention : 8.065 
iteration : 20000 loss : 18.129 NLL : -10.035 KLD : 0.000 KLD_attention : 8.094 
iteration : 20200 loss : 37.428 NLL : -29.279 KLD : 0.001 KLD_attention : 8.148 
iteration : 20400 loss : 30.835 NLL : -22.411 KLD : 0.000 KLD_attention : 8.424 
iteration : 20600 loss : 18.843 NLL : -10.587 KLD : 0.000 KLD_attention : 8.256 
iteration : 20800 loss : 23.557 NLL : -15.220 KLD : 0.003 KLD_attention : 8.334 
iteration : 21000 loss : 14.133 NLL : -5.864 KLD : 0.000 KLD_attention : 8.269 
iteration : 21200 loss : 29.385 NLL : -21.313 KLD : 0.000 KLD_attention : 8.072 
iteration : 21400 loss : 20.384 NLL : -12.169 KLD : 0.000 KLD_attention : 8.215 
iteration : 21600 loss : 13.129 NLL : -4.951 KLD : 0.000 KLD_attention : 8.179 
iteration : 21800 loss : 24.625 NLL : -16.567 KLD : 0.002 KLD_attention : 8.056 
iteration : 22000 loss : 30.595 NLL : -22.451 KLD : 0.000 KLD_attention : 8.145 
iteration : 22200 loss : 24.426 NLL : -16.449 KLD : 0.000 KLD_attention : 7.976 
iteration : 22400 loss : 26.550 NLL : -18.442 KLD : 0.000 KLD_attention : 8.108 
iteration : 22600 loss : 23.416 NLL : -15.208 KLD : 0.001 KLD_attention : 8.207 
iteration : 22800 loss : 27.990 NLL : -19.823 KLD : 0.003 KLD_attention : 8.164 
iteration : 23000 loss : 21.053 NLL : -13.022 KLD : 0.000 KLD_attention : 8.031 
iteration : 23200 loss : 22.395 NLL : -14.163 KLD : 0.000 KLD_attention : 8.232 
iteration : 23400 loss : 22.086 NLL : -13.667 KLD : 0.001 KLD_attention : 8.418 
iteration : 23600 loss : 18.045 NLL : -9.801 KLD : 0.000 KLD_attention : 8.244 
iteration : 23800 loss : 20.580 NLL : -12.612 KLD : 0.001 KLD_attention : 7.967 
iteration : 24000 loss : 17.464 NLL : -9.467 KLD : 0.000 KLD_attention : 7.996 
iteration : 24200 loss : 17.662 NLL : -9.316 KLD : 0.000 KLD_attention : 8.346 
iteration : 24400 loss : 30.124 NLL : -21.941 KLD : 0.001 KLD_attention : 8.181 
iteration : 24600 loss : 19.590 NLL : -11.214 KLD : 0.000 KLD_attention : 8.376 
iteration : 24800 loss : 27.810 NLL : -19.897 KLD : 0.000 KLD_attention : 7.912 
iteration : 25000 loss : 13.617 NLL : -5.421 KLD : 0.004 KLD_attention : 8.192 
iteration : 25200 loss : 19.751 NLL : -11.711 KLD : 0.000 KLD_attention : 8.040 
iteration : 25400 loss : 27.488 NLL : -19.421 KLD : 0.000 KLD_attention : 8.067 
iteration : 25600 loss : 18.261 NLL : -9.891 KLD : 0.000 KLD_attention : 8.370 
iteration : 25800 loss : 17.831 NLL : -9.756 KLD : 0.001 KLD_attention : 8.074 
iteration : 26000 loss : 16.165 NLL : -7.894 KLD : 0.000 KLD_attention : 8.271 
iteration : 26200 loss : 20.292 NLL : -12.205 KLD : 0.000 KLD_attention : 8.087 
iteration : 26400 loss : 16.160 NLL : -8.344 KLD : 0.001 KLD_attention : 7.815 
iteration : 26600 loss : 13.288 NLL : -5.133 KLD : 0.001 KLD_attention : 8.154 
iteration : 26800 loss : 19.277 NLL : -11.278 KLD : 0.000 KLD_attention : 7.999 
iteration : 27000 loss : 19.797 NLL : -11.659 KLD : 0.000 KLD_attention : 8.138 
iteration : 27200 loss : 24.675 NLL : -16.528 KLD : 0.001 KLD_attention : 8.146 
iteration : 27400 loss : 17.301 NLL : -8.928 KLD : 0.001 KLD_attention : 8.373 
iteration : 27600 loss : 20.827 NLL : -12.885 KLD : 0.001 KLD_attention : 7.941 
iteration : 27800 loss : 14.827 NLL : -6.564 KLD : 0.000 KLD_attention : 8.263 
iteration : 28000 loss : 16.871 NLL : -8.530 KLD : 0.000 KLD_attention : 8.341 
iteration : 28200 loss : 21.791 NLL : -13.595 KLD : 0.000 KLD_attention : 8.195 
iteration : 28400 loss : 13.790 NLL : -5.712 KLD : 0.001 KLD_attention : 8.076 
iteration : 28600 loss : 16.150 NLL : -7.935 KLD : 0.000 KLD_attention : 8.215 
iteration : 28800 loss : 19.111 NLL : -11.116 KLD : 0.000 KLD_attention : 7.994 
iteration : 29000 loss : 22.049 NLL : -13.729 KLD : 0.000 KLD_attention : 8.321 
iteration : 29200 loss : 29.758 NLL : -21.858 KLD : 0.003 KLD_attention : 7.898 
iteration : 29400 loss : 13.669 NLL : -5.313 KLD : 0.000 KLD_attention : 8.356 
iteration : 29600 loss : 24.526 NLL : -16.421 KLD : 0.001 KLD_attention : 8.104 
iteration : 29800 loss : 18.984 NLL : -10.599 KLD : 0.000 KLD_attention : 8.385 
iteration : 30000 loss : 16.764 NLL : -8.475 KLD : 0.000 KLD_attention : 8.289 
---------- Training loss 20.606 updated ! and save the model! (step:30000) ----------
---------- Training loss 20.291 updated ! and save the model! (step:30012) ----------
---------- Training loss 20.129 updated ! and save the model! (step:30036) ----------
---------- Training loss 19.755 updated ! and save the model! (step:30060) ----------
---------- Training loss 18.879 updated ! and save the model! (step:30066) ----------
---------- Training loss 18.438 updated ! and save the model! (step:30090) ----------
---------- Training loss 17.630 updated ! and save the model! (step:30132) ----------
iteration : 30200 loss : 16.451 NLL : -8.142 KLD : 0.000 KLD_attention : 8.309 
---------- Training loss 15.958 updated ! and save the model! (step:30210) ----------
iteration : 30400 loss : 17.571 NLL : -9.208 KLD : 0.000 KLD_attention : 8.363 
---------- Training loss 15.054 updated ! and save the model! (step:30438) ----------
iteration : 30600 loss : 26.518 NLL : -18.589 KLD : 0.002 KLD_attention : 7.927 
iteration : 30800 loss : 23.798 NLL : -15.716 KLD : 0.001 KLD_attention : 8.081 
iteration : 31000 loss : 22.138 NLL : -14.268 KLD : 0.002 KLD_attention : 7.868 
iteration : 31200 loss : 29.361 NLL : -21.331 KLD : 0.000 KLD_attention : 8.030 
iteration : 31400 loss : 21.646 NLL : -13.303 KLD : 0.000 KLD_attention : 8.342 
iteration : 31600 loss : 22.611 NLL : -14.541 KLD : 0.000 KLD_attention : 8.070 
iteration : 31800 loss : 17.679 NLL : -9.344 KLD : 0.000 KLD_attention : 8.335 
iteration : 32000 loss : 29.137 NLL : -21.118 KLD : 0.002 KLD_attention : 8.017 
iteration : 32200 loss : 26.818 NLL : -18.711 KLD : 0.000 KLD_attention : 8.107 
iteration : 32400 loss : 18.490 NLL : -10.212 KLD : 0.000 KLD_attention : 8.277 
iteration : 32600 loss : 19.577 NLL : -11.170 KLD : 0.000 KLD_attention : 8.406 
iteration : 32800 loss : 14.396 NLL : -6.153 KLD : 0.000 KLD_attention : 8.243 
iteration : 33000 loss : 17.511 NLL : -9.485 KLD : 0.002 KLD_attention : 8.025 
iteration : 33200 loss : 19.643 NLL : -11.391 KLD : 0.000 KLD_attention : 8.252 
iteration : 33400 loss : 23.673 NLL : -15.677 KLD : 0.001 KLD_attention : 7.995 
iteration : 33600 loss : 18.914 NLL : -10.568 KLD : 0.000 KLD_attention : 8.346 
iteration : 33800 loss : 16.740 NLL : -8.598 KLD : 0.000 KLD_attention : 8.141 
iteration : 34000 loss : 19.269 NLL : -10.949 KLD : 0.000 KLD_attention : 8.319 
iteration : 34200 loss : 20.930 NLL : -12.560 KLD : 0.000 KLD_attention : 8.369 
iteration : 34400 loss : 25.362 NLL : -17.207 KLD : 0.001 KLD_attention : 8.153 
iteration : 34600 loss : 27.751 NLL : -19.423 KLD : 0.001 KLD_attention : 8.327 
iteration : 34800 loss : 23.142 NLL : -14.838 KLD : 0.000 KLD_attention : 8.303 
iteration : 35000 loss : 19.409 NLL : -11.222 KLD : 0.001 KLD_attention : 8.186 
iteration : 35200 loss : 25.710 NLL : -17.519 KLD : 0.000 KLD_attention : 8.191 
iteration : 35400 loss : 26.144 NLL : -18.183 KLD : 0.005 KLD_attention : 7.957 
iteration : 35600 loss : 18.270 NLL : -10.412 KLD : 0.002 KLD_attention : 7.856 
iteration : 35800 loss : 18.377 NLL : -10.044 KLD : 0.000 KLD_attention : 8.333 
iteration : 36000 loss : 19.990 NLL : -11.609 KLD : 0.000 KLD_attention : 8.381 
iteration : 36200 loss : 17.336 NLL : -8.951 KLD : 0.000 KLD_attention : 8.385 
iteration : 36400 loss : 23.871 NLL : -15.618 KLD : 0.000 KLD_attention : 8.253 
iteration : 36600 loss : 15.390 NLL : -7.088 KLD : 0.000 KLD_attention : 8.303 
iteration : 36800 loss : 19.791 NLL : -11.423 KLD : 0.000 KLD_attention : 8.368 
iteration : 37000 loss : 29.939 NLL : -21.927 KLD : 0.002 KLD_attention : 8.010 
iteration : 37200 loss : 19.043 NLL : -11.099 KLD : 0.001 KLD_attention : 7.943 
iteration : 37400 loss : 19.557 NLL : -11.302 KLD : 0.000 KLD_attention : 8.255 
iteration : 37600 loss : 17.705 NLL : -9.369 KLD : 0.000 KLD_attention : 8.336 
iteration : 37800 loss : 14.507 NLL : -6.500 KLD : 0.001 KLD_attention : 8.006 
iteration : 38000 loss : 15.712 NLL : -7.360 KLD : 0.000 KLD_attention : 8.351 
iteration : 38200 loss : 21.483 NLL : -13.146 KLD : 0.000 KLD_attention : 8.337 
iteration : 38400 loss : 12.532 NLL : -4.679 KLD : 0.004 KLD_attention : 7.849 
iteration : 38600 loss : 15.608 NLL : -7.275 KLD : 0.000 KLD_attention : 8.333 
iteration : 38800 loss : 26.875 NLL : -18.914 KLD : 0.001 KLD_attention : 7.960 
iteration : 39000 loss : 19.077 NLL : -11.117 KLD : 0.002 KLD_attention : 7.959 
iteration : 39200 loss : 27.958 NLL : -19.856 KLD : 0.002 KLD_attention : 8.100 
iteration : 39400 loss : 23.958 NLL : -16.046 KLD : 0.001 KLD_attention : 7.911 
iteration : 39600 loss : 22.530 NLL : -14.472 KLD : 0.003 KLD_attention : 8.054 
iteration : 39800 loss : 22.370 NLL : -14.362 KLD : 0.001 KLD_attention : 8.007 
iteration : 40000 loss : 25.710 NLL : -17.533 KLD : 0.000 KLD_attention : 8.177 
---------- Training loss 14.681 updated ! and save the model! (step:40038) ----------
iteration : 40200 loss : 16.251 NLL : -7.953 KLD : 0.000 KLD_attention : 8.298 
iteration : 40400 loss : 14.770 NLL : -6.757 KLD : 0.005 KLD_attention : 8.008 
iteration : 40600 loss : 21.893 NLL : -13.770 KLD : 0.002 KLD_attention : 8.122 
iteration : 40800 loss : 14.142 NLL : -5.935 KLD : 0.000 KLD_attention : 8.206 
iteration : 41000 loss : 17.185 NLL : -9.239 KLD : 0.002 KLD_attention : 7.944 
iteration : 41200 loss : 17.941 NLL : -9.595 KLD : 0.000 KLD_attention : 8.346 
---------- Training loss 13.952 updated ! and save the model! (step:41340) ----------
iteration : 41400 loss : 12.688 NLL : -4.585 KLD : 0.000 KLD_attention : 8.103 
iteration : 41600 loss : 21.083 NLL : -13.080 KLD : 0.001 KLD_attention : 8.002 
iteration : 41800 loss : 15.844 NLL : -7.538 KLD : 0.000 KLD_attention : 8.305 
iteration : 42000 loss : 17.067 NLL : -8.714 KLD : 0.000 KLD_attention : 8.353 
iteration : 42200 loss : 16.401 NLL : -8.207 KLD : 0.000 KLD_attention : 8.194 
iteration : 42400 loss : 20.204 NLL : -12.130 KLD : 0.003 KLD_attention : 8.071 
iteration : 42600 loss : 22.022 NLL : -13.711 KLD : 0.000 KLD_attention : 8.310 
iteration : 42800 loss : 24.963 NLL : -17.021 KLD : 0.002 KLD_attention : 7.940 
iteration : 43000 loss : 14.606 NLL : -6.605 KLD : 0.004 KLD_attention : 7.998 
iteration : 43200 loss : 13.400 NLL : -5.078 KLD : 0.000 KLD_attention : 8.321 
iteration : 43400 loss : 29.906 NLL : -21.914 KLD : 0.002 KLD_attention : 7.990 
iteration : 43600 loss : 14.523 NLL : -6.270 KLD : 0.000 KLD_attention : 8.254 
iteration : 43800 loss : 33.876 NLL : -25.865 KLD : 0.005 KLD_attention : 8.006 
iteration : 44000 loss : 29.708 NLL : -21.756 KLD : 0.002 KLD_attention : 7.950 
iteration : 44200 loss : 12.948 NLL : -5.086 KLD : 0.002 KLD_attention : 7.861 
iteration : 44400 loss : 26.454 NLL : -18.209 KLD : 0.000 KLD_attention : 8.245 
iteration : 44600 loss : 15.654 NLL : -7.293 KLD : 0.000 KLD_attention : 8.360 
iteration : 44800 loss : 18.292 NLL : -10.046 KLD : 0.000 KLD_attention : 8.245 
iteration : 45000 loss : 30.346 NLL : -22.454 KLD : 0.001 KLD_attention : 7.890 
iteration : 45200 loss : 16.211 NLL : -8.224 KLD : 0.002 KLD_attention : 7.985 
iteration : 45400 loss : 22.543 NLL : -14.477 KLD : 0.001 KLD_attention : 8.065 
iteration : 45600 loss : 31.535 NLL : -23.626 KLD : 0.002 KLD_attention : 7.907 
iteration : 45800 loss : 19.046 NLL : -10.751 KLD : 0.000 KLD_attention : 8.295 
iteration : 46000 loss : 17.342 NLL : -9.214 KLD : 0.000 KLD_attention : 8.128 
iteration : 46200 loss : 16.633 NLL : -8.358 KLD : 0.001 KLD_attention : 8.275 
iteration : 46400 loss : 22.142 NLL : -14.038 KLD : 0.001 KLD_attention : 8.103 
iteration : 46600 loss : 20.306 NLL : -12.140 KLD : 0.001 KLD_attention : 8.166 
iteration : 46800 loss : 23.839 NLL : -15.891 KLD : 0.001 KLD_attention : 7.947 
iteration : 47000 loss : 15.547 NLL : -7.247 KLD : 0.000 KLD_attention : 8.300 
iteration : 47200 loss : 22.627 NLL : -14.636 KLD : 0.001 KLD_attention : 7.990 
iteration : 47400 loss : 26.645 NLL : -18.466 KLD : 0.004 KLD_attention : 8.175 
iteration : 47600 loss : 24.312 NLL : -16.319 KLD : 0.001 KLD_attention : 7.992 
iteration : 47800 loss : 13.166 NLL : -5.049 KLD : 0.000 KLD_attention : 8.117 
iteration : 48000 loss : 24.017 NLL : -16.050 KLD : 0.001 KLD_attention : 7.966 
iteration : 48200 loss : 22.662 NLL : -14.626 KLD : 0.001 KLD_attention : 8.036 
iteration : 48400 loss : 22.842 NLL : -14.906 KLD : 0.001 KLD_attention : 7.935 
iteration : 48600 loss : 15.524 NLL : -7.174 KLD : 0.000 KLD_attention : 8.349 
iteration : 48800 loss : 19.434 NLL : -11.248 KLD : 0.001 KLD_attention : 8.186 
iteration : 49000 loss : 21.597 NLL : -13.331 KLD : 0.000 KLD_attention : 8.266 
iteration : 49200 loss : 34.862 NLL : -26.972 KLD : 0.005 KLD_attention : 7.885 
iteration : 49400 loss : 20.682 NLL : -12.382 KLD : 0.000 KLD_attention : 8.300 
iteration : 49600 loss : 30.634 NLL : -22.630 KLD : 0.002 KLD_attention : 8.002 
iteration : 49800 loss : 16.617 NLL : -8.657 KLD : 0.002 KLD_attention : 7.958 
iteration : 50000 loss : 26.909 NLL : -19.054 KLD : 0.005 KLD_attention : 7.850 
iteration : 50200 loss : 30.098 NLL : -22.013 KLD : 0.002 KLD_attention : 8.082 
iteration : 50400 loss : 19.584 NLL : -11.463 KLD : 0.001 KLD_attention : 8.120 
iteration : 50600 loss : 14.830 NLL : -6.586 KLD : 0.000 KLD_attention : 8.244 
iteration : 50800 loss : 13.250 NLL : -5.332 KLD : 0.003 KLD_attention : 7.915 
iteration : 51000 loss : 18.109 NLL : -9.759 KLD : 0.000 KLD_attention : 8.350 
iteration : 51200 loss : 16.670 NLL : -8.291 KLD : 0.000 KLD_attention : 8.379 
iteration : 51400 loss : 21.301 NLL : -13.129 KLD : 0.000 KLD_attention : 8.172 
iteration : 51600 loss : 29.898 NLL : -21.962 KLD : 0.002 KLD_attention : 7.935 
---------- Training loss 13.884 updated ! and save the model! (step:51630) ----------
iteration : 51800 loss : 14.828 NLL : -6.499 KLD : 0.000 KLD_attention : 8.329 
iteration : 52000 loss : 21.812 NLL : -13.472 KLD : 0.000 KLD_attention : 8.339 
iteration : 52200 loss : 13.165 NLL : -5.122 KLD : 0.002 KLD_attention : 8.040 
iteration : 52400 loss : 21.908 NLL : -13.880 KLD : 0.000 KLD_attention : 8.028 
iteration : 52600 loss : 20.419 NLL : -12.456 KLD : 0.000 KLD_attention : 7.962 
iteration : 52800 loss : 12.964 NLL : -4.989 KLD : 0.001 KLD_attention : 7.974 
iteration : 53000 loss : 24.338 NLL : -16.507 KLD : 0.001 KLD_attention : 7.830 
iteration : 53200 loss : 28.042 NLL : -20.130 KLD : 0.004 KLD_attention : 7.908 
iteration : 53400 loss : 16.751 NLL : -8.812 KLD : 0.003 KLD_attention : 7.935 
iteration : 53600 loss : 18.532 NLL : -10.546 KLD : 0.001 KLD_attention : 7.985 
iteration : 53800 loss : 15.235 NLL : -6.898 KLD : 0.000 KLD_attention : 8.337 
iteration : 54000 loss : 20.216 NLL : -11.867 KLD : 0.000 KLD_attention : 8.349 
iteration : 54200 loss : 20.307 NLL : -12.329 KLD : 0.006 KLD_attention : 7.973 
iteration : 54400 loss : 13.844 NLL : -5.511 KLD : 0.000 KLD_attention : 8.333 
iteration : 54600 loss : 19.273 NLL : -11.212 KLD : 0.001 KLD_attention : 8.060 
---------- Training loss 13.370 updated ! and save the model! (step:54798) ----------
iteration : 54800 loss : 11.675 NLL : -3.559 KLD : 0.000 KLD_attention : 8.116 
iteration : 55000 loss : 18.409 NLL : -10.157 KLD : 0.001 KLD_attention : 8.251 
iteration : 55200 loss : 16.367 NLL : -8.333 KLD : 0.001 KLD_attention : 8.033 
iteration : 55400 loss : 14.487 NLL : -6.404 KLD : 0.001 KLD_attention : 8.081 
iteration : 55600 loss : 17.903 NLL : -9.818 KLD : 0.001 KLD_attention : 8.084 
iteration : 55800 loss : 18.767 NLL : -10.954 KLD : 0.003 KLD_attention : 7.810 
iteration : 56000 loss : 12.773 NLL : -4.491 KLD : 0.000 KLD_attention : 8.282 
iteration : 56200 loss : 17.122 NLL : -9.110 KLD : 0.001 KLD_attention : 8.011 
iteration : 56400 loss : 25.092 NLL : -17.113 KLD : 0.003 KLD_attention : 7.976 
iteration : 56600 loss : 22.837 NLL : -14.710 KLD : 0.002 KLD_attention : 8.124 
iteration : 56800 loss : 13.285 NLL : -5.215 KLD : 0.000 KLD_attention : 8.070 
iteration : 57000 loss : 18.624 NLL : -10.280 KLD : 0.002 KLD_attention : 8.341 
iteration : 57200 loss : 24.209 NLL : -16.304 KLD : 0.002 KLD_attention : 7.903 
iteration : 57400 loss : 15.304 NLL : -6.968 KLD : 0.000 KLD_attention : 8.336 
iteration : 57600 loss : 23.731 NLL : -15.917 KLD : 0.002 KLD_attention : 7.813 
iteration : 57800 loss : 13.128 NLL : -5.208 KLD : 0.000 KLD_attention : 7.920 
iteration : 58000 loss : 27.204 NLL : -19.352 KLD : 0.001 KLD_attention : 7.851 
iteration : 58200 loss : 16.135 NLL : -8.249 KLD : 0.001 KLD_attention : 7.885 
iteration : 58400 loss : 23.075 NLL : -14.755 KLD : 0.000 KLD_attention : 8.319 
iteration : 58600 loss : 26.206 NLL : -18.318 KLD : 0.003 KLD_attention : 7.885 
iteration : 58800 loss : 20.944 NLL : -12.865 KLD : 0.001 KLD_attention : 8.079 
iteration : 59000 loss : 13.235 NLL : -4.896 KLD : 0.001 KLD_attention : 8.339 
iteration : 59200 loss : 24.238 NLL : -16.086 KLD : 0.002 KLD_attention : 8.150 
iteration : 59400 loss : 24.098 NLL : -16.246 KLD : 0.003 KLD_attention : 7.849 
iteration : 59600 loss : 21.030 NLL : -12.808 KLD : 0.000 KLD_attention : 8.222 
iteration : 59800 loss : 16.574 NLL : -8.335 KLD : 0.002 KLD_attention : 8.237 
iteration : 60000 loss : 28.305 NLL : -20.292 KLD : 0.001 KLD_attention : 8.012 
---------- Training loss 19.260 updated ! and save the model! (step:60000) ----------
---------- Training loss 19.047 updated ! and save the model! (step:60012) ----------
---------- Training loss 17.642 updated ! and save the model! (step:60030) ----------
---------- Training loss 17.437 updated ! and save the model! (step:60066) ----------
---------- Training loss 16.985 updated ! and save the model! (step:60072) ----------
---------- Training loss 16.272 updated ! and save the model! (step:60090) ----------
iteration : 60200 loss : 26.212 NLL : -18.224 KLD : 0.001 KLD_attention : 7.987 
---------- Training loss 15.908 updated ! and save the model! (step:60348) ----------
---------- Training loss 15.021 updated ! and save the model! (step:60360) ----------
iteration : 60400 loss : 15.920 NLL : -7.735 KLD : 0.001 KLD_attention : 8.184 
iteration : 60600 loss : 12.155 NLL : -4.295 KLD : 0.003 KLD_attention : 7.857 
iteration : 60800 loss : 15.418 NLL : -7.490 KLD : 0.004 KLD_attention : 7.924 
---------- Training loss 14.493 updated ! and save the model! (step:60858) ----------
iteration : 61000 loss : 22.583 NLL : -14.608 KLD : 0.009 KLD_attention : 7.966 
iteration : 61200 loss : 15.088 NLL : -6.875 KLD : 0.000 KLD_attention : 8.213 
iteration : 61400 loss : 31.375 NLL : -23.406 KLD : 0.001 KLD_attention : 7.967 
iteration : 61600 loss : 23.498 NLL : -15.504 KLD : 0.003 KLD_attention : 7.991 
iteration : 61800 loss : 18.369 NLL : -10.133 KLD : 0.001 KLD_attention : 8.235 
iteration : 62000 loss : 27.557 NLL : -19.561 KLD : 0.004 KLD_attention : 7.992 
iteration : 62200 loss : 24.405 NLL : -16.536 KLD : 0.003 KLD_attention : 7.865 
iteration : 62400 loss : 28.218 NLL : -20.257 KLD : 0.010 KLD_attention : 7.952 
iteration : 62600 loss : 18.098 NLL : -10.040 KLD : 0.001 KLD_attention : 8.057 
---------- Training loss 14.123 updated ! and save the model! (step:62772) ----------
iteration : 62800 loss : 15.801 NLL : -7.598 KLD : 0.001 KLD_attention : 8.202 
iteration : 63000 loss : 30.664 NLL : -22.698 KLD : 0.003 KLD_attention : 7.963 
iteration : 63200 loss : 14.329 NLL : -6.025 KLD : 0.000 KLD_attention : 8.304 
iteration : 63400 loss : 14.772 NLL : -6.553 KLD : 0.000 KLD_attention : 8.219 
iteration : 63600 loss : 19.462 NLL : -11.188 KLD : 0.001 KLD_attention : 8.273 
iteration : 63800 loss : 27.642 NLL : -19.801 KLD : 0.003 KLD_attention : 7.839 
iteration : 64000 loss : 16.055 NLL : -7.968 KLD : 0.001 KLD_attention : 8.086 
iteration : 64200 loss : 13.155 NLL : -4.839 KLD : 0.000 KLD_attention : 8.315 
---------- Training loss 13.981 updated ! and save the model! (step:64200) ----------
iteration : 64400 loss : 19.738 NLL : -11.621 KLD : 0.001 KLD_attention : 8.117 
iteration : 64600 loss : 17.691 NLL : -9.575 KLD : 0.002 KLD_attention : 8.114 
iteration : 64800 loss : 18.465 NLL : -10.127 KLD : 0.000 KLD_attention : 8.337 
iteration : 65000 loss : 24.234 NLL : -16.148 KLD : 0.002 KLD_attention : 8.084 
iteration : 65200 loss : 14.235 NLL : -6.508 KLD : 0.006 KLD_attention : 7.721 
iteration : 65400 loss : 15.502 NLL : -7.176 KLD : 0.000 KLD_attention : 8.326 
iteration : 65600 loss : 29.647 NLL : -21.774 KLD : 0.001 KLD_attention : 7.871 
iteration : 65800 loss : 12.936 NLL : -5.099 KLD : 0.001 KLD_attention : 7.836 
iteration : 66000 loss : 20.342 NLL : -12.198 KLD : 0.001 KLD_attention : 8.143 
iteration : 66200 loss : 22.117 NLL : -14.093 KLD : 0.001 KLD_attention : 8.022 
iteration : 66400 loss : 12.893 NLL : -4.953 KLD : 0.001 KLD_attention : 7.940 
iteration : 66600 loss : 16.328 NLL : -8.520 KLD : 0.001 KLD_attention : 7.807 
iteration : 66800 loss : 20.195 NLL : -11.922 KLD : 0.000 KLD_attention : 8.273 
---------- Training loss 13.798 updated ! and save the model! (step:66966) ----------
iteration : 67000 loss : 18.310 NLL : -10.096 KLD : 0.001 KLD_attention : 8.214 
iteration : 67200 loss : 27.855 NLL : -19.960 KLD : 0.007 KLD_attention : 7.888 
iteration : 67400 loss : 17.395 NLL : -9.083 KLD : 0.000 KLD_attention : 8.312 
iteration : 67600 loss : 22.351 NLL : -14.195 KLD : 0.001 KLD_attention : 8.155 
iteration : 67800 loss : 14.443 NLL : -6.301 KLD : 0.001 KLD_attention : 8.142 
iteration : 68000 loss : 27.829 NLL : -19.930 KLD : 0.002 KLD_attention : 7.897 
iteration : 68200 loss : 15.971 NLL : -7.663 KLD : 0.000 KLD_attention : 8.308 
iteration : 68400 loss : 12.130 NLL : -4.244 KLD : 0.000 KLD_attention : 7.886 
iteration : 68600 loss : 21.216 NLL : -13.012 KLD : 0.001 KLD_attention : 8.202 
iteration : 68800 loss : 18.752 NLL : -10.427 KLD : 0.001 KLD_attention : 8.324 
iteration : 69000 loss : 15.148 NLL : -7.058 KLD : 0.002 KLD_attention : 8.088 
iteration : 69200 loss : 19.911 NLL : -11.632 KLD : 0.000 KLD_attention : 8.278 
iteration : 69400 loss : 12.358 NLL : -4.265 KLD : 0.001 KLD_attention : 8.092 
iteration : 69600 loss : 18.303 NLL : -10.407 KLD : 0.006 KLD_attention : 7.890 
iteration : 69800 loss : 24.207 NLL : -16.358 KLD : 0.002 KLD_attention : 7.847 
iteration : 70000 loss : 17.776 NLL : -9.477 KLD : 0.001 KLD_attention : 8.298 
iteration : 70200 loss : 17.176 NLL : -8.868 KLD : 0.000 KLD_attention : 8.308 
---------- Training loss 13.703 updated ! and save the model! (step:70332) ----------
iteration : 70400 loss : 18.487 NLL : -10.231 KLD : 0.000 KLD_attention : 8.255 
iteration : 70600 loss : 12.711 NLL : -4.918 KLD : 0.001 KLD_attention : 7.792 
---------- Training loss 13.457 updated ! and save the model! (step:70752) ----------
iteration : 70800 loss : 17.750 NLL : -9.534 KLD : 0.002 KLD_attention : 8.214 
iteration : 71000 loss : 23.825 NLL : -15.905 KLD : 0.002 KLD_attention : 7.918 
iteration : 71200 loss : 19.168 NLL : -10.849 KLD : 0.002 KLD_attention : 8.318 
iteration : 71400 loss : 19.162 NLL : -10.972 KLD : 0.003 KLD_attention : 8.188 
iteration : 71600 loss : 30.902 NLL : -22.957 KLD : 0.002 KLD_attention : 7.942 
iteration : 71800 loss : 22.688 NLL : -14.551 KLD : 0.001 KLD_attention : 8.136 
iteration : 72000 loss : 20.327 NLL : -12.056 KLD : 0.001 KLD_attention : 8.270 
iteration : 72200 loss : 24.073 NLL : -16.046 KLD : 0.001 KLD_attention : 8.026 
iteration : 72400 loss : 16.508 NLL : -8.197 KLD : 0.000 KLD_attention : 8.311 
iteration : 72600 loss : 20.054 NLL : -11.860 KLD : 0.000 KLD_attention : 8.193 
iteration : 72800 loss : 33.002 NLL : -25.109 KLD : 0.005 KLD_attention : 7.889 
iteration : 73000 loss : 15.827 NLL : -7.747 KLD : 0.032 KLD_attention : 8.049 
iteration : 73200 loss : 25.378 NLL : -17.507 KLD : 0.002 KLD_attention : 7.869 
iteration : 73400 loss : 25.859 NLL : -17.897 KLD : 0.002 KLD_attention : 7.960 
iteration : 73600 loss : 13.236 NLL : -5.196 KLD : 0.000 KLD_attention : 8.040 
iteration : 73800 loss : 22.456 NLL : -14.372 KLD : 0.003 KLD_attention : 8.081 
iteration : 74000 loss : 24.570 NLL : -16.622 KLD : 0.002 KLD_attention : 7.945 
iteration : 74200 loss : 15.512 NLL : -7.625 KLD : 0.002 KLD_attention : 7.885 
iteration : 74400 loss : 22.133 NLL : -14.195 KLD : 0.027 KLD_attention : 7.912 
iteration : 74600 loss : 13.111 NLL : -4.882 KLD : 0.000 KLD_attention : 8.228 
iteration : 74800 loss : 16.370 NLL : -8.575 KLD : 0.004 KLD_attention : 7.792 
iteration : 75000 loss : 19.602 NLL : -11.257 KLD : 0.002 KLD_attention : 8.342 
iteration : 75200 loss : 24.681 NLL : -16.844 KLD : 0.005 KLD_attention : 7.832 
iteration : 75400 loss : 15.755 NLL : -7.774 KLD : 0.001 KLD_attention : 7.981 
iteration : 75600 loss : 19.614 NLL : -11.762 KLD : 0.007 KLD_attention : 7.844 
iteration : 75800 loss : 17.180 NLL : -8.949 KLD : 0.002 KLD_attention : 8.229 
iteration : 76000 loss : 33.291 NLL : -25.353 KLD : 0.006 KLD_attention : 7.931 
iteration : 76200 loss : 13.505 NLL : -5.498 KLD : 0.003 KLD_attention : 8.005 
iteration : 76400 loss : 21.598 NLL : -13.434 KLD : 0.001 KLD_attention : 8.163 
iteration : 76600 loss : 21.555 NLL : -13.657 KLD : 0.005 KLD_attention : 7.893 
iteration : 76800 loss : 17.684 NLL : -9.472 KLD : 0.001 KLD_attention : 8.211 
iteration : 77000 loss : 11.710 NLL : -3.880 KLD : 0.000 KLD_attention : 7.829 
iteration : 77200 loss : 16.436 NLL : -8.129 KLD : 0.001 KLD_attention : 8.306 
iteration : 77400 loss : 19.856 NLL : -11.718 KLD : 0.000 KLD_attention : 8.137 
iteration : 77600 loss : 15.773 NLL : -7.564 KLD : 0.001 KLD_attention : 8.208 
iteration : 77800 loss : 17.856 NLL : -10.013 KLD : 0.002 KLD_attention : 7.840 
iteration : 78000 loss : 17.815 NLL : -9.627 KLD : 0.001 KLD_attention : 8.187 
iteration : 78200 loss : 18.740 NLL : -10.812 KLD : 0.012 KLD_attention : 7.916 
iteration : 78400 loss : 23.908 NLL : -15.827 KLD : 0.003 KLD_attention : 8.079 
iteration : 78600 loss : 22.153 NLL : -14.248 KLD : 0.002 KLD_attention : 7.903 
iteration : 78800 loss : 21.022 NLL : -13.041 KLD : 0.002 KLD_attention : 7.978 
iteration : 79000 loss : 18.526 NLL : -10.174 KLD : 0.000 KLD_attention : 8.351 
iteration : 79200 loss : 13.167 NLL : -5.299 KLD : 0.007 KLD_attention : 7.860 
iteration : 79400 loss : 19.601 NLL : -11.588 KLD : 0.002 KLD_attention : 8.011 
iteration : 79600 loss : 23.380 NLL : -15.541 KLD : 0.004 KLD_attention : 7.835 
iteration : 79800 loss : 28.813 NLL : -20.978 KLD : 0.006 KLD_attention : 7.829 
iteration : 80000 loss : 17.122 NLL : -8.905 KLD : 0.001 KLD_attention : 8.216 
iteration : 80200 loss : 20.943 NLL : -12.897 KLD : 0.001 KLD_attention : 8.045 
iteration : 80400 loss : 18.805 NLL : -10.612 KLD : 0.000 KLD_attention : 8.193 
iteration : 80600 loss : 18.153 NLL : -10.092 KLD : 0.001 KLD_attention : 8.061 
iteration : 80800 loss : 15.966 NLL : -7.585 KLD : 0.001 KLD_attention : 8.380 
iteration : 81000 loss : 30.061 NLL : -22.177 KLD : 0.009 KLD_attention : 7.876 
iteration : 81200 loss : 24.556 NLL : -16.739 KLD : 0.002 KLD_attention : 7.815 
iteration : 81400 loss : 17.294 NLL : -9.080 KLD : 0.001 KLD_attention : 8.213 
iteration : 81600 loss : 18.912 NLL : -11.086 KLD : 0.004 KLD_attention : 7.822 
iteration : 81800 loss : 31.021 NLL : -23.222 KLD : 0.002 KLD_attention : 7.797 
iteration : 82000 loss : 18.912 NLL : -10.591 KLD : 0.000 KLD_attention : 8.320 
iteration : 82200 loss : 21.025 NLL : -12.690 KLD : 0.001 KLD_attention : 8.334 
iteration : 82400 loss : 13.120 NLL : -4.861 KLD : 0.000 KLD_attention : 8.259 
iteration : 82600 loss : 20.778 NLL : -12.943 KLD : 0.001 KLD_attention : 7.834 
iteration : 82800 loss : 22.624 NLL : -14.587 KLD : 0.002 KLD_attention : 8.035 
iteration : 83000 loss : 16.494 NLL : -8.629 KLD : 0.003 KLD_attention : 7.862 
iteration : 83200 loss : 20.802 NLL : -12.605 KLD : 0.001 KLD_attention : 8.196 
iteration : 83400 loss : 24.524 NLL : -16.530 KLD : 0.002 KLD_attention : 7.992 
iteration : 83600 loss : 18.626 NLL : -10.685 KLD : 0.001 KLD_attention : 7.940 
iteration : 83800 loss : 22.616 NLL : -14.070 KLD : 0.002 KLD_attention : 8.544 
iteration : 84000 loss : 20.337 NLL : -12.061 KLD : 0.003 KLD_attention : 8.274 
iteration : 84200 loss : 33.756 NLL : -25.863 KLD : 0.003 KLD_attention : 7.889 
iteration : 84400 loss : 13.863 NLL : -5.656 KLD : 0.000 KLD_attention : 8.207 
iteration : 84600 loss : 13.333 NLL : -5.179 KLD : 0.001 KLD_attention : 8.154 
iteration : 84800 loss : 26.741 NLL : -18.917 KLD : 0.002 KLD_attention : 7.823 
iteration : 85000 loss : 21.098 NLL : -12.907 KLD : 0.000 KLD_attention : 8.191 
iteration : 85200 loss : 24.494 NLL : -16.383 KLD : 0.002 KLD_attention : 8.109 
iteration : 85400 loss : 25.410 NLL : -17.568 KLD : 0.004 KLD_attention : 7.838 
iteration : 85600 loss : 17.907 NLL : -9.782 KLD : 0.001 KLD_attention : 8.125 
iteration : 85800 loss : 13.510 NLL : -5.698 KLD : 0.002 KLD_attention : 7.810 
iteration : 86000 loss : 12.910 NLL : -4.841 KLD : 0.001 KLD_attention : 8.068 
iteration : 86200 loss : 24.678 NLL : -16.859 KLD : 0.003 KLD_attention : 7.816 
iteration : 86400 loss : 18.592 NLL : -10.674 KLD : 0.001 KLD_attention : 7.917 
iteration : 86600 loss : 29.169 NLL : -21.323 KLD : 0.009 KLD_attention : 7.837 
iteration : 86800 loss : 28.283 NLL : -20.352 KLD : 0.002 KLD_attention : 7.929 
iteration : 87000 loss : 13.507 NLL : -5.316 KLD : 0.001 KLD_attention : 8.190 
iteration : 87200 loss : 13.344 NLL : -5.038 KLD : 0.001 KLD_attention : 8.305 
iteration : 87400 loss : 15.736 NLL : -7.892 KLD : 0.002 KLD_attention : 7.843 
iteration : 87600 loss : 14.555 NLL : -6.402 KLD : 0.002 KLD_attention : 8.151 
iteration : 87800 loss : 17.851 NLL : -9.780 KLD : 0.001 KLD_attention : 8.070 
iteration : 88000 loss : 23.209 NLL : -15.491 KLD : 0.003 KLD_attention : 7.714 
iteration : 88200 loss : 23.997 NLL : -15.973 KLD : 0.002 KLD_attention : 8.021 
iteration : 88400 loss : 21.719 NLL : -13.765 KLD : 0.004 KLD_attention : 7.949 
iteration : 88600 loss : 19.259 NLL : -10.958 KLD : 0.001 KLD_attention : 8.300 
iteration : 88800 loss : 14.844 NLL : -6.597 KLD : 0.000 KLD_attention : 8.246 
iteration : 89000 loss : 26.467 NLL : -18.527 KLD : 0.001 KLD_attention : 7.939 
iteration : 89200 loss : 18.506 NLL : -10.133 KLD : 0.000 KLD_attention : 8.373 
iteration : 89400 loss : 27.220 NLL : -19.295 KLD : 0.005 KLD_attention : 7.920 
iteration : 89600 loss : 22.106 NLL : -14.032 KLD : 0.001 KLD_attention : 8.073 
iteration : 89800 loss : 20.653 NLL : -12.486 KLD : 0.002 KLD_attention : 8.165 
iteration : 90000 loss : 18.306 NLL : -10.087 KLD : 0.002 KLD_attention : 8.217 
---------- Training loss 21.963 updated ! and save the model! (step:90000) ----------
---------- Training loss 18.828 updated ! and save the model! (step:90006) ----------
---------- Training loss 17.842 updated ! and save the model! (step:90024) ----------
---------- Training loss 15.506 updated ! and save the model! (step:90030) ----------
iteration : 90200 loss : 25.710 NLL : -17.758 KLD : 0.009 KLD_attention : 7.944 
iteration : 90400 loss : 16.236 NLL : -8.329 KLD : 0.008 KLD_attention : 7.899 
---------- Training loss 14.840 updated ! and save the model! (step:90438) ----------
---------- Training loss 14.446 updated ! and save the model! (step:90444) ----------
---------- Training loss 13.805 updated ! and save the model! (step:90570) ----------
iteration : 90600 loss : 19.919 NLL : -11.775 KLD : 0.001 KLD_attention : 8.142 
iteration : 90800 loss : 17.740 NLL : -9.462 KLD : 0.001 KLD_attention : 8.277 
iteration : 91000 loss : 23.109 NLL : -15.176 KLD : 0.007 KLD_attention : 7.926 
iteration : 91200 loss : 23.456 NLL : -15.388 KLD : 0.001 KLD_attention : 8.068 
---------- Training loss 13.700 updated ! and save the model! (step:91380) ----------
iteration : 91400 loss : 28.022 NLL : -20.183 KLD : 0.007 KLD_attention : 7.832 
iteration : 91600 loss : 15.977 NLL : -7.752 KLD : 0.001 KLD_attention : 8.224 
iteration : 91800 loss : 21.917 NLL : -14.039 KLD : 0.005 KLD_attention : 7.873 
iteration : 92000 loss : 19.247 NLL : -11.073 KLD : 0.001 KLD_attention : 8.174 
---------- Training loss 13.396 updated ! and save the model! (step:92112) ----------
iteration : 92200 loss : 11.827 NLL : -3.484 KLD : 0.001 KLD_attention : 8.342 
iteration : 92400 loss : 17.510 NLL : -9.360 KLD : 0.000 KLD_attention : 8.150 
iteration : 92600 loss : 12.998 NLL : -5.026 KLD : 0.001 KLD_attention : 7.971 
iteration : 92800 loss : 14.135 NLL : -6.085 KLD : 0.004 KLD_attention : 8.046 
iteration : 93000 loss : 13.007 NLL : -5.116 KLD : 0.002 KLD_attention : 7.890 
iteration : 93200 loss : 12.455 NLL : -4.122 KLD : 0.000 KLD_attention : 8.333 
iteration : 93400 loss : 26.296 NLL : -18.396 KLD : 0.004 KLD_attention : 7.896 
iteration : 93600 loss : 13.549 NLL : -5.415 KLD : 0.001 KLD_attention : 8.133 
iteration : 93800 loss : 11.817 NLL : -3.671 KLD : 0.001 KLD_attention : 8.144 
iteration : 94000 loss : 14.554 NLL : -6.346 KLD : 0.001 KLD_attention : 8.207 
iteration : 94200 loss : 11.644 NLL : -3.706 KLD : 0.001 KLD_attention : 7.937 
iteration : 94400 loss : 10.639 NLL : -2.451 KLD : 0.001 KLD_attention : 8.187 
---------- Training loss 12.784 updated ! and save the model! (step:94404) ----------
iteration : 94600 loss : 28.474 NLL : -20.476 KLD : 0.006 KLD_attention : 7.993 
iteration : 94800 loss : 29.055 NLL : -21.202 KLD : 0.004 KLD_attention : 7.849 
iteration : 95000 loss : 13.251 NLL : -4.884 KLD : 0.000 KLD_attention : 8.367 
iteration : 95200 loss : 26.330 NLL : -18.462 KLD : 0.007 KLD_attention : 7.861 
iteration : 95400 loss : 29.696 NLL : -21.856 KLD : 0.001 KLD_attention : 7.838 
iteration : 95600 loss : 17.647 NLL : -9.324 KLD : 0.000 KLD_attention : 8.323 
iteration : 95800 loss : 15.534 NLL : -7.374 KLD : 0.003 KLD_attention : 8.158 
iteration : 96000 loss : 18.195 NLL : -10.348 KLD : 0.003 KLD_attention : 7.843 
iteration : 96200 loss : 28.150 NLL : -20.217 KLD : 0.000 KLD_attention : 7.933 
iteration : 96400 loss : 12.353 NLL : -4.558 KLD : 0.017 KLD_attention : 7.778 
iteration : 96600 loss : 13.827 NLL : -5.998 KLD : 0.003 KLD_attention : 7.827 
iteration : 96800 loss : 18.074 NLL : -9.858 KLD : 0.000 KLD_attention : 8.216 
iteration : 97000 loss : 24.765 NLL : -16.898 KLD : 0.002 KLD_attention : 7.865 
iteration : 97200 loss : 20.502 NLL : -12.532 KLD : 0.010 KLD_attention : 7.959 
iteration : 97400 loss : 12.787 NLL : -4.657 KLD : 0.000 KLD_attention : 8.129 
iteration : 97600 loss : 17.773 NLL : -9.671 KLD : 0.002 KLD_attention : 8.100 
iteration : 97800 loss : 30.453 NLL : -22.619 KLD : 0.004 KLD_attention : 7.830 
iteration : 98000 loss : 18.928 NLL : -10.802 KLD : 0.001 KLD_attention : 8.126 
iteration : 98200 loss : 13.161 NLL : -4.844 KLD : 0.001 KLD_attention : 8.316 
iteration : 98400 loss : 29.378 NLL : -21.555 KLD : 0.011 KLD_attention : 7.812 
iteration : 98600 loss : 25.846 NLL : -18.008 KLD : 0.004 KLD_attention : 7.834 
iteration : 98800 loss : 11.297 NLL : -3.363 KLD : 0.002 KLD_attention : 7.932 
iteration : 99000 loss : 16.551 NLL : -8.715 KLD : 0.007 KLD_attention : 7.829 
iteration : 99200 loss : 24.437 NLL : -16.602 KLD : 0.003 KLD_attention : 7.832 
iteration : 99400 loss : 30.883 NLL : -23.008 KLD : 0.009 KLD_attention : 7.866 
iteration : 99600 loss : 24.593 NLL : -16.411 KLD : 0.001 KLD_attention : 8.181 
iteration : 99800 loss : 13.970 NLL : -5.815 KLD : 0.001 KLD_attention : 8.154 
iteration : 100000 loss : 22.299 NLL : -14.359 KLD : 0.001 KLD_attention : 7.939 
iteration : 100200 loss : 19.678 NLL : -11.909 KLD : 0.002 KLD_attention : 7.766 
iteration : 100400 loss : 22.997 NLL : -14.870 KLD : 0.001 KLD_attention : 8.126 
iteration : 100600 loss : 18.495 NLL : -10.382 KLD : 0.003 KLD_attention : 8.110 
iteration : 100800 loss : 12.519 NLL : -4.195 KLD : 0.001 KLD_attention : 8.323 
iteration : 101000 loss : 17.065 NLL : -8.871 KLD : 0.001 KLD_attention : 8.193 
iteration : 101200 loss : 11.640 NLL : -3.366 KLD : 0.000 KLD_attention : 8.274 
iteration : 101400 loss : 25.142 NLL : -17.236 KLD : 0.003 KLD_attention : 7.904 
iteration : 101600 loss : 17.942 NLL : -9.615 KLD : 0.001 KLD_attention : 8.327 
iteration : 101800 loss : 24.024 NLL : -16.198 KLD : 0.003 KLD_attention : 7.823 
iteration : 102000 loss : 34.016 NLL : -25.935 KLD : 0.001 KLD_attention : 8.081 
iteration : 102200 loss : 32.395 NLL : -24.506 KLD : 0.010 KLD_attention : 7.879 
iteration : 102400 loss : 25.872 NLL : -17.994 KLD : 0.002 KLD_attention : 7.876 
iteration : 102600 loss : 19.587 NLL : -11.383 KLD : 0.002 KLD_attention : 8.202 
iteration : 102800 loss : 15.462 NLL : -7.138 KLD : 0.000 KLD_attention : 8.323 
iteration : 103000 loss : 29.082 NLL : -21.230 KLD : 0.009 KLD_attention : 7.842 
iteration : 103200 loss : 13.459 NLL : -5.604 KLD : 0.001 KLD_attention : 7.854 
iteration : 103400 loss : 26.255 NLL : -18.406 KLD : 0.016 KLD_attention : 7.834 
iteration : 103600 loss : 21.179 NLL : -12.996 KLD : 0.001 KLD_attention : 8.182 
iteration : 103800 loss : 13.617 NLL : -5.296 KLD : 0.000 KLD_attention : 8.320 
iteration : 104000 loss : 12.028 NLL : -4.250 KLD : 0.005 KLD_attention : 7.774 
iteration : 104200 loss : 22.705 NLL : -14.932 KLD : 0.012 KLD_attention : 7.762 
iteration : 104400 loss : 15.485 NLL : -7.625 KLD : 0.001 KLD_attention : 7.858 
iteration : 104600 loss : 19.994 NLL : -11.711 KLD : 0.001 KLD_attention : 8.282 
iteration : 104800 loss : 17.756 NLL : -9.554 KLD : 0.002 KLD_attention : 8.200 
iteration : 105000 loss : 17.911 NLL : -10.029 KLD : 0.007 KLD_attention : 7.875 
iteration : 105200 loss : 16.611 NLL : -8.339 KLD : 0.000 KLD_attention : 8.271 
iteration : 105400 loss : 17.339 NLL : -9.120 KLD : 0.001 KLD_attention : 8.217 
iteration : 105600 loss : 32.742 NLL : -24.859 KLD : 0.038 KLD_attention : 7.845 
iteration : 105800 loss : 17.000 NLL : -9.144 KLD : 0.004 KLD_attention : 7.852 
iteration : 106000 loss : 26.019 NLL : -18.089 KLD : 0.006 KLD_attention : 7.925 
iteration : 106200 loss : 12.373 NLL : -4.111 KLD : 0.001 KLD_attention : 8.260 
iteration : 106400 loss : 17.071 NLL : -9.065 KLD : 0.002 KLD_attention : 8.004 
iteration : 106600 loss : 20.678 NLL : -12.480 KLD : 0.004 KLD_attention : 8.194 
iteration : 106800 loss : 17.884 NLL : -9.544 KLD : 0.001 KLD_attention : 8.339 
iteration : 107000 loss : 16.963 NLL : -8.740 KLD : 0.001 KLD_attention : 8.222 
iteration : 107200 loss : 19.505 NLL : -11.237 KLD : 0.004 KLD_attention : 8.264 
iteration : 107400 loss : 22.869 NLL : -14.859 KLD : 0.004 KLD_attention : 8.007 
iteration : 107600 loss : 24.895 NLL : -16.885 KLD : 0.003 KLD_attention : 8.008 
iteration : 107800 loss : 17.904 NLL : -9.838 KLD : 0.002 KLD_attention : 8.064 
iteration : 108000 loss : 21.290 NLL : -13.316 KLD : 0.008 KLD_attention : 7.966 
iteration : 108200 loss : 28.578 NLL : -20.705 KLD : 0.002 KLD_attention : 7.872 
iteration : 108400 loss : 21.887 NLL : -13.822 KLD : 0.002 KLD_attention : 8.062 
iteration : 108600 loss : 20.320 NLL : -12.149 KLD : 0.001 KLD_attention : 8.170 
iteration : 108800 loss : 26.020 NLL : -17.853 KLD : 0.000 KLD_attention : 8.167 
iteration : 109000 loss : 21.389 NLL : -13.126 KLD : 0.001 KLD_attention : 8.262 
iteration : 109200 loss : 19.150 NLL : -11.076 KLD : 0.001 KLD_attention : 8.073 
iteration : 109400 loss : 15.586 NLL : -7.651 KLD : 0.007 KLD_attention : 7.928 
iteration : 109600 loss : 14.083 NLL : -6.262 KLD : 0.002 KLD_attention : 7.819 
iteration : 109800 loss : 29.012 NLL : -21.159 KLD : 0.003 KLD_attention : 7.850 
iteration : 110000 loss : 20.230 NLL : -12.082 KLD : 0.001 KLD_attention : 8.146 
iteration : 110200 loss : 15.069 NLL : -7.112 KLD : 0.010 KLD_attention : 7.947 
iteration : 110400 loss : 17.763 NLL : -9.625 KLD : 0.001 KLD_attention : 8.137 
iteration : 110600 loss : 14.145 NLL : -6.085 KLD : 0.002 KLD_attention : 8.058 
iteration : 110800 loss : 13.094 NLL : -5.095 KLD : 0.002 KLD_attention : 7.998 
iteration : 111000 loss : 11.454 NLL : -3.267 KLD : 0.001 KLD_attention : 8.187 
iteration : 111200 loss : 14.307 NLL : -6.355 KLD : 0.004 KLD_attention : 7.948 
iteration : 111400 loss : 19.189 NLL : -11.157 KLD : 0.001 KLD_attention : 8.030 
iteration : 111600 loss : 10.136 NLL : -1.801 KLD : 0.000 KLD_attention : 8.334 
iteration : 111800 loss : 11.217 NLL : -3.291 KLD : 0.000 KLD_attention : 7.926 
iteration : 112000 loss : 15.317 NLL : -7.391 KLD : 0.004 KLD_attention : 7.922 
iteration : 112200 loss : 16.454 NLL : -8.246 KLD : 0.002 KLD_attention : 8.206 
iteration : 112400 loss : 26.788 NLL : -18.764 KLD : 0.002 KLD_attention : 8.022 
iteration : 112600 loss : 30.292 NLL : -22.451 KLD : 0.022 KLD_attention : 7.819 
iteration : 112800 loss : 24.562 NLL : -16.761 KLD : 0.005 KLD_attention : 7.797 
iteration : 113000 loss : 12.873 NLL : -4.728 KLD : 0.002 KLD_attention : 8.143 
iteration : 113200 loss : 10.699 NLL : -2.452 KLD : 0.001 KLD_attention : 8.246 
iteration : 113400 loss : 14.625 NLL : -6.420 KLD : 0.000 KLD_attention : 8.204 
iteration : 113600 loss : 18.109 NLL : -10.026 KLD : 0.003 KLD_attention : 8.079 
iteration : 113800 loss : 30.175 NLL : -22.360 KLD : 0.004 KLD_attention : 7.812 
iteration : 114000 loss : 18.463 NLL : -10.210 KLD : 0.001 KLD_attention : 8.252 
iteration : 114200 loss : 21.696 NLL : -13.669 KLD : 0.001 KLD_attention : 8.026 
iteration : 114400 loss : 17.225 NLL : -9.023 KLD : 0.004 KLD_attention : 8.198 
iteration : 114600 loss : 14.149 NLL : -5.877 KLD : 0.001 KLD_attention : 8.272 
iteration : 114800 loss : 12.614 NLL : -4.730 KLD : 0.003 KLD_attention : 7.881 
iteration : 115000 loss : 14.593 NLL : -6.412 KLD : 0.001 KLD_attention : 8.180 
iteration : 115200 loss : 21.285 NLL : -13.402 KLD : 0.004 KLD_attention : 7.879 
iteration : 115400 loss : 17.530 NLL : -9.365 KLD : 0.001 KLD_attention : 8.165 
iteration : 115600 loss : 27.828 NLL : -19.734 KLD : 0.008 KLD_attention : 8.087 
iteration : 115800 loss : 12.125 NLL : -3.981 KLD : 0.001 KLD_attention : 8.144 
iteration : 116000 loss : 11.523 NLL : -3.262 KLD : 0.000 KLD_attention : 8.260 
iteration : 116200 loss : 21.508 NLL : -13.548 KLD : 0.002 KLD_attention : 7.958 
iteration : 116400 loss : 11.636 NLL : -3.725 KLD : 0.001 KLD_attention : 7.910 
iteration : 116600 loss : 25.444 NLL : -17.639 KLD : 0.004 KLD_attention : 7.801 
iteration : 116800 loss : 12.551 NLL : -4.400 KLD : 0.000 KLD_attention : 8.150 
iteration : 117000 loss : 12.701 NLL : -4.917 KLD : 0.001 KLD_attention : 7.783 
iteration : 117200 loss : 14.482 NLL : -6.296 KLD : 0.001 KLD_attention : 8.186 
iteration : 117400 loss : 21.735 NLL : -13.922 KLD : 0.004 KLD_attention : 7.808 
iteration : 117600 loss : 17.469 NLL : -9.651 KLD : 0.003 KLD_attention : 7.815 
iteration : 117800 loss : 21.200 NLL : -13.295 KLD : 0.011 KLD_attention : 7.894 
iteration : 118000 loss : 13.328 NLL : -5.426 KLD : 0.005 KLD_attention : 7.896 
iteration : 118200 loss : 16.944 NLL : -8.748 KLD : 0.001 KLD_attention : 8.195 
iteration : 118400 loss : 11.500 NLL : -3.590 KLD : 0.002 KLD_attention : 7.909 
iteration : 118600 loss : 17.104 NLL : -8.958 KLD : 0.004 KLD_attention : 8.142 
iteration : 118800 loss : 24.776 NLL : -16.780 KLD : 0.005 KLD_attention : 7.992 
iteration : 119000 loss : 10.960 NLL : -2.702 KLD : 0.000 KLD_attention : 8.258 
iteration : 119200 loss : 16.484 NLL : -8.625 KLD : 0.006 KLD_attention : 7.852 
iteration : 119400 loss : 16.614 NLL : -8.471 KLD : 0.001 KLD_attention : 8.143 
iteration : 119600 loss : 24.490 NLL : -16.683 KLD : 0.002 KLD_attention : 7.806 
iteration : 119800 loss : 19.560 NLL : -11.428 KLD : 0.001 KLD_attention : 8.131 
iteration : 120000 loss : 17.972 NLL : -9.913 KLD : 0.001 KLD_attention : 8.059 
---------- Training loss 19.008 updated ! and save the model! (step:120000) ----------
---------- Training loss 18.575 updated ! and save the model! (step:120024) ----------
---------- Training loss 16.668 updated ! and save the model! (step:120030) ----------
---------- Training loss 16.587 updated ! and save the model! (step:120060) ----------
---------- Training loss 16.389 updated ! and save the model! (step:120096) ----------
---------- Training loss 13.669 updated ! and save the model! (step:120120) ----------
iteration : 120200 loss : 20.162 NLL : -11.996 KLD : 0.002 KLD_attention : 8.164 
iteration : 120400 loss : 25.215 NLL : -17.370 KLD : 0.007 KLD_attention : 7.839 
iteration : 120600 loss : 11.879 NLL : -3.738 KLD : 0.001 KLD_attention : 8.139 
iteration : 120800 loss : 18.861 NLL : -10.677 KLD : 0.001 KLD_attention : 8.183 
---------- Training loss 13.474 updated ! and save the model! (step:120888) ----------
iteration : 121000 loss : 25.506 NLL : -17.077 KLD : 0.002 KLD_attention : 8.427 
iteration : 121200 loss : 25.290 NLL : -17.204 KLD : 0.002 KLD_attention : 8.084 
iteration : 121400 loss : 18.411 NLL : -10.329 KLD : 0.001 KLD_attention : 8.081 
iteration : 121600 loss : 21.732 NLL : -13.351 KLD : 0.004 KLD_attention : 8.378 
iteration : 121800 loss : 15.833 NLL : -7.469 KLD : 0.001 KLD_attention : 8.363 
iteration : 122000 loss : 11.575 NLL : -3.331 KLD : 0.002 KLD_attention : 8.243 
iteration : 122200 loss : 19.469 NLL : -11.329 KLD : 0.001 KLD_attention : 8.138 
iteration : 122400 loss : 14.134 NLL : -5.864 KLD : 0.001 KLD_attention : 8.269 
iteration : 122600 loss : 22.775 NLL : -14.909 KLD : 0.005 KLD_attention : 7.861 
iteration : 122800 loss : 17.796 NLL : -9.722 KLD : 0.004 KLD_attention : 8.071 
---------- Training loss 13.258 updated ! and save the model! (step:122994) ----------
iteration : 123000 loss : 10.914 NLL : -2.712 KLD : 0.000 KLD_attention : 8.202 
iteration : 123200 loss : 30.196 NLL : -22.391 KLD : 0.018 KLD_attention : 7.786 
iteration : 123400 loss : 20.947 NLL : -12.831 KLD : 0.001 KLD_attention : 8.116 
iteration : 123600 loss : 12.619 NLL : -4.552 KLD : 0.001 KLD_attention : 8.067 
iteration : 123800 loss : 16.321 NLL : -8.017 KLD : 0.001 KLD_attention : 8.304 
iteration : 124000 loss : 22.228 NLL : -14.413 KLD : 0.002 KLD_attention : 7.812 
iteration : 124200 loss : 15.336 NLL : -7.279 KLD : 0.002 KLD_attention : 8.054 
iteration : 124400 loss : 16.862 NLL : -9.056 KLD : 0.002 KLD_attention : 7.803 
iteration : 124600 loss : 20.729 NLL : -12.548 KLD : 0.002 KLD_attention : 8.179 
iteration : 124800 loss : 23.784 NLL : -15.904 KLD : 0.005 KLD_attention : 7.875 
iteration : 125000 loss : 26.998 NLL : -19.123 KLD : 0.004 KLD_attention : 7.870 
iteration : 125200 loss : 20.324 NLL : -12.183 KLD : 0.003 KLD_attention : 8.137 
iteration : 125400 loss : 28.155 NLL : -20.279 KLD : 0.002 KLD_attention : 7.874 
iteration : 125600 loss : 17.094 NLL : -8.909 KLD : 0.004 KLD_attention : 8.181 
iteration : 125800 loss : 19.053 NLL : -11.301 KLD : 0.014 KLD_attention : 7.737 
iteration : 126000 loss : 19.008 NLL : -10.950 KLD : 0.001 KLD_attention : 8.058 
iteration : 126200 loss : 16.666 NLL : -8.681 KLD : 0.001 KLD_attention : 7.984 
iteration : 126400 loss : 12.607 NLL : -4.635 KLD : 0.004 KLD_attention : 7.969 
iteration : 126600 loss : 22.524 NLL : -14.692 KLD : 0.004 KLD_attention : 7.828 
iteration : 126800 loss : 15.268 NLL : -7.047 KLD : 0.002 KLD_attention : 8.219 
iteration : 127000 loss : 17.114 NLL : -8.944 KLD : 0.004 KLD_attention : 8.166 
iteration : 127200 loss : 26.430 NLL : -18.599 KLD : 0.011 KLD_attention : 7.820 
iteration : 127400 loss : 24.399 NLL : -16.412 KLD : 0.007 KLD_attention : 7.980 
iteration : 127600 loss : 29.699 NLL : -21.909 KLD : 0.009 KLD_attention : 7.780 
---------- Training loss 12.743 updated ! and save the model! (step:127734) ----------
iteration : 127800 loss : 24.428 NLL : -16.408 KLD : 0.001 KLD_attention : 8.019 
iteration : 128000 loss : 29.204 NLL : -21.331 KLD : 0.003 KLD_attention : 7.869 
iteration : 128200 loss : 18.434 NLL : -10.288 KLD : 0.001 KLD_attention : 8.145 
iteration : 128400 loss : 18.292 NLL : -9.903 KLD : 0.001 KLD_attention : 8.388 
iteration : 128600 loss : 19.958 NLL : -12.167 KLD : 0.006 KLD_attention : 7.785 
iteration : 128800 loss : 19.997 NLL : -12.192 KLD : 0.001 KLD_attention : 7.804 
iteration : 129000 loss : 17.965 NLL : -10.111 KLD : 0.013 KLD_attention : 7.842 
iteration : 129200 loss : 16.407 NLL : -8.402 KLD : 0.002 KLD_attention : 8.003 
iteration : 129400 loss : 15.325 NLL : -7.080 KLD : 0.002 KLD_attention : 8.243 
iteration : 129600 loss : 29.210 NLL : -21.188 KLD : 0.003 KLD_attention : 8.019 
iteration : 129800 loss : 15.251 NLL : -6.937 KLD : 0.001 KLD_attention : 8.313 
iteration : 130000 loss : 24.951 NLL : -17.133 KLD : 0.036 KLD_attention : 7.781 
iteration : 130200 loss : 18.312 NLL : -10.044 KLD : 0.002 KLD_attention : 8.266 
iteration : 130400 loss : 20.049 NLL : -11.800 KLD : 0.000 KLD_attention : 8.248 
iteration : 130600 loss : 23.951 NLL : -15.826 KLD : 0.000 KLD_attention : 8.124 
iteration : 130800 loss : 16.750 NLL : -8.637 KLD : 0.001 KLD_attention : 8.112 
iteration : 131000 loss : 15.248 NLL : -7.427 KLD : 0.011 KLD_attention : 7.810 
iteration : 131200 loss : 13.300 NLL : -4.977 KLD : 0.000 KLD_attention : 8.322 
iteration : 131400 loss : 23.038 NLL : -15.062 KLD : 0.010 KLD_attention : 7.966 
iteration : 131600 loss : 21.886 NLL : -14.025 KLD : 0.003 KLD_attention : 7.858 
iteration : 131800 loss : 24.915 NLL : -17.027 KLD : 0.002 KLD_attention : 7.885 
iteration : 132000 loss : 13.479 NLL : -5.653 KLD : 0.001 KLD_attention : 7.825 
iteration : 132200 loss : 32.023 NLL : -24.242 KLD : 0.003 KLD_attention : 7.778 
iteration : 132400 loss : 13.734 NLL : -5.538 KLD : 0.006 KLD_attention : 8.190 
iteration : 132600 loss : 12.535 NLL : -4.729 KLD : 0.002 KLD_attention : 7.804 
iteration : 132800 loss : 12.274 NLL : -4.507 KLD : 0.001 KLD_attention : 7.766 
iteration : 133000 loss : 26.845 NLL : -18.922 KLD : 0.018 KLD_attention : 7.904 
iteration : 133200 loss : 21.717 NLL : -13.582 KLD : 0.001 KLD_attention : 8.134 
iteration : 133400 loss : 11.555 NLL : -3.234 KLD : 0.001 KLD_attention : 8.321 
iteration : 133600 loss : 9.569 NLL : -1.389 KLD : 0.000 KLD_attention : 8.180 
iteration : 133800 loss : 19.629 NLL : -11.383 KLD : 0.002 KLD_attention : 8.244 
iteration : 134000 loss : 20.144 NLL : -12.151 KLD : 0.002 KLD_attention : 7.990 
---------- Training loss 12.499 updated ! and save the model! (step:134046) ----------
iteration : 134200 loss : 18.826 NLL : -10.646 KLD : 0.001 KLD_attention : 8.179 
iteration : 134400 loss : 14.657 NLL : -6.355 KLD : 0.001 KLD_attention : 8.301 
iteration : 134600 loss : 14.764 NLL : -6.696 KLD : 0.001 KLD_attention : 8.067 
iteration : 134800 loss : 12.733 NLL : -4.539 KLD : 0.000 KLD_attention : 8.194 
iteration : 135000 loss : 12.373 NLL : -4.558 KLD : 0.008 KLD_attention : 7.807 
iteration : 135200 loss : 12.815 NLL : -4.714 KLD : 0.001 KLD_attention : 8.100 
iteration : 135400 loss : 11.087 NLL : -3.348 KLD : 0.002 KLD_attention : 7.737 
iteration : 135600 loss : 24.539 NLL : -16.727 KLD : 0.003 KLD_attention : 7.809 
iteration : 135800 loss : 16.964 NLL : -8.909 KLD : 0.001 KLD_attention : 8.054 
iteration : 136000 loss : 13.279 NLL : -5.428 KLD : 0.003 KLD_attention : 7.847 
iteration : 136200 loss : 18.390 NLL : -10.593 KLD : 0.003 KLD_attention : 7.793 
iteration : 136400 loss : 19.884 NLL : -11.750 KLD : 0.001 KLD_attention : 8.133 
iteration : 136600 loss : 19.422 NLL : -11.526 KLD : 0.010 KLD_attention : 7.886 
iteration : 136800 loss : 18.418 NLL : -10.170 KLD : 0.002 KLD_attention : 8.246 
iteration : 137000 loss : 26.917 NLL : -18.721 KLD : 0.006 KLD_attention : 8.190 
iteration : 137200 loss : 16.012 NLL : -7.734 KLD : 0.001 KLD_attention : 8.277 
iteration : 137400 loss : 19.166 NLL : -10.918 KLD : 0.001 KLD_attention : 8.247 
iteration : 137600 loss : 14.541 NLL : -6.401 KLD : 0.001 KLD_attention : 8.139 
iteration : 137800 loss : 18.568 NLL : -10.343 KLD : 0.002 KLD_attention : 8.223 
iteration : 138000 loss : 21.137 NLL : -13.064 KLD : 0.002 KLD_attention : 8.070 
iteration : 138200 loss : 14.768 NLL : -6.785 KLD : 0.003 KLD_attention : 7.980 
iteration : 138400 loss : 21.402 NLL : -13.194 KLD : 0.002 KLD_attention : 8.206 
iteration : 138600 loss : 26.527 NLL : -18.369 KLD : 0.002 KLD_attention : 8.156 
iteration : 138800 loss : 11.348 NLL : -3.175 KLD : 0.001 KLD_attention : 8.172 
iteration : 139000 loss : 20.553 NLL : -12.629 KLD : 0.019 KLD_attention : 7.905 
iteration : 139200 loss : 19.920 NLL : -11.660 KLD : 0.004 KLD_attention : 8.256 
iteration : 139400 loss : 20.196 NLL : -12.111 KLD : 0.003 KLD_attention : 8.081 
iteration : 139600 loss : 26.751 NLL : -18.838 KLD : 0.030 KLD_attention : 7.883 
iteration : 139800 loss : 15.682 NLL : -7.856 KLD : 0.013 KLD_attention : 7.813 
iteration : 140000 loss : 12.005 NLL : -3.964 KLD : 0.001 KLD_attention : 8.040 
iteration : 140200 loss : 11.197 NLL : -3.004 KLD : 0.000 KLD_attention : 8.193 
iteration : 140400 loss : 16.879 NLL : -8.577 KLD : 0.002 KLD_attention : 8.300 
iteration : 140600 loss : 15.926 NLL : -7.668 KLD : 0.001 KLD_attention : 8.257 
iteration : 140800 loss : 23.304 NLL : -15.168 KLD : 0.002 KLD_attention : 8.135 
iteration : 141000 loss : 15.345 NLL : -7.470 KLD : 0.001 KLD_attention : 7.874 
iteration : 141200 loss : 12.110 NLL : -3.883 KLD : 0.001 KLD_attention : 8.226 
iteration : 141400 loss : 22.916 NLL : -14.823 KLD : 0.004 KLD_attention : 8.090 
iteration : 141600 loss : 15.745 NLL : -7.756 KLD : 0.003 KLD_attention : 7.986 
iteration : 141800 loss : 11.320 NLL : -3.281 KLD : 0.003 KLD_attention : 8.036 
iteration : 142000 loss : 14.839 NLL : -6.675 KLD : 0.003 KLD_attention : 8.160 
iteration : 142200 loss : 16.215 NLL : -7.900 KLD : 0.001 KLD_attention : 8.313 
iteration : 142400 loss : 15.190 NLL : -7.221 KLD : 0.001 KLD_attention : 7.968 
iteration : 142600 loss : 15.298 NLL : -7.176 KLD : 0.001 KLD_attention : 8.121 
iteration : 142800 loss : 13.248 NLL : -4.992 KLD : 0.003 KLD_attention : 8.253 
iteration : 143000 loss : 28.723 NLL : -20.908 KLD : 0.009 KLD_attention : 7.806 
iteration : 143200 loss : 12.234 NLL : -3.924 KLD : 0.002 KLD_attention : 8.309 
iteration : 143400 loss : 23.510 NLL : -15.533 KLD : 0.001 KLD_attention : 7.976 
iteration : 143600 loss : 17.526 NLL : -9.346 KLD : 0.003 KLD_attention : 8.178 
iteration : 143800 loss : 17.847 NLL : -10.021 KLD : 0.003 KLD_attention : 7.824 
iteration : 144000 loss : 13.938 NLL : -5.965 KLD : 0.001 KLD_attention : 7.972 
iteration : 144200 loss : 21.498 NLL : -13.617 KLD : 0.002 KLD_attention : 7.879 
iteration : 144400 loss : 18.195 NLL : -9.877 KLD : 0.000 KLD_attention : 8.317 
iteration : 144600 loss : 29.089 NLL : -21.260 KLD : 0.004 KLD_attention : 7.825 
iteration : 144800 loss : 13.789 NLL : -5.603 KLD : 0.001 KLD_attention : 8.185 
iteration : 145000 loss : 15.045 NLL : -7.320 KLD : 0.003 KLD_attention : 7.723 
---------- Training loss 12.482 updated ! and save the model! (step:145074) ----------
iteration : 145200 loss : 12.711 NLL : -4.766 KLD : 0.001 KLD_attention : 7.944 
iteration : 145400 loss : 15.551 NLL : -7.487 KLD : 0.000 KLD_attention : 8.064 
iteration : 145600 loss : 15.974 NLL : -8.236 KLD : 0.017 KLD_attention : 7.721 
iteration : 145800 loss : 16.440 NLL : -8.463 KLD : 0.001 KLD_attention : 7.976 
iteration : 146000 loss : 16.822 NLL : -8.844 KLD : 0.006 KLD_attention : 7.972 
iteration : 146200 loss : 31.385 NLL : -23.622 KLD : 0.004 KLD_attention : 7.760 
iteration : 146400 loss : 14.645 NLL : -6.344 KLD : 0.000 KLD_attention : 8.301 
iteration : 146600 loss : 11.767 NLL : -3.814 KLD : 0.001 KLD_attention : 7.952 
iteration : 146800 loss : 13.952 NLL : -5.650 KLD : 0.000 KLD_attention : 8.301 
iteration : 147000 loss : 17.196 NLL : -9.283 KLD : 0.014 KLD_attention : 7.899 
iteration : 147200 loss : 17.153 NLL : -9.070 KLD : 0.004 KLD_attention : 8.079 
iteration : 147400 loss : 24.111 NLL : -16.212 KLD : 0.004 KLD_attention : 7.896 
iteration : 147600 loss : 26.983 NLL : -19.210 KLD : 0.009 KLD_attention : 7.765 
iteration : 147800 loss : 13.425 NLL : -5.482 KLD : 0.004 KLD_attention : 7.940 
iteration : 148000 loss : 19.902 NLL : -11.760 KLD : 0.001 KLD_attention : 8.141 
iteration : 148200 loss : 15.852 NLL : -7.762 KLD : 0.001 KLD_attention : 8.088 
iteration : 148400 loss : 15.313 NLL : -7.493 KLD : 0.003 KLD_attention : 7.817 
iteration : 148600 loss : 11.022 NLL : -2.854 KLD : 0.001 KLD_attention : 8.167 
---------- Training loss 12.288 updated ! and save the model! (step:148602) ----------
iteration : 148800 loss : 17.684 NLL : -9.704 KLD : 0.008 KLD_attention : 7.972 
iteration : 149000 loss : 20.131 NLL : -12.068 KLD : 0.002 KLD_attention : 8.061 
iteration : 149200 loss : 21.005 NLL : -12.819 KLD : 0.001 KLD_attention : 8.185 
iteration : 149400 loss : 19.240 NLL : -11.481 KLD : 0.004 KLD_attention : 7.755 
iteration : 149600 loss : 17.193 NLL : -8.934 KLD : 0.000 KLD_attention : 8.259 
iteration : 149800 loss : 16.069 NLL : -7.993 KLD : 0.002 KLD_attention : 8.074 
iteration : 150000 loss : 28.967 NLL : -20.815 KLD : 0.004 KLD_attention : 8.149 
---------- Training loss 24.822 updated ! and save the model! (step:150000) ----------
---------- Training loss 22.086 updated ! and save the model! (step:150006) ----------
---------- Training loss 21.592 updated ! and save the model! (step:150012) ----------
---------- Training loss 20.867 updated ! and save the model! (step:150024) ----------
---------- Training loss 20.590 updated ! and save the model! (step:150054) ----------
---------- Training loss 18.336 updated ! and save the model! (step:150060) ----------
---------- Training loss 16.475 updated ! and save the model! (step:150072) ----------
---------- Training loss 15.792 updated ! and save the model! (step:150114) ----------
---------- Training loss 15.363 updated ! and save the model! (step:150132) ----------
iteration : 150200 loss : 17.335 NLL : -9.548 KLD : 0.006 KLD_attention : 7.781 
---------- Training loss 15.093 updated ! and save the model! (step:150210) ----------
iteration : 150400 loss : 20.811 NLL : -12.763 KLD : 0.001 KLD_attention : 8.047 
---------- Training loss 14.615 updated ! and save the model! (step:150564) ----------
iteration : 150600 loss : 20.805 NLL : -12.725 KLD : 0.002 KLD_attention : 8.078 
iteration : 150800 loss : 15.601 NLL : -7.597 KLD : 0.002 KLD_attention : 8.002 
iteration : 151000 loss : 16.215 NLL : -8.274 KLD : 0.004 KLD_attention : 7.936 
iteration : 151200 loss : 20.025 NLL : -11.993 KLD : 0.005 KLD_attention : 8.027 
---------- Training loss 14.514 updated ! and save the model! (step:151242) ----------
---------- Training loss 14.309 updated ! and save the model! (step:151308) ----------
iteration : 151400 loss : 29.324 NLL : -21.475 KLD : 0.022 KLD_attention : 7.827 
iteration : 151600 loss : 13.877 NLL : -5.699 KLD : 0.001 KLD_attention : 8.177 
iteration : 151800 loss : 23.855 NLL : -15.956 KLD : 0.006 KLD_attention : 7.893 
---------- Training loss 14.033 updated ! and save the model! (step:151860) ----------
iteration : 152000 loss : 12.231 NLL : -4.498 KLD : 0.003 KLD_attention : 7.731 
iteration : 152200 loss : 13.007 NLL : -4.898 KLD : 0.001 KLD_attention : 8.109 
---------- Training loss 13.778 updated ! and save the model! (step:152262) ----------
iteration : 152400 loss : 18.979 NLL : -11.128 KLD : 0.010 KLD_attention : 7.841 
iteration : 152600 loss : 18.466 NLL : -10.244 KLD : 0.002 KLD_attention : 8.219 
iteration : 152800 loss : 24.574 NLL : -16.566 KLD : 0.003 KLD_attention : 8.005 
iteration : 153000 loss : 15.153 NLL : -6.938 KLD : 0.001 KLD_attention : 8.214 
---------- Training loss 13.532 updated ! and save the model! (step:153084) ----------
iteration : 153200 loss : 22.353 NLL : -14.215 KLD : 0.001 KLD_attention : 8.137 
iteration : 153400 loss : 19.798 NLL : -11.666 KLD : 0.002 KLD_attention : 8.130 
iteration : 153600 loss : 24.880 NLL : -16.956 KLD : 0.015 KLD_attention : 7.909 
iteration : 153800 loss : 16.979 NLL : -9.100 KLD : 0.004 KLD_attention : 7.876 
iteration : 154000 loss : 12.767 NLL : -4.775 KLD : 0.001 KLD_attention : 7.990 
iteration : 154200 loss : 25.833 NLL : -17.831 KLD : 0.002 KLD_attention : 8.001 
iteration : 154400 loss : 22.493 NLL : -14.741 KLD : 0.002 KLD_attention : 7.751 
iteration : 154600 loss : 14.666 NLL : -6.397 KLD : 0.001 KLD_attention : 8.268 
iteration : 154800 loss : 11.764 NLL : -4.047 KLD : 0.006 KLD_attention : 7.711 
iteration : 155000 loss : 12.841 NLL : -4.927 KLD : 0.001 KLD_attention : 7.913 
iteration : 155200 loss : 23.469 NLL : -15.500 KLD : 0.005 KLD_attention : 7.964 
iteration : 155400 loss : 27.219 NLL : -19.412 KLD : 0.005 KLD_attention : 7.802 
iteration : 155600 loss : 27.058 NLL : -19.147 KLD : 0.004 KLD_attention : 7.906 
iteration : 155800 loss : 21.007 NLL : -13.198 KLD : 0.004 KLD_attention : 7.804 
iteration : 156000 loss : 15.885 NLL : -7.677 KLD : 0.001 KLD_attention : 8.207 
iteration : 156200 loss : 13.887 NLL : -5.768 KLD : 0.001 KLD_attention : 8.118 
iteration : 156400 loss : 16.583 NLL : -8.323 KLD : 0.001 KLD_attention : 8.258 
iteration : 156600 loss : 19.453 NLL : -11.513 KLD : 0.002 KLD_attention : 7.938 
iteration : 156800 loss : 20.972 NLL : -12.857 KLD : 0.001 KLD_attention : 8.114 
iteration : 157000 loss : 11.352 NLL : -3.087 KLD : 0.001 KLD_attention : 8.264 
iteration : 157200 loss : 17.975 NLL : -9.792 KLD : 0.001 KLD_attention : 8.182 
iteration : 157400 loss : 14.639 NLL : -6.808 KLD : 0.003 KLD_attention : 7.828 
---------- Training loss 13.138 updated ! and save the model! (step:157470) ----------
iteration : 157600 loss : 27.687 NLL : -19.789 KLD : 0.005 KLD_attention : 7.893 
iteration : 157800 loss : 15.634 NLL : -7.437 KLD : 0.001 KLD_attention : 8.195 
iteration : 158000 loss : 29.166 NLL : -21.343 KLD : 0.012 KLD_attention : 7.812 
iteration : 158200 loss : 26.129 NLL : -18.268 KLD : 0.002 KLD_attention : 7.859 
iteration : 158400 loss : 27.433 NLL : -19.633 KLD : 0.007 KLD_attention : 7.794 
iteration : 158600 loss : 22.146 NLL : -14.119 KLD : 0.004 KLD_attention : 8.022 
iteration : 158800 loss : 22.691 NLL : -14.896 KLD : 0.006 KLD_attention : 7.789 
iteration : 159000 loss : 19.238 NLL : -11.324 KLD : 0.002 KLD_attention : 7.913 
---------- Training loss 13.097 updated ! and save the model! (step:159066) ----------
iteration : 159200 loss : 11.868 NLL : -3.561 KLD : 0.000 KLD_attention : 8.306 
iteration : 159400 loss : 11.747 NLL : -3.944 KLD : 0.002 KLD_attention : 7.801 
iteration : 159600 loss : 14.543 NLL : -6.405 KLD : 0.001 KLD_attention : 8.137 
iteration : 159800 loss : 26.442 NLL : -18.545 KLD : 0.008 KLD_attention : 7.890 
iteration : 160000 loss : 13.788 NLL : -5.643 KLD : 0.001 KLD_attention : 8.143 
iteration : 160200 loss : 20.427 NLL : -12.605 KLD : 0.003 KLD_attention : 7.818 
iteration : 160400 loss : 10.230 NLL : -1.911 KLD : 0.001 KLD_attention : 8.318 
iteration : 160600 loss : 12.683 NLL : -4.517 KLD : 0.001 KLD_attention : 8.165 
iteration : 160800 loss : 25.956 NLL : -17.940 KLD : 0.001 KLD_attention : 8.015 
iteration : 161000 loss : 16.267 NLL : -7.943 KLD : 0.001 KLD_attention : 8.323 
iteration : 161200 loss : 14.849 NLL : -6.541 KLD : 0.001 KLD_attention : 8.307 
iteration : 161400 loss : 15.653 NLL : -7.357 KLD : 0.001 KLD_attention : 8.295 
iteration : 161600 loss : 15.358 NLL : -7.108 KLD : 0.001 KLD_attention : 8.249 
iteration : 161800 loss : 12.965 NLL : -4.614 KLD : 0.001 KLD_attention : 8.350 
iteration : 162000 loss : 12.148 NLL : -3.839 KLD : 0.001 KLD_attention : 8.309 
iteration : 162200 loss : 21.904 NLL : -13.914 KLD : 0.005 KLD_attention : 7.985 
iteration : 162400 loss : 15.192 NLL : -7.322 KLD : 0.002 KLD_attention : 7.867 
iteration : 162600 loss : 15.334 NLL : -7.491 KLD : 0.009 KLD_attention : 7.835 
---------- Training loss 12.826 updated ! and save the model! (step:162786) ----------
iteration : 162800 loss : 24.479 NLL : -16.651 KLD : 0.004 KLD_attention : 7.824 
iteration : 163000 loss : 30.837 NLL : -22.957 KLD : 0.024 KLD_attention : 7.855 
iteration : 163200 loss : 24.576 NLL : -16.712 KLD : 0.014 KLD_attention : 7.850 
iteration : 163400 loss : 14.633 NLL : -6.520 KLD : 0.003 KLD_attention : 8.110 
iteration : 163600 loss : 20.106 NLL : -12.067 KLD : 0.008 KLD_attention : 8.031 
iteration : 163800 loss : 12.013 NLL : -4.240 KLD : 0.007 KLD_attention : 7.766 
iteration : 164000 loss : 13.746 NLL : -5.441 KLD : 0.001 KLD_attention : 8.304 
---------- Training loss 12.717 updated ! and save the model! (step:164088) ----------
iteration : 164200 loss : 21.397 NLL : -13.195 KLD : 0.002 KLD_attention : 8.200 
iteration : 164400 loss : 18.077 NLL : -9.903 KLD : 0.001 KLD_attention : 8.174 
iteration : 164600 loss : 26.890 NLL : -18.981 KLD : 0.003 KLD_attention : 7.906 
iteration : 164800 loss : 12.241 NLL : -4.042 KLD : 0.002 KLD_attention : 8.197 
iteration : 165000 loss : 20.726 NLL : -12.606 KLD : 0.004 KLD_attention : 8.116 
iteration : 165200 loss : 17.419 NLL : -9.384 KLD : 0.001 KLD_attention : 8.033 
iteration : 165400 loss : 11.218 NLL : -2.899 KLD : 0.001 KLD_attention : 8.318 
iteration : 165600 loss : 14.353 NLL : -6.002 KLD : 0.001 KLD_attention : 8.351 
iteration : 165800 loss : 14.041 NLL : -6.129 KLD : 0.005 KLD_attention : 7.906 
iteration : 166000 loss : 17.748 NLL : -9.816 KLD : 0.003 KLD_attention : 7.929 
iteration : 166200 loss : 11.731 NLL : -3.898 KLD : 0.004 KLD_attention : 7.829 
iteration : 166400 loss : 16.090 NLL : -8.186 KLD : 0.003 KLD_attention : 7.901 
iteration : 166600 loss : 14.034 NLL : -5.738 KLD : 0.001 KLD_attention : 8.295 
iteration : 166800 loss : 15.226 NLL : -7.033 KLD : 0.002 KLD_attention : 8.191 
iteration : 167000 loss : 30.747 NLL : -22.934 KLD : 0.009 KLD_attention : 7.805 
iteration : 167200 loss : 23.588 NLL : -15.542 KLD : 0.006 KLD_attention : 8.039 
iteration : 167400 loss : 11.540 NLL : -3.222 KLD : 0.001 KLD_attention : 8.317 
iteration : 167600 loss : 16.545 NLL : -8.698 KLD : 0.002 KLD_attention : 7.846 
iteration : 167800 loss : 18.850 NLL : -11.075 KLD : 0.023 KLD_attention : 7.752 
iteration : 168000 loss : 22.823 NLL : -15.068 KLD : 0.011 KLD_attention : 7.744 
iteration : 168200 loss : 14.369 NLL : -6.439 KLD : 0.001 KLD_attention : 7.929 
iteration : 168400 loss : 14.539 NLL : -6.579 KLD : 0.018 KLD_attention : 7.942 
iteration : 168600 loss : 16.134 NLL : -7.833 KLD : 0.001 KLD_attention : 8.300 
iteration : 168800 loss : 12.104 NLL : -3.788 KLD : 0.002 KLD_attention : 8.314 
iteration : 169000 loss : 19.083 NLL : -10.975 KLD : 0.001 KLD_attention : 8.107 
iteration : 169200 loss : 14.530 NLL : -6.531 KLD : 0.004 KLD_attention : 7.995 
iteration : 169400 loss : 20.462 NLL : -12.335 KLD : 0.001 KLD_attention : 8.127 
iteration : 169600 loss : 19.519 NLL : -11.327 KLD : 0.004 KLD_attention : 8.188 
iteration : 169800 loss : 17.689 NLL : -9.502 KLD : 0.001 KLD_attention : 8.186 
iteration : 170000 loss : 26.096 NLL : -18.318 KLD : 0.015 KLD_attention : 7.763 
iteration : 170200 loss : 22.254 NLL : -14.113 KLD : 0.004 KLD_attention : 8.137 
iteration : 170400 loss : 13.155 NLL : -5.082 KLD : 0.001 KLD_attention : 8.072 
iteration : 170600 loss : 15.070 NLL : -7.042 KLD : 0.001 KLD_attention : 8.027 
iteration : 170800 loss : 20.122 NLL : -12.158 KLD : 0.002 KLD_attention : 7.962 
iteration : 171000 loss : 20.991 NLL : -13.177 KLD : 0.008 KLD_attention : 7.806 
iteration : 171200 loss : 18.516 NLL : -10.425 KLD : 0.003 KLD_attention : 8.088 
iteration : 171400 loss : 14.526 NLL : -6.279 KLD : 0.001 KLD_attention : 8.246 
iteration : 171600 loss : 13.279 NLL : -4.975 KLD : 0.001 KLD_attention : 8.303 
iteration : 171800 loss : 14.633 NLL : -6.585 KLD : 0.001 KLD_attention : 8.047 
iteration : 172000 loss : 15.039 NLL : -6.736 KLD : 0.003 KLD_attention : 8.300 
iteration : 172200 loss : 18.193 NLL : -10.203 KLD : 0.013 KLD_attention : 7.978 
iteration : 172400 loss : 19.078 NLL : -10.920 KLD : 0.002 KLD_attention : 8.156 
iteration : 172600 loss : 20.054 NLL : -12.276 KLD : 0.017 KLD_attention : 7.761 
iteration : 172800 loss : 17.211 NLL : -8.919 KLD : 0.001 KLD_attention : 8.290 
iteration : 173000 loss : 15.181 NLL : -7.131 KLD : 0.002 KLD_attention : 8.048 
iteration : 173200 loss : 18.959 NLL : -10.916 KLD : 0.002 KLD_attention : 8.041 
iteration : 173400 loss : 16.723 NLL : -8.977 KLD : 0.010 KLD_attention : 7.736 
iteration : 173600 loss : 21.298 NLL : -13.473 KLD : 0.002 KLD_attention : 7.822 
---------- Training loss 11.541 updated ! and save the model! (step:173610) ----------
iteration : 173800 loss : 23.659 NLL : -15.666 KLD : 0.005 KLD_attention : 7.989 
iteration : 174000 loss : 15.880 NLL : -7.887 KLD : 0.001 KLD_attention : 7.992 
iteration : 174200 loss : 11.870 NLL : -3.642 KLD : 0.000 KLD_attention : 8.228 
iteration : 174400 loss : 12.686 NLL : -4.586 KLD : 0.005 KLD_attention : 8.095 
iteration : 174600 loss : 27.617 NLL : -19.822 KLD : 0.003 KLD_attention : 7.793 
iteration : 174800 loss : 14.928 NLL : -6.809 KLD : 0.003 KLD_attention : 8.116 
iteration : 175000 loss : 10.900 NLL : -2.799 KLD : 0.001 KLD_attention : 8.100 
iteration : 175200 loss : 17.359 NLL : -9.471 KLD : 0.005 KLD_attention : 7.883 
iteration : 175400 loss : 18.904 NLL : -10.791 KLD : 0.002 KLD_attention : 8.111 
iteration : 175600 loss : 21.829 NLL : -13.700 KLD : 0.004 KLD_attention : 8.126 
iteration : 175800 loss : 22.286 NLL : -14.094 KLD : 0.005 KLD_attention : 8.188 
iteration : 176000 loss : 22.305 NLL : -14.325 KLD : 0.004 KLD_attention : 7.976 
iteration : 176200 loss : 23.370 NLL : -15.605 KLD : 0.028 KLD_attention : 7.736 
iteration : 176400 loss : 21.978 NLL : -14.238 KLD : 0.003 KLD_attention : 7.737 
iteration : 176600 loss : 15.547 NLL : -7.318 KLD : 0.001 KLD_attention : 8.228 
iteration : 176800 loss : 13.024 NLL : -4.828 KLD : 0.001 KLD_attention : 8.195 
iteration : 177000 loss : 15.495 NLL : -7.506 KLD : 0.002 KLD_attention : 7.987 
iteration : 177200 loss : 16.985 NLL : -9.167 KLD : 0.006 KLD_attention : 7.812 
iteration : 177400 loss : 24.773 NLL : -16.969 KLD : 0.005 KLD_attention : 7.800 
iteration : 177600 loss : 14.186 NLL : -6.223 KLD : 0.004 KLD_attention : 7.959 
iteration : 177800 loss : 11.000 NLL : -3.254 KLD : 0.008 KLD_attention : 7.738 
iteration : 178000 loss : 22.509 NLL : -14.600 KLD : 0.005 KLD_attention : 7.903 
iteration : 178200 loss : 21.063 NLL : -13.309 KLD : 0.006 KLD_attention : 7.749 
iteration : 178400 loss : 18.378 NLL : -10.718 KLD : 0.013 KLD_attention : 7.647 
iteration : 178600 loss : 16.061 NLL : -8.282 KLD : 0.005 KLD_attention : 7.774 
iteration : 178800 loss : 16.329 NLL : -8.422 KLD : 0.011 KLD_attention : 7.895 
iteration : 179000 loss : 15.419 NLL : -7.330 KLD : 0.002 KLD_attention : 8.088 
iteration : 179200 loss : 19.724 NLL : -11.464 KLD : 0.006 KLD_attention : 8.255 
iteration : 179400 loss : 16.281 NLL : -8.486 KLD : 0.016 KLD_attention : 7.779 
iteration : 179600 loss : 15.806 NLL : -7.554 KLD : 0.001 KLD_attention : 8.252 
iteration : 179800 loss : 24.606 NLL : -16.516 KLD : 0.022 KLD_attention : 8.068 
iteration : 180000 loss : 19.102 NLL : -11.332 KLD : 0.003 KLD_attention : 7.767 
---------- Training loss 19.984 updated ! and save the model! (step:180000) ----------
---------- Training loss 17.701 updated ! and save the model! (step:180012) ----------
---------- Training loss 15.488 updated ! and save the model! (step:180030) ----------
---------- Training loss 13.684 updated ! and save the model! (step:180102) ----------
iteration : 180200 loss : 23.601 NLL : -15.711 KLD : 0.027 KLD_attention : 7.863 
---------- Training loss 13.280 updated ! and save the model! (step:180270) ----------
iteration : 180400 loss : 21.702 NLL : -13.939 KLD : 0.009 KLD_attention : 7.754 
iteration : 180600 loss : 12.795 NLL : -4.972 KLD : 0.002 KLD_attention : 7.822 
iteration : 180800 loss : 15.512 NLL : -7.626 KLD : 0.007 KLD_attention : 7.879 
iteration : 181000 loss : 17.790 NLL : -9.889 KLD : 0.003 KLD_attention : 7.899 
---------- Training loss 12.648 updated ! and save the model! (step:181170) ----------
iteration : 181200 loss : 20.322 NLL : -12.594 KLD : 0.011 KLD_attention : 7.718 
iteration : 181400 loss : 18.761 NLL : -10.647 KLD : 0.001 KLD_attention : 8.113 
iteration : 181600 loss : 25.078 NLL : -17.283 KLD : 0.006 KLD_attention : 7.788 
iteration : 181800 loss : 20.262 NLL : -12.009 KLD : 0.003 KLD_attention : 8.250 
iteration : 182000 loss : 23.632 NLL : -15.892 KLD : 0.022 KLD_attention : 7.719 
iteration : 182200 loss : 15.165 NLL : -7.269 KLD : 0.001 KLD_attention : 7.894 
iteration : 182400 loss : 28.875 NLL : -21.068 KLD : 0.004 KLD_attention : 7.803 /home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
 /home/mgyukim/workspaces/AI701/recommend_sys/models/parts/attention.py:668: UserWarning:Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.

---------- Training loss 12.467 updated ! and save the model! (step:182484) ----------
iteration : 182600 loss : 13.060 NLL : -5.184 KLD : 0.002 KLD_attention : 7.874 
iteration : 182800 loss : 31.555 NLL : -23.747 KLD : 0.014 KLD_attention : 7.793 
iteration : 183000 loss : 18.750 NLL : -10.853 KLD : 0.002 KLD_attention : 7.895 
iteration : 183200 loss : 14.180 NLL : -6.174 KLD : 0.002 KLD_attention : 8.004 
iteration : 183400 loss : 13.497 NLL : -5.558 KLD : 0.004 KLD_attention : 7.935 
iteration : 183600 loss : 25.997 NLL : -18.213 KLD : 0.017 KLD_attention : 7.767 
iteration : 183800 loss : 24.023 NLL : -16.180 KLD : 0.007 KLD_attention : 7.837 
iteration : 184000 loss : 12.946 NLL : -4.713 KLD : 0.001 KLD_attention : 8.232 
iteration : 184200 loss : 30.208 NLL : -22.449 KLD : 0.015 KLD_attention : 7.744 
iteration : 184400 loss : 21.586 NLL : -13.319 KLD : 0.006 KLD_attention : 8.261 
iteration : 184600 loss : 11.767 NLL : -4.061 KLD : 0.005 KLD_attention : 7.700 
iteration : 184800 loss : 16.819 NLL : -8.628 KLD : 0.010 KLD_attention : 8.180 
iteration : 185000 loss : 13.234 NLL : -5.104 KLD : 0.001 KLD_attention : 8.129 
iteration : 185200 loss : 14.879 NLL : -6.695 KLD : 0.001 KLD_attention : 8.183 
iteration : 185400 loss : 22.480 NLL : -14.580 KLD : 0.006 KLD_attention : 7.894 
iteration : 185600 loss : 24.356 NLL : -16.395 KLD : 0.004 KLD_attention : 7.957 
iteration : 185800 loss : 14.830 NLL : -6.605 KLD : 0.001 KLD_attention : 8.224 
iteration : 186000 loss : 15.272 NLL : -7.458 KLD : 0.014 KLD_attention : 7.801 
iteration : 186200 loss : 23.587 NLL : -15.475 KLD : 0.002 KLD_attention : 8.110 
iteration : 186400 loss : 20.889 NLL : -13.112 KLD : 0.021 KLD_attention : 7.755 
iteration : 186600 loss : 16.458 NLL : -8.690 KLD : 0.012 KLD_attention : 7.756 
iteration : 186800 loss : 12.084 NLL : -3.846 KLD : 0.000 KLD_attention : 8.238 
iteration : 187000 loss : 17.524 NLL : -9.597 KLD : 0.006 KLD_attention : 7.920 
iteration : 187200 loss : 22.260 NLL : -14.294 KLD : 0.011 KLD_attention : 7.955 
iteration : 187400 loss : 12.254 NLL : -3.988 KLD : 0.001 KLD_attention : 8.265 
iteration : 187600 loss : 19.184 NLL : -11.268 KLD : 0.010 KLD_attention : 7.906 
iteration : 187800 loss : 18.711 NLL : -10.558 KLD : 0.018 KLD_attention : 8.135 
iteration : 188000 loss : 21.203 NLL : -13.054 KLD : 0.014 KLD_attention : 8.135 
iteration : 188200 loss : 11.978 NLL : -3.652 KLD : 0.001 KLD_attention : 8.326 
iteration : 188400 loss : 18.118 NLL : -9.865 KLD : 0.001 KLD_attention : 8.253 
iteration : 188600 loss : 18.336 NLL : -10.204 KLD : 0.004 KLD_attention : 8.128 
iteration : 188800 loss : 12.347 NLL : -4.165 KLD : 0.002 KLD_attention : 8.181 
iteration : 189000 loss : 16.697 NLL : -8.831 KLD : 0.004 KLD_attention : 7.862 
iteration : 189200 loss : 22.226 NLL : -14.230 KLD : 0.003 KLD_attention : 7.993 
iteration : 189400 loss : 25.501 NLL : -17.681 KLD : 0.006 KLD_attention : 7.815 
iteration : 189600 loss : 24.283 NLL : -16.475 KLD : 0.004 KLD_attention : 7.804 
iteration : 189800 loss : 20.962 NLL : -12.913 KLD : 0.001 KLD_attention : 8.047 
iteration : 190000 loss : 18.438 NLL : -10.665 KLD : 0.001 KLD_attention : 7.772 
iteration : 190200 loss : 11.674 NLL : -3.454 KLD : 0.001 KLD_attention : 8.220 
iteration : 190400 loss : 25.261 NLL : -17.508 KLD : 0.004 KLD_attention : 7.748 
iteration : 190600 loss : 24.614 NLL : -16.791 KLD : 0.078 KLD_attention : 7.746 
iteration : 190800 loss : 15.995 NLL : -8.120 KLD : 0.021 KLD_attention : 7.854 
iteration : 191000 loss : 26.797 NLL : -18.917 KLD : 0.004 KLD_attention : 7.875 
iteration : 191200 loss : 26.713 NLL : -18.880 KLD : 0.003 KLD_attention : 7.831 
iteration : 191400 loss : 16.299 NLL : -8.289 KLD : 0.002 KLD_attention : 8.008 
iteration : 191600 loss : 24.895 NLL : -16.806 KLD : 0.008 KLD_attention : 8.082 
iteration : 191800 loss : 17.572 NLL : -9.391 KLD : 0.004 KLD_attention : 8.177 
iteration : 192000 loss : 18.509 NLL : -10.180 KLD : 0.000 KLD_attention : 8.329 
iteration : 192200 loss : 26.841 NLL : -19.068 KLD : 0.005 KLD_attention : 7.769 
iteration : 192400 loss : 17.706 NLL : -9.441 KLD : 0.010 KLD_attention : 8.256 
iteration : 192600 loss : 18.882 NLL : -10.714 KLD : 0.002 KLD_attention : 8.166 
iteration : 192800 loss : 13.789 NLL : -5.594 KLD : 0.001 KLD_attention : 8.194 
iteration : 193000 loss : 25.870 NLL : -18.054 KLD : 0.002 KLD_attention : 7.814 
iteration : 193200 loss : 19.016 NLL : -11.114 KLD : 0.001 KLD_attention : 7.901 
iteration : 193400 loss : 19.001 NLL : -10.960 KLD : 0.001 KLD_attention : 8.040 
iteration : 193600 loss : 15.476 NLL : -7.651 KLD : 0.003 KLD_attention : 7.823 
iteration : 193800 loss : 30.815 NLL : -22.988 KLD : 0.016 KLD_attention : 7.810 
iteration : 194000 loss : 11.374 NLL : -3.227 KLD : 0.001 KLD_attention : 8.146 
iteration : 194200 loss : 20.160 NLL : -12.095 KLD : 0.001 KLD_attention : 8.063 
iteration : 194400 loss : 14.692 NLL : -6.518 KLD : 0.002 KLD_attention : 8.172 
iteration : 194600 loss : 12.600 NLL : -4.424 KLD : 0.003 KLD_attention : 8.173 
iteration : 194800 loss : 30.556 NLL : -22.796 KLD : 0.024 KLD_attention : 7.737 
---------- Training loss 12.428 updated ! and save the model! (step:194976) ----------
iteration : 195000 loss : 27.288 NLL : -19.467 KLD : 0.034 KLD_attention : 7.787 
iteration : 195200 loss : 17.380 NLL : -9.655 KLD : 0.005 KLD_attention : 7.721 
iteration : 195400 loss : 11.207 NLL : -3.230 KLD : 0.001 KLD_attention : 7.975 
iteration : 195600 loss : 21.828 NLL : -14.065 KLD : 0.008 KLD_attention : 7.754 
iteration : 195800 loss : 14.430 NLL : -6.385 KLD : 0.001 KLD_attention : 8.044 
iteration : 196000 loss : 11.396 NLL : -3.106 KLD : 0.000 KLD_attention : 8.290 
iteration : 196200 loss : 12.478 NLL : -4.500 KLD : 0.002 KLD_attention : 7.976 
iteration : 196400 loss : 20.990 NLL : -12.805 KLD : 0.003 KLD_attention : 8.183 
iteration : 196600 loss : 13.478 NLL : -5.596 KLD : 0.011 KLD_attention : 7.871 
iteration : 196800 loss : 10.854 NLL : -3.099 KLD : 0.005 KLD_attention : 7.750 
iteration : 197000 loss : 27.802 NLL : -19.919 KLD : 0.014 KLD_attention : 7.869 
iteration : 197200 loss : 13.230 NLL : -5.095 KLD : 0.001 KLD_attention : 8.134 
iteration : 197400 loss : 16.378 NLL : -8.064 KLD : 0.005 KLD_attention : 8.309 
iteration : 197600 loss : 15.792 NLL : -7.688 KLD : 0.002 KLD_attention : 8.101 
iteration : 197800 loss : 18.243 NLL : -10.127 KLD : 0.003 KLD_attention : 8.112 
iteration : 198000 loss : 18.334 NLL : -10.130 KLD : 0.002 KLD_attention : 8.202 
iteration : 198200 loss : 18.480 NLL : -10.480 KLD : 0.003 KLD_attention : 7.996 
iteration : 198400 loss : 15.660 NLL : -7.351 KLD : 0.004 KLD_attention : 8.304 
iteration : 198600 loss : 11.610 NLL : -3.309 KLD : 0.000 KLD_attention : 8.300 
iteration : 198800 loss : 15.092 NLL : -7.149 KLD : 0.003 KLD_attention : 7.939 
iteration : 199000 loss : 12.593 NLL : -4.461 KLD : 0.001 KLD_attention : 8.131 
iteration : 199200 loss : 11.705 NLL : -3.670 KLD : 0.001 KLD_attention : 8.034 
iteration : 199400 loss : 25.863 NLL : -18.109 KLD : 0.015 KLD_attention : 7.739 
iteration : 199600 loss : 23.583 NLL : -15.723 KLD : 0.015 KLD_attention : 7.845 
iteration : 199800 loss : 16.091 NLL : -7.840 KLD : 0.012 KLD_attention : 8.239 
iteration : 200000 loss : 21.095 NLL : -13.101 KLD : 0.009 KLD_attention : 7.985 
---------- Save the model! (step:None) ----------
