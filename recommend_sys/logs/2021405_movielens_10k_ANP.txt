---------- Training loss 115.253 updated ! and save the model! (step:6) ----------
---------- Training loss 36.290 updated ! and save the model! (step:12) ----------
---------- Training loss 34.653 updated ! and save the model! (step:24) ----------
---------- Training loss 34.158 updated ! and save the model! (step:30) ----------
---------- Training loss 26.213 updated ! and save the model! (step:36) ----------
---------- Training loss 24.173 updated ! and save the model! (step:48) ----------
---------- Training loss 22.079 updated ! and save the model! (step:60) ----------
---------- Training loss 20.243 updated ! and save the model! (step:102) ----------
---------- Training loss 18.328 updated ! and save the model! (step:138) ----------
---------- Training loss 18.149 updated ! and save the model! (step:162) ----------
---------- Training loss 17.569 updated ! and save the model! (step:192) ----------
iteration : 200 loss : 20.522 NLL : -20.516 KLD : 0.006 
---------- Training loss 17.563 updated ! and save the model! (step:288) ----------
iteration : 400 loss : 19.230 NLL : -19.229 KLD : 0.001 
iteration : 600 loss : 22.979 NLL : -22.978 KLD : 0.000 
---------- Training loss 16.262 updated ! and save the model! (step:612) ----------
---------- Training loss 15.552 updated ! and save the model! (step:660) ----------
iteration : 800 loss : 14.932 NLL : -14.931 KLD : 0.000 
---------- Training loss 14.699 updated ! and save the model! (step:966) ----------
iteration : 1000 loss : 16.453 NLL : -16.453 KLD : 0.000 
iteration : 1200 loss : 13.503 NLL : -13.503 KLD : 0.000 
iteration : 1400 loss : 16.367 NLL : -16.367 KLD : 0.000 
iteration : 1600 loss : 20.275 NLL : -20.275 KLD : 0.000 
iteration : 1800 loss : 19.666 NLL : -19.666 KLD : 0.000 
iteration : 2000 loss : 21.066 NLL : -21.066 KLD : 0.000 
iteration : 2200 loss : 12.618 NLL : -12.618 KLD : 0.000 
---------- Training loss 14.235 updated ! and save the model! (step:2280) ----------
iteration : 2400 loss : 19.077 NLL : -19.077 KLD : 0.000 
iteration : 2600 loss : 9.350 NLL : -9.350 KLD : 0.000 
---------- Training loss 13.353 updated ! and save the model! (step:2682) ----------
iteration : 2800 loss : 18.013 NLL : -18.013 KLD : 0.000 
iteration : 3000 loss : 17.732 NLL : -17.732 KLD : 0.000 
iteration : 3200 loss : 17.327 NLL : -17.327 KLD : 0.000 
iteration : 3400 loss : 30.992 NLL : -30.992 KLD : 0.000 
iteration : 3600 loss : 22.564 NLL : -22.564 KLD : 0.000 
iteration : 3800 loss : 9.427 NLL : -9.427 KLD : 0.000 
iteration : 4000 loss : 19.423 NLL : -19.423 KLD : 0.000 
---------- Training loss 13.295 updated ! and save the model! (step:4152) ----------
iteration : 4200 loss : 20.666 NLL : -20.666 KLD : 0.000 
---------- Training loss 12.957 updated ! and save the model! (step:4236) ----------
---------- Training loss 12.468 updated ! and save the model! (step:4290) ----------
iteration : 4400 loss : 12.364 NLL : -12.364 KLD : 0.000 
iteration : 4600 loss : 20.458 NLL : -20.458 KLD : 0.000 
iteration : 4800 loss : 13.298 NLL : -13.298 KLD : 0.000 
---------- Training loss 12.282 updated ! and save the model! (step:4884) ----------
iteration : 5000 loss : 19.955 NLL : -19.955 KLD : 0.000 
iteration : 5200 loss : 22.737 NLL : -22.737 KLD : 0.000 
iteration : 5400 loss : 16.696 NLL : -16.696 KLD : 0.000 
iteration : 5600 loss : 23.140 NLL : -23.140 KLD : 0.000 
iteration : 5800 loss : 17.318 NLL : -17.318 KLD : 0.000 
iteration : 6000 loss : 8.856 NLL : -8.856 KLD : 0.000 
iteration : 6200 loss : 18.587 NLL : -18.587 KLD : 0.000 
---------- Training loss 11.289 updated ! and save the model! (step:6336) ----------
iteration : 6400 loss : 14.267 NLL : -14.267 KLD : 0.000 
iteration : 6600 loss : 14.136 NLL : -14.136 KLD : 0.000 
iteration : 6800 loss : 26.204 NLL : -26.203 KLD : 0.000 
iteration : 7000 loss : 32.302 NLL : -32.301 KLD : 0.000 
---------- Training loss 10.548 updated ! and save the model! (step:7050) ----------
iteration : 7200 loss : 17.610 NLL : -17.610 KLD : 0.000 
iteration : 7400 loss : 18.012 NLL : -18.012 KLD : 0.000 
iteration : 7600 loss : 17.348 NLL : -17.348 KLD : 0.000 
iteration : 7800 loss : 10.971 NLL : -10.971 KLD : 0.000 
iteration : 8000 loss : 8.688 NLL : -8.688 KLD : 0.000 
iteration : 8200 loss : 12.245 NLL : -12.245 KLD : 0.000 
---------- Training loss 9.887 updated ! and save the model! (step:8334) ----------
iteration : 8400 loss : 14.730 NLL : -14.729 KLD : 0.000 
iteration : 8600 loss : 17.615 NLL : -17.615 KLD : 0.000 
iteration : 8800 loss : 22.890 NLL : -22.889 KLD : 0.000 
iteration : 9000 loss : 16.903 NLL : -16.903 KLD : 0.000 
iteration : 9200 loss : 10.530 NLL : -10.530 KLD : 0.000 
iteration : 9400 loss : 27.247 NLL : -27.247 KLD : 0.000 
iteration : 9600 loss : 21.476 NLL : -21.476 KLD : 0.000 
iteration : 9800 loss : 14.723 NLL : -14.723 KLD : 0.000 
iteration : 10000 loss : 17.443 NLL : -17.443 KLD : 0.000 
iteration : 10200 loss : 23.237 NLL : -23.237 KLD : 0.000 
iteration : 10400 loss : 12.901 NLL : -12.901 KLD : 0.000 
iteration : 10600 loss : 18.908 NLL : -18.908 KLD : 0.000 
iteration : 10800 loss : 20.548 NLL : -20.548 KLD : 0.000 
iteration : 11000 loss : 6.326 NLL : -6.326 KLD : 0.000 
iteration : 11200 loss : 10.437 NLL : -10.437 KLD : 0.000 
iteration : 11400 loss : 16.133 NLL : -16.133 KLD : 0.000 
iteration : 11600 loss : 13.346 NLL : -13.346 KLD : 0.000 
---------- Training loss 9.689 updated ! and save the model! (step:11790) ----------
iteration : 11800 loss : 14.806 NLL : -14.806 KLD : 0.000 
iteration : 12000 loss : 17.919 NLL : -17.919 KLD : 0.000 
iteration : 12200 loss : 17.318 NLL : -17.318 KLD : 0.000 
iteration : 12400 loss : 6.949 NLL : -6.949 KLD : 0.000 
iteration : 12600 loss : 11.588 NLL : -11.588 KLD : 0.000 
iteration : 12800 loss : 18.426 NLL : -18.426 KLD : 0.000 
iteration : 13000 loss : 12.918 NLL : -12.918 KLD : 0.000 
iteration : 13200 loss : 26.491 NLL : -26.491 KLD : 0.000 
iteration : 13400 loss : 22.033 NLL : -22.033 KLD : 0.000 
iteration : 13600 loss : 13.653 NLL : -13.653 KLD : 0.000 
iteration : 13800 loss : 11.485 NLL : -11.485 KLD : 0.000 
iteration : 14000 loss : 8.966 NLL : -8.966 KLD : 0.000 
---------- Training loss 8.268 updated ! and save the model! (step:14076) ----------
iteration : 14200 loss : 29.630 NLL : -29.630 KLD : 0.000 
iteration : 14400 loss : 23.232 NLL : -23.226 KLD : 0.005 
iteration : 14600 loss : 11.102 NLL : -11.102 KLD : 0.000 
iteration : 14800 loss : 8.289 NLL : -8.289 KLD : 0.000 
iteration : 15000 loss : 17.562 NLL : -17.562 KLD : 0.000 
iteration : 15200 loss : 20.607 NLL : -20.607 KLD : 0.000 
iteration : 15400 loss : 14.955 NLL : -14.955 KLD : 0.000 
iteration : 15600 loss : 20.554 NLL : -20.554 KLD : 0.000 
iteration : 15800 loss : 18.954 NLL : -18.954 KLD : 0.000 
iteration : 16000 loss : 18.635 NLL : -18.634 KLD : 0.000 
iteration : 16200 loss : 8.625 NLL : -8.625 KLD : 0.000 
iteration : 16400 loss : 18.730 NLL : -18.729 KLD : 0.000 
iteration : 16600 loss : 14.902 NLL : -14.902 KLD : 0.000 
iteration : 16800 loss : 8.872 NLL : -8.872 KLD : 0.000 
iteration : 17000 loss : 15.575 NLL : -15.575 KLD : 0.000 
iteration : 17200 loss : 8.303 NLL : -8.303 KLD : 0.000 
iteration : 17400 loss : 19.524 NLL : -19.524 KLD : 0.000 
iteration : 17600 loss : 15.611 NLL : -15.611 KLD : 0.000 
iteration : 17800 loss : 12.820 NLL : -12.820 KLD : 0.000 
iteration : 18000 loss : 22.011 NLL : -22.011 KLD : 0.000 
iteration : 18200 loss : 16.794 NLL : -16.794 KLD : 0.000 
iteration : 18400 loss : 19.845 NLL : -19.845 KLD : 0.000 
iteration : 18600 loss : 19.099 NLL : -19.099 KLD : 0.000 
iteration : 18800 loss : 18.744 NLL : -18.744 KLD : 0.000 
iteration : 19000 loss : 15.055 NLL : -15.055 KLD : 0.000 
iteration : 19200 loss : 8.814 NLL : -8.814 KLD : 0.000 
iteration : 19400 loss : 11.837 NLL : -11.837 KLD : 0.000 
iteration : 19600 loss : 8.920 NLL : -8.920 KLD : 0.000 
iteration : 19800 loss : 13.249 NLL : -13.249 KLD : 0.000 
iteration : 20000 loss : 8.163 NLL : -8.163 KLD : 0.000 
iteration : 20200 loss : 13.767 NLL : -13.767 KLD : 0.000 
iteration : 20400 loss : 19.985 NLL : -19.985 KLD : 0.000 
iteration : 20600 loss : 7.397 NLL : -7.397 KLD : 0.000 
iteration : 20800 loss : 9.159 NLL : -9.158 KLD : 0.000 
iteration : 21000 loss : 18.325 NLL : -18.325 KLD : 0.000 
iteration : 21200 loss : 12.205 NLL : -12.205 KLD : 0.000 
iteration : 21400 loss : 22.046 NLL : -22.045 KLD : 0.000 
iteration : 21600 loss : 14.576 NLL : -14.576 KLD : 0.000 
iteration : 21800 loss : 13.859 NLL : -13.859 KLD : 0.000 
iteration : 22000 loss : 9.038 NLL : -9.038 KLD : 0.000 
iteration : 22200 loss : 17.028 NLL : -17.028 KLD : 0.000 
iteration : 22400 loss : 15.901 NLL : -15.901 KLD : 0.000 
iteration : 22600 loss : 16.398 NLL : -16.398 KLD : 0.000 
---------- Training loss 8.260 updated ! and save the model! (step:22752) ----------
iteration : 22800 loss : 19.106 NLL : -19.106 KLD : 0.000 
iteration : 23000 loss : 16.013 NLL : -16.012 KLD : 0.000 
iteration : 23200 loss : 18.512 NLL : -18.512 KLD : 0.000 
iteration : 23400 loss : 20.914 NLL : -20.914 KLD : 0.000 
iteration : 23600 loss : 22.376 NLL : -22.376 KLD : 0.000 
iteration : 23800 loss : 14.291 NLL : -14.291 KLD : 0.000 
iteration : 24000 loss : 20.455 NLL : -20.455 KLD : 0.000 
iteration : 24200 loss : 19.821 NLL : -19.821 KLD : 0.000 
iteration : 24400 loss : 9.281 NLL : -9.281 KLD : 0.000 
iteration : 24600 loss : 7.657 NLL : -7.657 KLD : 0.000 
iteration : 24800 loss : 12.546 NLL : -12.546 KLD : 0.000 
iteration : 25000 loss : 10.711 NLL : -10.711 KLD : 0.000 
iteration : 25200 loss : 18.224 NLL : -18.224 KLD : 0.000 
iteration : 25400 loss : 9.792 NLL : -9.792 KLD : 0.000 
iteration : 25600 loss : 8.673 NLL : -8.673 KLD : 0.000 
iteration : 25800 loss : 8.145 NLL : -8.145 KLD : 0.000 
iteration : 26000 loss : 7.796 NLL : -7.796 KLD : 0.000 
iteration : 26200 loss : 29.524 NLL : -29.522 KLD : 0.001 
iteration : 26400 loss : 20.181 NLL : -20.181 KLD : 0.000 
iteration : 26600 loss : 13.622 NLL : -13.622 KLD : 0.000 
iteration : 26800 loss : 5.956 NLL : -5.956 KLD : 0.000 
iteration : 27000 loss : 16.376 NLL : -16.376 KLD : 0.000 
iteration : 27200 loss : 20.848 NLL : -20.848 KLD : 0.000 
iteration : 27400 loss : 21.558 NLL : -21.558 KLD : 0.000 
iteration : 27600 loss : 16.610 NLL : -16.610 KLD : 0.000 
iteration : 27800 loss : 11.618 NLL : -11.618 KLD : 0.000 
iteration : 28000 loss : 15.009 NLL : -15.009 KLD : 0.000 
iteration : 28200 loss : 13.247 NLL : -13.247 KLD : 0.000 
iteration : 28400 loss : 12.238 NLL : -12.238 KLD : 0.000 
iteration : 28600 loss : 18.232 NLL : -18.232 KLD : 0.000 
iteration : 28800 loss : 18.692 NLL : -18.692 KLD : 0.000 
iteration : 29000 loss : 15.630 NLL : -15.630 KLD : 0.000 
iteration : 29200 loss : 11.068 NLL : -11.068 KLD : 0.000 
---------- Training loss 8.165 updated ! and save the model! (step:29268) ----------
iteration : 29400 loss : 6.212 NLL : -6.212 KLD : 0.000 
iteration : 29600 loss : 5.911 NLL : -5.911 KLD : 0.000 
iteration : 29800 loss : 14.252 NLL : -14.252 KLD : 0.000 
---------- Training loss 7.935 updated ! and save the model! (step:29922) ----------
iteration : 30000 loss : 10.126 NLL : -10.126 KLD : 0.000 
---------- Training loss 15.772 updated ! and save the model! (step:30000) ----------
---------- Training loss 12.045 updated ! and save the model! (step:30012) ----------
---------- Training loss 10.659 updated ! and save the model! (step:30042) ----------
---------- Training loss 9.797 updated ! and save the model! (step:30066) ----------
---------- Training loss 8.847 updated ! and save the model! (step:30144) ----------
iteration : 30200 loss : 11.842 NLL : -11.842 KLD : 0.000 
iteration : 30400 loss : 22.371 NLL : -22.370 KLD : 0.001 
iteration : 30600 loss : 15.255 NLL : -15.255 KLD : 0.000 
---------- Training loss 8.774 updated ! and save the model! (step:30648) ----------
iteration : 30800 loss : 12.878 NLL : -12.878 KLD : 0.000 
iteration : 31000 loss : 19.375 NLL : -19.375 KLD : 0.000 
iteration : 31200 loss : 8.777 NLL : -8.777 KLD : 0.000 
iteration : 31400 loss : 12.853 NLL : -12.853 KLD : 0.000 
iteration : 31600 loss : 8.441 NLL : -8.441 KLD : 0.000 
iteration : 31800 loss : 25.345 NLL : -25.343 KLD : 0.002 
---------- Training loss 8.537 updated ! and save the model! (step:31872) ----------
iteration : 32000 loss : 24.613 NLL : -24.613 KLD : 0.000 
iteration : 32200 loss : 9.856 NLL : -9.856 KLD : 0.000 
iteration : 32400 loss : 4.766 NLL : -4.766 KLD : 0.000 
iteration : 32600 loss : 16.496 NLL : -16.496 KLD : 0.000 
iteration : 32800 loss : 22.099 NLL : -22.099 KLD : 0.000 
iteration : 33000 loss : 12.967 NLL : -12.967 KLD : 0.000 
iteration : 33200 loss : 10.412 NLL : -10.412 KLD : 0.000 
iteration : 33400 loss : 14.566 NLL : -14.566 KLD : 0.000 
iteration : 33600 loss : 19.398 NLL : -19.397 KLD : 0.001 
iteration : 33800 loss : 15.831 NLL : -15.831 KLD : 0.000 
iteration : 34000 loss : 14.436 NLL : -14.435 KLD : 0.001 
iteration : 34200 loss : 7.057 NLL : -7.057 KLD : 0.000 
iteration : 34400 loss : 5.437 NLL : -5.437 KLD : 0.000 
iteration : 34600 loss : 23.316 NLL : -23.316 KLD : 0.001 
iteration : 34800 loss : 12.422 NLL : -12.422 KLD : 0.000 
iteration : 35000 loss : 8.548 NLL : -8.548 KLD : 0.000 
iteration : 35200 loss : 13.838 NLL : -13.837 KLD : 0.000 
iteration : 35400 loss : 10.653 NLL : -10.653 KLD : 0.000 
---------- Training loss 7.564 updated ! and save the model! (step:35454) ----------
iteration : 35600 loss : 16.050 NLL : -16.050 KLD : 0.000 
iteration : 35800 loss : 18.956 NLL : -18.956 KLD : 0.000 
iteration : 36000 loss : 25.756 NLL : -25.756 KLD : 0.000 
iteration : 36200 loss : 10.605 NLL : -10.605 KLD : 0.000 
iteration : 36400 loss : 6.724 NLL : -6.724 KLD : 0.000 
iteration : 36600 loss : 18.872 NLL : -18.872 KLD : 0.000 
iteration : 36800 loss : 15.397 NLL : -15.395 KLD : 0.001 
iteration : 37000 loss : 12.128 NLL : -12.128 KLD : 0.000 
iteration : 37200 loss : 9.801 NLL : -9.801 KLD : 0.000 
iteration : 37400 loss : 25.237 NLL : -25.237 KLD : 0.000 
iteration : 37600 loss : 14.017 NLL : -14.016 KLD : 0.000 
iteration : 37800 loss : 9.250 NLL : -9.250 KLD : 0.000 
iteration : 38000 loss : 16.915 NLL : -16.915 KLD : 0.000 
iteration : 38200 loss : 15.856 NLL : -15.856 KLD : 0.000 
iteration : 38400 loss : 10.663 NLL : -10.663 KLD : 0.000 
iteration : 38600 loss : 12.671 NLL : -12.671 KLD : 0.000 
iteration : 38800 loss : 15.825 NLL : -15.825 KLD : 0.000 
iteration : 39000 loss : 10.360 NLL : -10.360 KLD : 0.000 
iteration : 39200 loss : 9.876 NLL : -9.876 KLD : 0.000 
iteration : 39400 loss : 11.994 NLL : -11.994 KLD : 0.000 
iteration : 39600 loss : 11.608 NLL : -11.607 KLD : 0.002 
iteration : 39800 loss : 13.378 NLL : -13.377 KLD : 0.000 
iteration : 40000 loss : 15.314 NLL : -15.314 KLD : 0.000 
iteration : 40200 loss : 20.944 NLL : -20.944 KLD : 0.001 
iteration : 40400 loss : 21.372 NLL : -21.372 KLD : 0.000 
iteration : 40600 loss : 18.551 NLL : -18.550 KLD : 0.000 
iteration : 40800 loss : 18.967 NLL : -18.967 KLD : 0.000 
iteration : 41000 loss : 13.898 NLL : -13.898 KLD : 0.000 
iteration : 41200 loss : 9.082 NLL : -9.082 KLD : 0.000 
iteration : 41400 loss : 19.866 NLL : -19.865 KLD : 0.001 
iteration : 41600 loss : 8.183 NLL : -8.182 KLD : 0.000 
iteration : 41800 loss : 20.727 NLL : -20.727 KLD : 0.000 
iteration : 42000 loss : 12.189 NLL : -12.189 KLD : 0.000 
iteration : 42200 loss : 10.185 NLL : -10.185 KLD : 0.000 
iteration : 42400 loss : 15.500 NLL : -15.500 KLD : 0.000 
iteration : 42600 loss : 16.200 NLL : -16.200 KLD : 0.000 
---------- Training loss 7.488 updated ! and save the model! (step:42726) ----------
iteration : 42800 loss : 15.359 NLL : -15.359 KLD : 0.000 
iteration : 43000 loss : 19.971 NLL : -19.971 KLD : 0.000 
iteration : 43200 loss : 13.122 NLL : -13.122 KLD : 0.000 
iteration : 43400 loss : 7.281 NLL : -7.281 KLD : 0.000 
iteration : 43600 loss : 6.837 NLL : -6.836 KLD : 0.001 
iteration : 43800 loss : 18.828 NLL : -18.828 KLD : 0.000 
iteration : 44000 loss : 18.485 NLL : -18.484 KLD : 0.000 
iteration : 44200 loss : 16.478 NLL : -16.478 KLD : 0.000 
iteration : 44400 loss : 23.445 NLL : -23.445 KLD : 0.000 
iteration : 44600 loss : 9.587 NLL : -9.586 KLD : 0.000 
iteration : 44800 loss : 15.409 NLL : -15.409 KLD : 0.000 
iteration : 45000 loss : 10.598 NLL : -10.598 KLD : 0.000 
iteration : 45200 loss : 18.602 NLL : -18.600 KLD : 0.001 
iteration : 45400 loss : 18.363 NLL : -18.362 KLD : 0.001 
iteration : 45600 loss : 10.478 NLL : -10.478 KLD : 0.000 
iteration : 45800 loss : 9.286 NLL : -9.286 KLD : 0.000 
iteration : 46000 loss : 9.649 NLL : -9.649 KLD : 0.000 
iteration : 46200 loss : 5.430 NLL : -5.430 KLD : 0.000 
iteration : 46400 loss : 6.919 NLL : -6.918 KLD : 0.001 
iteration : 46600 loss : 25.844 NLL : -25.843 KLD : 0.001 
iteration : 46800 loss : 14.981 NLL : -14.981 KLD : 0.000 
iteration : 47000 loss : 21.871 NLL : -21.871 KLD : 0.000 
iteration : 47200 loss : 15.790 NLL : -15.790 KLD : 0.000 
iteration : 47400 loss : 13.771 NLL : -13.769 KLD : 0.002 
iteration : 47600 loss : 19.899 NLL : -19.898 KLD : 0.001 
iteration : 47800 loss : 11.715 NLL : -11.715 KLD : 0.000 
iteration : 48000 loss : 16.905 NLL : -16.905 KLD : 0.000 
iteration : 48200 loss : 7.399 NLL : -7.399 KLD : 0.000 
iteration : 48400 loss : 13.754 NLL : -13.754 KLD : 0.000 
iteration : 48600 loss : 16.792 NLL : -16.792 KLD : 0.000 
iteration : 48800 loss : 28.459 NLL : -28.458 KLD : 0.001 
iteration : 49000 loss : 14.291 NLL : -14.291 KLD : 0.000 
iteration : 49200 loss : 9.229 NLL : -9.229 KLD : 0.000 
---------- Training loss 7.417 updated ! and save the model! (step:49248) ----------
iteration : 49400 loss : 13.963 NLL : -13.962 KLD : 0.000 
iteration : 49600 loss : 8.799 NLL : -8.799 KLD : 0.000 
iteration : 49800 loss : 12.673 NLL : -12.673 KLD : 0.000 
iteration : 50000 loss : 21.139 NLL : -21.139 KLD : 0.000 
iteration : 50200 loss : 19.107 NLL : -19.107 KLD : 0.000 
---------- Training loss 7.153 updated ! and save the model! (step:50238) ----------
iteration : 50400 loss : 16.309 NLL : -16.309 KLD : 0.000 
iteration : 50600 loss : 11.673 NLL : -11.673 KLD : 0.000 
iteration : 50800 loss : 4.794 NLL : -4.794 KLD : 0.000 
iteration : 51000 loss : 15.091 NLL : -15.089 KLD : 0.002 
iteration : 51200 loss : 17.698 NLL : -17.694 KLD : 0.004 
iteration : 51400 loss : 26.526 NLL : -26.525 KLD : 0.001 
iteration : 51600 loss : 17.080 NLL : -17.080 KLD : 0.000 
iteration : 51800 loss : 10.908 NLL : -10.908 KLD : 0.000 
iteration : 52000 loss : 9.232 NLL : -9.232 KLD : 0.000 
iteration : 52200 loss : 12.599 NLL : -12.599 KLD : 0.000 
iteration : 52400 loss : 8.496 NLL : -8.496 KLD : 0.000 
iteration : 52600 loss : 18.765 NLL : -18.764 KLD : 0.000 
iteration : 52800 loss : 12.431 NLL : -12.430 KLD : 0.000 
iteration : 53000 loss : 18.329 NLL : -18.328 KLD : 0.000 
iteration : 53200 loss : 12.499 NLL : -12.499 KLD : 0.000 
iteration : 53400 loss : 9.951 NLL : -9.951 KLD : 0.000 
iteration : 53600 loss : 23.124 NLL : -23.124 KLD : 0.000 
iteration : 53800 loss : 9.411 NLL : -9.411 KLD : 0.000 
iteration : 54000 loss : 17.842 NLL : -17.841 KLD : 0.001 
iteration : 54200 loss : 5.966 NLL : -5.965 KLD : 0.000 
iteration : 54400 loss : 15.541 NLL : -15.539 KLD : 0.002 
iteration : 54600 loss : 9.876 NLL : -9.876 KLD : 0.000 
iteration : 54800 loss : 14.134 NLL : -14.134 KLD : 0.000 
iteration : 55000 loss : 6.363 NLL : -6.363 KLD : 0.000 
iteration : 55200 loss : 13.656 NLL : -13.656 KLD : 0.000 
iteration : 55400 loss : 12.337 NLL : -12.336 KLD : 0.000 
iteration : 55600 loss : 24.152 NLL : -24.150 KLD : 0.002 
iteration : 55800 loss : 10.833 NLL : -10.833 KLD : 0.000 
iteration : 56000 loss : 6.624 NLL : -6.624 KLD : 0.000 
iteration : 56200 loss : 14.276 NLL : -14.276 KLD : 0.000 
iteration : 56400 loss : 9.682 NLL : -9.681 KLD : 0.001 
iteration : 56600 loss : 15.022 NLL : -15.021 KLD : 0.001 
iteration : 56800 loss : 16.808 NLL : -16.807 KLD : 0.001 
iteration : 57000 loss : 13.475 NLL : -13.474 KLD : 0.001 
iteration : 57200 loss : 19.110 NLL : -19.110 KLD : 0.000 
iteration : 57400 loss : 11.308 NLL : -11.307 KLD : 0.001 
iteration : 57600 loss : 19.088 NLL : -19.087 KLD : 0.001 
iteration : 57800 loss : 6.448 NLL : -6.448 KLD : 0.000 
iteration : 58000 loss : 15.289 NLL : -15.288 KLD : 0.000 
iteration : 58200 loss : 20.226 NLL : -20.225 KLD : 0.001 
iteration : 58400 loss : 19.206 NLL : -19.206 KLD : 0.000 
iteration : 58600 loss : 19.694 NLL : -19.693 KLD : 0.001 
iteration : 58800 loss : 12.532 NLL : -12.532 KLD : 0.000 
iteration : 59000 loss : 24.612 NLL : -24.611 KLD : 0.000 
iteration : 59200 loss : 12.748 NLL : -12.748 KLD : 0.000 
iteration : 59400 loss : 24.499 NLL : -24.498 KLD : 0.000 
iteration : 59600 loss : 19.383 NLL : -19.383 KLD : 0.000 
iteration : 59800 loss : 17.666 NLL : -17.666 KLD : 0.001 
iteration : 60000 loss : 12.538 NLL : -12.538 KLD : 0.000 
---------- Training loss 14.376 updated ! and save the model! (step:60000) ----------
---------- Training loss 14.268 updated ! and save the model! (step:60006) ----------
---------- Training loss 14.228 updated ! and save the model! (step:60024) ----------
---------- Training loss 13.962 updated ! and save the model! (step:60096) ----------
iteration : 60200 loss : 19.796 NLL : -19.796 KLD : 0.000 
---------- Training loss 13.768 updated ! and save the model! (step:60216) ----------
---------- Training loss 13.497 updated ! and save the model! (step:60282) ----------
iteration : 60400 loss : 8.477 NLL : -8.476 KLD : 0.000 
---------- Training loss 13.366 updated ! and save the model! (step:60510) ----------
---------- Training loss 13.097 updated ! and save the model! (step:60516) ----------
---------- Training loss 11.497 updated ! and save the model! (step:60534) ----------
iteration : 60600 loss : 17.340 NLL : -17.340 KLD : 0.000 
iteration : 60800 loss : 14.768 NLL : -14.768 KLD : 0.000 
---------- Training loss 10.098 updated ! and save the model! (step:60978) ----------
iteration : 61000 loss : 11.305 NLL : -11.304 KLD : 0.000 
iteration : 61200 loss : 20.231 NLL : -20.230 KLD : 0.001 
iteration : 61400 loss : 13.999 NLL : -13.999 KLD : 0.000 
iteration : 61600 loss : 18.227 NLL : -18.215 KLD : 0.013 
---------- Training loss 9.114 updated ! and save the model! (step:61638) ----------
iteration : 61800 loss : 15.589 NLL : -15.588 KLD : 0.001 
iteration : 62000 loss : 12.561 NLL : -12.561 KLD : 0.000 
iteration : 62200 loss : 13.118 NLL : -13.117 KLD : 0.000 
---------- Training loss 8.660 updated ! and save the model! (step:62298) ----------
iteration : 62400 loss : 6.776 NLL : -6.776 KLD : 0.000 
iteration : 62600 loss : 14.262 NLL : -14.261 KLD : 0.000 
iteration : 62800 loss : 13.923 NLL : -13.923 KLD : 0.000 
---------- Training loss 8.380 updated ! and save the model! (step:62874) ----------
iteration : 63000 loss : 12.898 NLL : -12.898 KLD : 0.000 
iteration : 63200 loss : 16.480 NLL : -16.480 KLD : 0.000 
iteration : 63400 loss : 15.524 NLL : -15.524 KLD : 0.000 
iteration : 63600 loss : 16.306 NLL : -16.306 KLD : 0.000 
iteration : 63800 loss : 15.819 NLL : -15.819 KLD : 0.000 
iteration : 64000 loss : 18.049 NLL : -18.049 KLD : 0.000 
iteration : 64200 loss : 12.748 NLL : -12.748 KLD : 0.000 
iteration : 64400 loss : 25.616 NLL : -25.615 KLD : 0.001 
iteration : 64600 loss : 5.032 NLL : -5.031 KLD : 0.000 
iteration : 64800 loss : 14.731 NLL : -14.731 KLD : 0.001 
iteration : 65000 loss : 8.007 NLL : -8.007 KLD : 0.000 
iteration : 65200 loss : 8.727 NLL : -8.727 KLD : 0.000 
iteration : 65400 loss : 16.241 NLL : -16.241 KLD : 0.000 
iteration : 65600 loss : 18.354 NLL : -18.354 KLD : 0.000 
iteration : 65800 loss : 19.503 NLL : -19.503 KLD : 0.000 
iteration : 66000 loss : 14.076 NLL : -14.075 KLD : 0.000 
iteration : 66200 loss : 17.646 NLL : -17.646 KLD : 0.000 
iteration : 66400 loss : 8.378 NLL : -8.378 KLD : 0.000 
iteration : 66600 loss : 24.637 NLL : -24.637 KLD : 0.001 
iteration : 66800 loss : 24.691 NLL : -24.689 KLD : 0.002 
iteration : 67000 loss : 12.306 NLL : -12.306 KLD : 0.000 
iteration : 67200 loss : 16.223 NLL : -16.223 KLD : 0.000 
iteration : 67400 loss : 18.561 NLL : -18.561 KLD : 0.000 
iteration : 67600 loss : 18.864 NLL : -18.864 KLD : 0.000 
iteration : 67800 loss : 10.097 NLL : -10.097 KLD : 0.000 
---------- Training loss 8.233 updated ! and save the model! (step:67884) ----------
iteration : 68000 loss : 8.366 NLL : -8.365 KLD : 0.001 
iteration : 68200 loss : 21.246 NLL : -21.246 KLD : 0.000 
iteration : 68400 loss : 13.417 NLL : -13.416 KLD : 0.000 
iteration : 68600 loss : 6.561 NLL : -6.561 KLD : 0.001 
iteration : 68800 loss : 17.763 NLL : -17.763 KLD : 0.000 
iteration : 69000 loss : 16.162 NLL : -16.161 KLD : 0.000 
iteration : 69200 loss : 18.047 NLL : -18.046 KLD : 0.001 
iteration : 69400 loss : 23.340 NLL : -23.340 KLD : 0.000 
iteration : 69600 loss : 11.720 NLL : -11.720 KLD : 0.000 
iteration : 69800 loss : 11.208 NLL : -11.208 KLD : 0.000 
iteration : 70000 loss : 10.008 NLL : -10.008 KLD : 0.000 
iteration : 70200 loss : 20.907 NLL : -20.907 KLD : 0.000 
iteration : 70400 loss : 21.092 NLL : -21.092 KLD : 0.000 
iteration : 70600 loss : 16.518 NLL : -16.518 KLD : 0.000 
iteration : 70800 loss : 16.244 NLL : -16.244 KLD : 0.000 
iteration : 71000 loss : 20.528 NLL : -20.527 KLD : 0.001 
iteration : 71200 loss : 15.882 NLL : -15.881 KLD : 0.000 
iteration : 71400 loss : 13.096 NLL : -13.096 KLD : 0.000 
iteration : 71600 loss : 18.603 NLL : -18.603 KLD : 0.000 
iteration : 71800 loss : 20.875 NLL : -20.874 KLD : 0.001 
iteration : 72000 loss : 19.724 NLL : -19.723 KLD : 0.000 
iteration : 72200 loss : 23.420 NLL : -23.418 KLD : 0.002 
iteration : 72400 loss : 17.376 NLL : -17.376 KLD : 0.000 
iteration : 72600 loss : 17.509 NLL : -17.509 KLD : 0.000 
iteration : 72800 loss : 21.394 NLL : -21.393 KLD : 0.001 
iteration : 73000 loss : 22.478 NLL : -22.478 KLD : 0.000 
iteration : 73200 loss : 24.075 NLL : -24.075 KLD : 0.000 
iteration : 73400 loss : 26.793 NLL : -26.792 KLD : 0.001 
iteration : 73600 loss : 10.416 NLL : -10.415 KLD : 0.000 
iteration : 73800 loss : 21.072 NLL : -21.072 KLD : 0.000 
iteration : 74000 loss : 14.829 NLL : -14.829 KLD : 0.000 
iteration : 74200 loss : 18.174 NLL : -18.174 KLD : 0.001 
iteration : 74400 loss : 10.009 NLL : -10.008 KLD : 0.000 
iteration : 74600 loss : 20.612 NLL : -20.612 KLD : 0.000 
iteration : 74800 loss : 21.532 NLL : -21.531 KLD : 0.001 
iteration : 75000 loss : 13.285 NLL : -13.285 KLD : 0.000 
iteration : 75200 loss : 15.244 NLL : -15.242 KLD : 0.001 
iteration : 75400 loss : 13.741 NLL : -13.741 KLD : 0.000 
iteration : 75600 loss : 15.442 NLL : -15.442 KLD : 0.000 
iteration : 75800 loss : 11.295 NLL : -11.295 KLD : 0.000 
iteration : 76000 loss : 16.513 NLL : -16.512 KLD : 0.001 
iteration : 76200 loss : 20.669 NLL : -20.668 KLD : 0.000 
iteration : 76400 loss : 10.811 NLL : -10.811 KLD : 0.000 
iteration : 76600 loss : 10.186 NLL : -10.186 KLD : 0.000 
iteration : 76800 loss : 15.578 NLL : -15.577 KLD : 0.001 
iteration : 77000 loss : 14.291 NLL : -14.290 KLD : 0.001 
iteration : 77200 loss : 12.202 NLL : -12.202 KLD : 0.000 
iteration : 77400 loss : 21.471 NLL : -21.469 KLD : 0.002 
iteration : 77600 loss : 13.305 NLL : -13.305 KLD : 0.000 
iteration : 77800 loss : 25.642 NLL : -25.641 KLD : 0.001 
iteration : 78000 loss : 14.813 NLL : -14.812 KLD : 0.000 
iteration : 78200 loss : 8.372 NLL : -8.372 KLD : 0.000 
iteration : 78400 loss : 11.595 NLL : -11.595 KLD : 0.000 
iteration : 78600 loss : 14.332 NLL : -14.332 KLD : 0.000 
iteration : 78800 loss : 16.445 NLL : -16.445 KLD : 0.000 
iteration : 79000 loss : 17.037 NLL : -17.036 KLD : 0.000 
iteration : 79200 loss : 17.174 NLL : -17.173 KLD : 0.001 
iteration : 79400 loss : 18.811 NLL : -18.810 KLD : 0.001 
iteration : 79600 loss : 24.476 NLL : -24.472 KLD : 0.003 
iteration : 79800 loss : 20.921 NLL : -20.919 KLD : 0.001 
iteration : 80000 loss : 15.631 NLL : -15.626 KLD : 0.005 
iteration : 80200 loss : 10.690 NLL : -10.690 KLD : 0.000 
iteration : 80400 loss : 18.762 NLL : -18.762 KLD : 0.000 
iteration : 80600 loss : 8.027 NLL : -8.026 KLD : 0.000 
iteration : 80800 loss : 10.637 NLL : -10.637 KLD : 0.000 
iteration : 81000 loss : 14.724 NLL : -14.724 KLD : 0.000 
iteration : 81200 loss : 17.778 NLL : -17.777 KLD : 0.001 
iteration : 81400 loss : 16.632 NLL : -16.631 KLD : 0.001 
iteration : 81600 loss : 9.243 NLL : -9.243 KLD : 0.000 
iteration : 81800 loss : 13.458 NLL : -13.458 KLD : 0.000 
iteration : 82000 loss : 11.088 NLL : -11.086 KLD : 0.001 
iteration : 82200 loss : 9.714 NLL : -9.714 KLD : 0.000 
iteration : 82400 loss : 6.009 NLL : -6.009 KLD : 0.000 
iteration : 82600 loss : 21.788 NLL : -21.785 KLD : 0.003 
iteration : 82800 loss : 14.015 NLL : -14.015 KLD : 0.000 
iteration : 83000 loss : 8.491 NLL : -8.491 KLD : 0.000 
iteration : 83200 loss : 9.306 NLL : -9.305 KLD : 0.000 
iteration : 83400 loss : 21.393 NLL : -21.392 KLD : 0.001 
iteration : 83600 loss : 19.378 NLL : -19.378 KLD : 0.000 
iteration : 83800 loss : 10.799 NLL : -10.798 KLD : 0.000 
iteration : 84000 loss : 9.749 NLL : -9.748 KLD : 0.000 
iteration : 84200 loss : 12.672 NLL : -12.671 KLD : 0.000 
---------- Training loss 8.176 updated ! and save the model! (step:84342) ----------
iteration : 84400 loss : 11.042 NLL : -11.041 KLD : 0.000 
iteration : 84600 loss : 23.242 NLL : -23.240 KLD : 0.002 
iteration : 84800 loss : 12.898 NLL : -12.896 KLD : 0.001 
iteration : 85000 loss : 13.614 NLL : -13.613 KLD : 0.001 
iteration : 85200 loss : 14.183 NLL : -14.182 KLD : 0.001 
iteration : 85400 loss : 14.571 NLL : -14.569 KLD : 0.002 
iteration : 85600 loss : 7.870 NLL : -7.870 KLD : 0.000 
iteration : 85800 loss : 13.905 NLL : -13.904 KLD : 0.000 
iteration : 86000 loss : 15.655 NLL : -15.654 KLD : 0.000 
iteration : 86200 loss : 16.533 NLL : -16.532 KLD : 0.001 
iteration : 86400 loss : 15.035 NLL : -15.029 KLD : 0.006 
iteration : 86600 loss : 14.691 NLL : -14.690 KLD : 0.001 
iteration : 86800 loss : 8.863 NLL : -8.863 KLD : 0.000 
---------- Training loss 8.037 updated ! and save the model! (step:86946) ----------
iteration : 87000 loss : 9.347 NLL : -9.347 KLD : 0.000 
iteration : 87200 loss : 15.697 NLL : -15.693 KLD : 0.004 
iteration : 87400 loss : 5.385 NLL : -5.384 KLD : 0.001 
iteration : 87600 loss : 14.652 NLL : -14.652 KLD : 0.000 
iteration : 87800 loss : 16.665 NLL : -16.665 KLD : 0.000 
---------- Training loss 7.350 updated ! and save the model! (step:87864) ----------
iteration : 88000 loss : 13.956 NLL : -13.956 KLD : 0.000 
iteration : 88200 loss : 8.840 NLL : -8.839 KLD : 0.001 
---------- Training loss 6.838 updated ! and save the model! (step:88308) ----------
iteration : 88400 loss : 20.947 NLL : -20.945 KLD : 0.002 
iteration : 88600 loss : 24.838 NLL : -24.830 KLD : 0.009 
iteration : 88800 loss : 15.124 NLL : -15.123 KLD : 0.001 
iteration : 89000 loss : 16.461 NLL : -16.460 KLD : 0.001 
iteration : 89200 loss : 16.274 NLL : -16.273 KLD : 0.000 
iteration : 89400 loss : 9.689 NLL : -9.689 KLD : 0.000 
iteration : 89600 loss : 14.026 NLL : -14.025 KLD : 0.001 
iteration : 89800 loss : 10.207 NLL : -10.204 KLD : 0.004 
iteration : 90000 loss : 13.040 NLL : -13.039 KLD : 0.000 
---------- Training loss 15.718 updated ! and save the model! (step:90000) ----------
---------- Training loss 11.367 updated ! and save the model! (step:90006) ----------
---------- Training loss 9.587 updated ! and save the model! (step:90024) ----------
iteration : 90200 loss : 5.654 NLL : -5.653 KLD : 0.000 
iteration : 90400 loss : 14.856 NLL : -14.856 KLD : 0.000 
iteration : 90600 loss : 21.358 NLL : -21.357 KLD : 0.001 
---------- Training loss 6.139 updated ! and save the model! (step:90780) ----------
iteration : 90800 loss : 4.404 NLL : -4.403 KLD : 0.000 
iteration : 91000 loss : 11.663 NLL : -11.663 KLD : 0.000 
iteration : 91200 loss : 15.089 NLL : -15.087 KLD : 0.002 
iteration : 91400 loss : 11.758 NLL : -11.758 KLD : 0.000 
iteration : 91600 loss : 10.827 NLL : -10.826 KLD : 0.000 
iteration : 91800 loss : 13.042 NLL : -13.042 KLD : 0.001 
iteration : 92000 loss : 7.631 NLL : -7.627 KLD : 0.004 
iteration : 92200 loss : 8.766 NLL : -8.765 KLD : 0.000 
iteration : 92400 loss : 18.756 NLL : -18.755 KLD : 0.001 
iteration : 92600 loss : 12.236 NLL : -12.236 KLD : 0.000 
iteration : 92800 loss : 20.138 NLL : -20.134 KLD : 0.003 
iteration : 93000 loss : 13.042 NLL : -13.041 KLD : 0.000 
iteration : 93200 loss : 12.411 NLL : -12.411 KLD : 0.000 
iteration : 93400 loss : 6.549 NLL : -6.548 KLD : 0.001 
iteration : 93600 loss : 9.117 NLL : -9.117 KLD : 0.000 
iteration : 93800 loss : 10.137 NLL : -10.137 KLD : 0.001 
iteration : 94000 loss : 16.671 NLL : -16.671 KLD : 0.000 
iteration : 94200 loss : 11.142 NLL : -11.141 KLD : 0.001 
iteration : 94400 loss : 19.686 NLL : -19.684 KLD : 0.003 
iteration : 94600 loss : 8.701 NLL : -8.701 KLD : 0.000 
iteration : 94800 loss : 13.180 NLL : -13.180 KLD : 0.000 
iteration : 95000 loss : 7.685 NLL : -7.684 KLD : 0.001 
iteration : 95200 loss : 12.055 NLL : -12.055 KLD : 0.000 
iteration : 95400 loss : 13.330 NLL : -13.329 KLD : 0.000 
iteration : 95600 loss : 10.167 NLL : -10.167 KLD : 0.000 
iteration : 95800 loss : 8.117 NLL : -8.117 KLD : 0.000 
iteration : 96000 loss : 9.415 NLL : -9.414 KLD : 0.001 
iteration : 96200 loss : 24.329 NLL : -24.323 KLD : 0.005 
iteration : 96400 loss : 17.847 NLL : -17.847 KLD : 0.001 
iteration : 96600 loss : 21.097 NLL : -21.095 KLD : 0.001 
iteration : 96800 loss : 13.782 NLL : -13.782 KLD : 0.000 
iteration : 97000 loss : 11.861 NLL : -11.860 KLD : 0.001 
iteration : 97200 loss : 10.696 NLL : -10.695 KLD : 0.000 
iteration : 97400 loss : 4.351 NLL : -4.350 KLD : 0.001 
iteration : 97600 loss : 23.188 NLL : -23.188 KLD : 0.000 
iteration : 97800 loss : 8.510 NLL : -8.510 KLD : 0.000 
iteration : 98000 loss : 5.969 NLL : -5.968 KLD : 0.001 
iteration : 98200 loss : 12.258 NLL : -12.258 KLD : 0.000 
iteration : 98400 loss : 14.042 NLL : -14.040 KLD : 0.002 
iteration : 98600 loss : 23.426 NLL : -23.425 KLD : 0.001 
iteration : 98800 loss : 13.689 NLL : -13.688 KLD : 0.001 
iteration : 99000 loss : 11.832 NLL : -11.832 KLD : 0.000 
iteration : 99200 loss : 5.577 NLL : -5.560 KLD : 0.016 
iteration : 99400 loss : 13.630 NLL : -13.630 KLD : 0.000 
iteration : 99600 loss : 17.498 NLL : -17.497 KLD : 0.001 
iteration : 99800 loss : 8.443 NLL : -8.443 KLD : 0.000 
iteration : 100000 loss : 18.093 NLL : -18.092 KLD : 0.001 
iteration : 100200 loss : 10.265 NLL : -10.265 KLD : 0.000 
iteration : 100400 loss : 7.326 NLL : -7.325 KLD : 0.000 
iteration : 100600 loss : 19.038 NLL : -19.038 KLD : 0.000 
iteration : 100800 loss : 6.372 NLL : -6.372 KLD : 0.000 
iteration : 101000 loss : 12.233 NLL : -12.232 KLD : 0.001 
iteration : 101200 loss : 16.054 NLL : -16.049 KLD : 0.005 
iteration : 101400 loss : 8.837 NLL : -8.835 KLD : 0.001 
iteration : 101600 loss : 9.168 NLL : -9.167 KLD : 0.001 
iteration : 101800 loss : 12.203 NLL : -12.203 KLD : 0.001 
iteration : 102000 loss : 11.911 NLL : -11.910 KLD : 0.001 
iteration : 102200 loss : 13.033 NLL : -13.033 KLD : 0.000 
iteration : 102400 loss : 6.653 NLL : -6.653 KLD : 0.001 
iteration : 102600 loss : 21.802 NLL : -21.800 KLD : 0.002 
iteration : 102800 loss : 8.577 NLL : -8.577 KLD : 0.000 
iteration : 103000 loss : 8.703 NLL : -8.703 KLD : 0.000 
iteration : 103200 loss : 15.522 NLL : -15.522 KLD : 0.000 
iteration : 103400 loss : 11.905 NLL : -11.905 KLD : 0.000 
iteration : 103600 loss : 8.020 NLL : -8.020 KLD : 0.000 
iteration : 103800 loss : 12.873 NLL : -12.873 KLD : 0.001 
iteration : 104000 loss : 5.476 NLL : -5.476 KLD : 0.000 
iteration : 104200 loss : 18.638 NLL : -18.637 KLD : 0.001 
iteration : 104400 loss : 14.039 NLL : -14.039 KLD : 0.000 
iteration : 104600 loss : 9.761 NLL : -9.761 KLD : 0.001 
iteration : 104800 loss : 10.976 NLL : -10.976 KLD : 0.000 
iteration : 105000 loss : 17.743 NLL : -17.743 KLD : 0.000 
iteration : 105200 loss : 6.649 NLL : -6.646 KLD : 0.002 
iteration : 105400 loss : 18.266 NLL : -18.259 KLD : 0.007 
iteration : 105600 loss : 17.224 NLL : -17.224 KLD : 0.001 
iteration : 105800 loss : 11.222 NLL : -11.220 KLD : 0.002 
iteration : 106000 loss : 20.642 NLL : -20.641 KLD : 0.001 
iteration : 106200 loss : 9.971 NLL : -9.971 KLD : 0.000 
iteration : 106400 loss : 6.972 NLL : -6.972 KLD : 0.000 
iteration : 106600 loss : 26.292 NLL : -26.287 KLD : 0.005 
iteration : 106800 loss : 18.071 NLL : -18.070 KLD : 0.000 
iteration : 107000 loss : 14.148 NLL : -14.148 KLD : 0.000 
iteration : 107200 loss : 15.804 NLL : -15.803 KLD : 0.001 
iteration : 107400 loss : 21.879 NLL : -21.870 KLD : 0.009 
iteration : 107600 loss : 16.940 NLL : -16.938 KLD : 0.002 
iteration : 107800 loss : 18.025 NLL : -18.024 KLD : 0.000 
iteration : 108000 loss : 9.912 NLL : -9.909 KLD : 0.003 
iteration : 108200 loss : 8.815 NLL : -8.814 KLD : 0.001 
iteration : 108400 loss : 19.262 NLL : -19.260 KLD : 0.002 
iteration : 108600 loss : 9.535 NLL : -9.533 KLD : 0.002 
iteration : 108800 loss : 19.815 NLL : -19.815 KLD : 0.000 
iteration : 109000 loss : 16.626 NLL : -16.626 KLD : 0.001 
iteration : 109200 loss : 17.761 NLL : -17.760 KLD : 0.000 
iteration : 109400 loss : 12.126 NLL : -12.124 KLD : 0.001 
iteration : 109600 loss : 18.306 NLL : -18.305 KLD : 0.001 
iteration : 109800 loss : 10.487 NLL : -10.486 KLD : 0.001 
iteration : 110000 loss : 7.810 NLL : -7.810 KLD : 0.000 
iteration : 110200 loss : 11.642 NLL : -11.641 KLD : 0.000 
iteration : 110400 loss : 12.066 NLL : -12.066 KLD : 0.000 
iteration : 110600 loss : 11.416 NLL : -11.415 KLD : 0.000 
iteration : 110800 loss : 16.533 NLL : -16.533 KLD : 0.000 
iteration : 111000 loss : 19.817 NLL : -19.816 KLD : 0.001 
iteration : 111200 loss : 16.152 NLL : -16.152 KLD : 0.000 
iteration : 111400 loss : 17.007 NLL : -17.007 KLD : 0.000 
iteration : 111600 loss : 9.824 NLL : -9.824 KLD : 0.000 
iteration : 111800 loss : 19.811 NLL : -19.808 KLD : 0.003 
iteration : 112000 loss : 14.910 NLL : -14.908 KLD : 0.002 
iteration : 112200 loss : 12.649 NLL : -12.644 KLD : 0.005 
iteration : 112400 loss : 19.656 NLL : -19.655 KLD : 0.000 
iteration : 112600 loss : 21.766 NLL : -21.765 KLD : 0.001 
iteration : 112800 loss : 16.042 NLL : -16.041 KLD : 0.000 
iteration : 113000 loss : 12.717 NLL : -12.717 KLD : 0.000 
iteration : 113200 loss : 12.030 NLL : -12.030 KLD : 0.000 
iteration : 113400 loss : 20.563 NLL : -20.562 KLD : 0.000 
iteration : 113600 loss : 14.653 NLL : -14.653 KLD : 0.000 
iteration : 113800 loss : 16.633 NLL : -16.633 KLD : 0.000 
iteration : 114000 loss : 10.023 NLL : -10.023 KLD : 0.000 
iteration : 114200 loss : 18.501 NLL : -18.501 KLD : 0.001 
iteration : 114400 loss : 17.108 NLL : -17.108 KLD : 0.000 
iteration : 114600 loss : 11.635 NLL : -11.635 KLD : 0.001 
iteration : 114800 loss : 6.285 NLL : -6.284 KLD : 0.001 
iteration : 115000 loss : 19.329 NLL : -19.327 KLD : 0.002 
iteration : 115200 loss : 20.305 NLL : -20.304 KLD : 0.001 
iteration : 115400 loss : 21.363 NLL : -21.362 KLD : 0.000 
iteration : 115600 loss : 21.982 NLL : -21.982 KLD : 0.000 
iteration : 115800 loss : 12.420 NLL : -12.419 KLD : 0.001 
iteration : 116000 loss : 12.618 NLL : -12.617 KLD : 0.002 
iteration : 116200 loss : 20.020 NLL : -20.019 KLD : 0.001 
iteration : 116400 loss : 12.748 NLL : -12.747 KLD : 0.000 
iteration : 116600 loss : 21.265 NLL : -21.264 KLD : 0.001 
iteration : 116800 loss : 19.672 NLL : -19.671 KLD : 0.001 
iteration : 117000 loss : 11.029 NLL : -11.029 KLD : 0.000 
iteration : 117200 loss : 15.399 NLL : -15.396 KLD : 0.003 
iteration : 117400 loss : 15.900 NLL : -15.900 KLD : 0.000 
iteration : 117600 loss : 16.947 NLL : -16.947 KLD : 0.000 
iteration : 117800 loss : 10.388 NLL : -10.387 KLD : 0.001 
iteration : 118000 loss : 6.014 NLL : -6.012 KLD : 0.002 
iteration : 118200 loss : 17.132 NLL : -17.131 KLD : 0.000 
iteration : 118400 loss : 16.859 NLL : -16.859 KLD : 0.000 
iteration : 118600 loss : 15.901 NLL : -15.901 KLD : 0.000 
iteration : 118800 loss : 19.330 NLL : -19.328 KLD : 0.002 
iteration : 119000 loss : 8.175 NLL : -8.175 KLD : 0.000 
iteration : 119200 loss : 9.817 NLL : -9.817 KLD : 0.000 
iteration : 119400 loss : 13.086 NLL : -13.085 KLD : 0.001 
iteration : 119600 loss : 18.143 NLL : -18.143 KLD : 0.000 
iteration : 119800 loss : 20.464 NLL : -20.464 KLD : 0.000 
iteration : 120000 loss : 8.965 NLL : -8.963 KLD : 0.001 
---------- Training loss 13.292 updated ! and save the model! (step:120000) ----------
iteration : 120200 loss : 17.044 NLL : -17.042 KLD : 0.002 
---------- Training loss 11.966 updated ! and save the model! (step:120240) ----------
iteration : 120400 loss : 23.180 NLL : -23.179 KLD : 0.001 
iteration : 120600 loss : 16.954 NLL : -16.954 KLD : 0.000 
iteration : 120800 loss : 14.730 NLL : -14.730 KLD : 0.000 
iteration : 121000 loss : 9.719 NLL : -9.718 KLD : 0.001 
---------- Training loss 11.062 updated ! and save the model! (step:121062) ----------
iteration : 121200 loss : 12.947 NLL : -12.947 KLD : 0.000 
iteration : 121400 loss : 12.127 NLL : -12.126 KLD : 0.001 
iteration : 121600 loss : 16.330 NLL : -16.329 KLD : 0.001 
iteration : 121800 loss : 11.713 NLL : -11.701 KLD : 0.013 
iteration : 122000 loss : 15.832 NLL : -15.830 KLD : 0.001 
iteration : 122200 loss : 21.306 NLL : -21.306 KLD : 0.000 
iteration : 122400 loss : 22.013 NLL : -22.013 KLD : 0.000 
iteration : 122600 loss : 20.917 NLL : -20.915 KLD : 0.002 
iteration : 122800 loss : 17.202 NLL : -17.201 KLD : 0.001 
iteration : 123000 loss : 21.067 NLL : -21.066 KLD : 0.000 
iteration : 123200 loss : 13.089 NLL : -13.087 KLD : 0.003 
iteration : 123400 loss : 17.851 NLL : -17.851 KLD : 0.000 
iteration : 123600 loss : 13.897 NLL : -13.891 KLD : 0.006 
iteration : 123800 loss : 13.955 NLL : -13.954 KLD : 0.001 
iteration : 124000 loss : 8.753 NLL : -8.752 KLD : 0.000 
iteration : 124200 loss : 10.376 NLL : -10.374 KLD : 0.002 
iteration : 124400 loss : 13.070 NLL : -13.068 KLD : 0.002 
iteration : 124600 loss : 17.950 NLL : -17.949 KLD : 0.001 
iteration : 124800 loss : 16.206 NLL : -16.204 KLD : 0.002 
iteration : 125000 loss : 15.776 NLL : -15.775 KLD : 0.000 
iteration : 125200 loss : 21.325 NLL : -21.325 KLD : 0.001 
iteration : 125400 loss : 20.080 NLL : -20.079 KLD : 0.002 
iteration : 125600 loss : 7.471 NLL : -7.465 KLD : 0.006 
iteration : 125800 loss : 20.471 NLL : -20.469 KLD : 0.002 
iteration : 126000 loss : 19.370 NLL : -19.369 KLD : 0.001 
iteration : 126200 loss : 20.192 NLL : -20.192 KLD : 0.000 
iteration : 126400 loss : 15.524 NLL : -15.523 KLD : 0.001 
iteration : 126600 loss : 20.931 NLL : -20.930 KLD : 0.000 
iteration : 126800 loss : 23.856 NLL : -23.840 KLD : 0.016 
iteration : 127000 loss : 10.443 NLL : -10.441 KLD : 0.002 
iteration : 127200 loss : 17.609 NLL : -17.608 KLD : 0.001 
iteration : 127400 loss : 5.612 NLL : -5.609 KLD : 0.003 
iteration : 127600 loss : 6.723 NLL : -6.722 KLD : 0.001 
iteration : 127800 loss : 18.182 NLL : -18.155 KLD : 0.027 
iteration : 128000 loss : 12.555 NLL : -12.553 KLD : 0.002 
---------- Training loss 10.782 updated ! and save the model! (step:128082) ----------
iteration : 128200 loss : 19.048 NLL : -19.047 KLD : 0.001 
iteration : 128400 loss : 16.307 NLL : -16.304 KLD : 0.002 
---------- Training loss 9.908 updated ! and save the model! (step:128538) ----------
iteration : 128600 loss : 17.280 NLL : -17.280 KLD : 0.000 
iteration : 128800 loss : 12.101 NLL : -12.100 KLD : 0.001 
iteration : 129000 loss : 17.100 NLL : -17.096 KLD : 0.003 
iteration : 129200 loss : 13.565 NLL : -13.560 KLD : 0.005 
iteration : 129400 loss : 17.032 NLL : -17.029 KLD : 0.003 
iteration : 129600 loss : 12.513 NLL : -12.509 KLD : 0.004 
iteration : 129800 loss : 20.100 NLL : -20.100 KLD : 0.000 
iteration : 130000 loss : 17.434 NLL : -17.434 KLD : 0.000 
iteration : 130200 loss : 18.381 NLL : -18.381 KLD : 0.001 
---------- Training loss 9.409 updated ! and save the model! (step:130284) ----------
iteration : 130400 loss : 17.129 NLL : -17.126 KLD : 0.004 
iteration : 130600 loss : 4.603 NLL : -4.602 KLD : 0.001 
iteration : 130800 loss : 18.569 NLL : -18.567 KLD : 0.002 
iteration : 131000 loss : 7.188 NLL : -7.181 KLD : 0.007 
iteration : 131200 loss : 15.105 NLL : -15.095 KLD : 0.010 
iteration : 131400 loss : 19.462 NLL : -19.461 KLD : 0.001 
iteration : 131600 loss : 8.267 NLL : -8.265 KLD : 0.002 
iteration : 131800 loss : 16.748 NLL : -16.747 KLD : 0.001 
iteration : 132000 loss : 12.252 NLL : -12.252 KLD : 0.000 
---------- Training loss 8.639 updated ! and save the model! (step:132042) ----------
iteration : 132200 loss : 17.847 NLL : -17.846 KLD : 0.002 
iteration : 132400 loss : 7.876 NLL : -7.865 KLD : 0.011 
iteration : 132600 loss : 10.746 NLL : -10.744 KLD : 0.002 
iteration : 132800 loss : 19.657 NLL : -19.656 KLD : 0.001 
iteration : 133000 loss : 5.016 NLL : -5.016 KLD : 0.000 
iteration : 133200 loss : 15.451 NLL : -15.450 KLD : 0.000 
iteration : 133400 loss : 18.691 NLL : -18.691 KLD : 0.001 
iteration : 133600 loss : 14.706 NLL : -14.705 KLD : 0.001 
iteration : 133800 loss : 5.969 NLL : -5.967 KLD : 0.002 
iteration : 134000 loss : 21.154 NLL : -21.149 KLD : 0.005 
iteration : 134200 loss : 14.435 NLL : -14.433 KLD : 0.001 
iteration : 134400 loss : 24.408 NLL : -24.404 KLD : 0.005 
iteration : 134600 loss : 10.697 NLL : -10.695 KLD : 0.001 
iteration : 134800 loss : 8.078 NLL : -8.077 KLD : 0.000 
iteration : 135000 loss : 10.081 NLL : -10.080 KLD : 0.001 
iteration : 135200 loss : 9.698 NLL : -9.697 KLD : 0.001 
iteration : 135400 loss : 24.328 NLL : -24.323 KLD : 0.006 
iteration : 135600 loss : 16.143 NLL : -16.142 KLD : 0.001 
iteration : 135800 loss : 17.371 NLL : -17.369 KLD : 0.002 
iteration : 136000 loss : 11.633 NLL : -11.632 KLD : 0.000 
iteration : 136200 loss : 9.863 NLL : -9.862 KLD : 0.001 
iteration : 136400 loss : 17.997 NLL : -17.997 KLD : 0.000 
iteration : 136600 loss : 19.282 NLL : -19.277 KLD : 0.005 
iteration : 136800 loss : 21.800 NLL : -21.798 KLD : 0.002 
iteration : 137000 loss : 14.162 NLL : -14.158 KLD : 0.003 
iteration : 137200 loss : 16.656 NLL : -16.655 KLD : 0.001 
iteration : 137400 loss : 12.193 NLL : -12.191 KLD : 0.001 
iteration : 137600 loss : 20.389 NLL : -20.388 KLD : 0.001 
iteration : 137800 loss : 11.437 NLL : -11.431 KLD : 0.007 
iteration : 138000 loss : 16.440 NLL : -16.438 KLD : 0.002 
iteration : 138200 loss : 16.055 NLL : -16.052 KLD : 0.003 
iteration : 138400 loss : 17.145 NLL : -17.140 KLD : 0.005 
iteration : 138600 loss : 7.573 NLL : -7.565 KLD : 0.008 
iteration : 138800 loss : 11.287 NLL : -11.286 KLD : 0.001 
iteration : 139000 loss : 23.416 NLL : -23.408 KLD : 0.007 
iteration : 139200 loss : 5.887 NLL : -5.886 KLD : 0.000 
iteration : 139400 loss : 13.155 NLL : -13.155 KLD : 0.000 
iteration : 139600 loss : 20.579 NLL : -20.576 KLD : 0.003 
iteration : 139800 loss : 11.013 NLL : -11.011 KLD : 0.001 
iteration : 140000 loss : 4.686 NLL : -4.684 KLD : 0.002 
iteration : 140200 loss : 16.515 NLL : -16.512 KLD : 0.003 
iteration : 140400 loss : 4.439 NLL : -4.435 KLD : 0.005 
iteration : 140600 loss : 16.611 NLL : -16.608 KLD : 0.003 
iteration : 140800 loss : 6.162 NLL : -6.160 KLD : 0.002 
iteration : 141000 loss : 12.477 NLL : -12.477 KLD : 0.000 
iteration : 141200 loss : 20.962 NLL : -20.957 KLD : 0.005 
iteration : 141400 loss : 17.592 NLL : -17.592 KLD : 0.001 
iteration : 141600 loss : 21.610 NLL : -21.607 KLD : 0.003 
iteration : 141800 loss : 18.216 NLL : -18.211 KLD : 0.006 
---------- Training loss 7.754 updated ! and save the model! (step:141864) ----------
iteration : 142000 loss : 16.409 NLL : -16.406 KLD : 0.003 
iteration : 142200 loss : 14.726 NLL : -14.724 KLD : 0.002 
iteration : 142400 loss : 8.685 NLL : -8.680 KLD : 0.005 
iteration : 142600 loss : 14.543 NLL : -14.543 KLD : 0.001 
iteration : 142800 loss : 7.866 NLL : -7.865 KLD : 0.001 
iteration : 143000 loss : 11.555 NLL : -11.555 KLD : 0.001 
iteration : 143200 loss : 14.867 NLL : -14.866 KLD : 0.000 
iteration : 143400 loss : 19.095 NLL : -19.093 KLD : 0.002 
iteration : 143600 loss : 14.767 NLL : -14.765 KLD : 0.002 
iteration : 143800 loss : 10.810 NLL : -10.810 KLD : 0.000 
iteration : 144000 loss : 24.866 NLL : -24.863 KLD : 0.003 
iteration : 144200 loss : 13.538 NLL : -13.537 KLD : 0.001 
iteration : 144400 loss : 22.433 NLL : -22.432 KLD : 0.001 
iteration : 144600 loss : 12.881 NLL : -12.879 KLD : 0.002 
iteration : 144800 loss : 19.219 NLL : -19.211 KLD : 0.008 
iteration : 145000 loss : 16.419 NLL : -16.416 KLD : 0.003 
iteration : 145200 loss : 7.174 NLL : -7.167 KLD : 0.007 
iteration : 145400 loss : 9.683 NLL : -9.678 KLD : 0.005 
iteration : 145600 loss : 9.908 NLL : -9.906 KLD : 0.002 
iteration : 145800 loss : 21.998 NLL : -21.993 KLD : 0.005 
iteration : 146000 loss : 9.841 NLL : -9.840 KLD : 0.001 
iteration : 146200 loss : 21.290 NLL : -21.289 KLD : 0.002 
iteration : 146400 loss : 18.473 NLL : -18.469 KLD : 0.004 
iteration : 146600 loss : 24.199 NLL : -24.186 KLD : 0.013 
iteration : 146800 loss : 14.710 NLL : -14.710 KLD : 0.000 
iteration : 147000 loss : 13.018 NLL : -13.017 KLD : 0.001 
iteration : 147200 loss : 16.900 NLL : -16.897 KLD : 0.002 
iteration : 147400 loss : 12.498 NLL : -12.498 KLD : 0.001 
iteration : 147600 loss : 22.434 NLL : -22.432 KLD : 0.003 
iteration : 147800 loss : 22.699 NLL : -22.698 KLD : 0.001 
iteration : 148000 loss : 11.877 NLL : -11.876 KLD : 0.001 
iteration : 148200 loss : 17.793 NLL : -17.793 KLD : 0.001 
iteration : 148400 loss : 13.856 NLL : -13.853 KLD : 0.003 
iteration : 148600 loss : 11.839 NLL : -11.838 KLD : 0.001 
iteration : 148800 loss : 17.769 NLL : -17.767 KLD : 0.002 
iteration : 149000 loss : 16.422 NLL : -16.420 KLD : 0.002 
iteration : 149200 loss : 9.500 NLL : -9.498 KLD : 0.003 
iteration : 149400 loss : 12.144 NLL : -12.144 KLD : 0.000 
iteration : 149600 loss : 17.696 NLL : -17.672 KLD : 0.024 
iteration : 149800 loss : 15.125 NLL : -15.119 KLD : 0.006 
iteration : 150000 loss : 18.260 NLL : -18.257 KLD : 0.002 
---------- Training loss 15.238 updated ! and save the model! (step:150000) ----------
---------- Training loss 12.716 updated ! and save the model! (step:150012) ----------
---------- Training loss 12.441 updated ! and save the model! (step:150024) ----------
---------- Training loss 11.125 updated ! and save the model! (step:150060) ----------
---------- Training loss 10.835 updated ! and save the model! (step:150168) ----------
iteration : 150200 loss : 8.343 NLL : -8.341 KLD : 0.002 
iteration : 150400 loss : 11.452 NLL : -11.451 KLD : 0.001 
---------- Training loss 10.599 updated ! and save the model! (step:150402) ----------
iteration : 150600 loss : 17.835 NLL : -17.834 KLD : 0.001 
---------- Training loss 10.545 updated ! and save the model! (step:150768) ----------
iteration : 150800 loss : 13.075 NLL : -13.070 KLD : 0.005 
---------- Training loss 9.390 updated ! and save the model! (step:150852) ----------
---------- Training loss 9.023 updated ! and save the model! (step:150996) ----------
iteration : 151000 loss : 25.117 NLL : -25.107 KLD : 0.010 
---------- Training loss 8.399 updated ! and save the model! (step:151158) ----------
iteration : 151200 loss : 4.968 NLL : -4.967 KLD : 0.001 
iteration : 151400 loss : 11.044 NLL : -11.043 KLD : 0.001 
iteration : 151600 loss : 8.804 NLL : -8.803 KLD : 0.001 
iteration : 151800 loss : 13.837 NLL : -13.835 KLD : 0.002 
iteration : 152000 loss : 18.562 NLL : -18.559 KLD : 0.003 
iteration : 152200 loss : 16.768 NLL : -16.760 KLD : 0.008 
iteration : 152400 loss : 12.302 NLL : -12.301 KLD : 0.001 
iteration : 152600 loss : 9.998 NLL : -9.998 KLD : 0.000 
iteration : 152800 loss : 15.184 NLL : -15.179 KLD : 0.005 
iteration : 153000 loss : 10.024 NLL : -10.024 KLD : 0.000 
iteration : 153200 loss : 11.797 NLL : -11.796 KLD : 0.001 
iteration : 153400 loss : 8.587 NLL : -8.587 KLD : 0.001 
iteration : 153600 loss : 22.290 NLL : -22.277 KLD : 0.014 
iteration : 153800 loss : 11.364 NLL : -11.362 KLD : 0.003 
iteration : 154000 loss : 8.587 NLL : -8.585 KLD : 0.002 
iteration : 154200 loss : 11.711 NLL : -11.707 KLD : 0.004 
iteration : 154400 loss : 19.001 NLL : -18.996 KLD : 0.006 
iteration : 154600 loss : 17.312 NLL : -17.312 KLD : 0.000 
iteration : 154800 loss : 18.667 NLL : -18.660 KLD : 0.006 
iteration : 155000 loss : 13.182 NLL : -13.182 KLD : 0.001 
iteration : 155200 loss : 16.973 NLL : -16.971 KLD : 0.002 
iteration : 155400 loss : 20.657 NLL : -20.653 KLD : 0.004 
iteration : 155600 loss : 16.874 NLL : -16.872 KLD : 0.001 
iteration : 155800 loss : 14.472 NLL : -14.470 KLD : 0.002 
iteration : 156000 loss : 6.132 NLL : -6.131 KLD : 0.001 
iteration : 156200 loss : 20.194 NLL : -20.177 KLD : 0.017 
iteration : 156400 loss : 18.332 NLL : -18.323 KLD : 0.009 
iteration : 156600 loss : 10.515 NLL : -10.512 KLD : 0.003 
iteration : 156800 loss : 13.765 NLL : -13.764 KLD : 0.001 
iteration : 157000 loss : 9.222 NLL : -9.220 KLD : 0.002 
iteration : 157200 loss : 11.545 NLL : -11.544 KLD : 0.001 
iteration : 157400 loss : 21.420 NLL : -21.413 KLD : 0.007 
iteration : 157600 loss : 16.483 NLL : -16.481 KLD : 0.001 
iteration : 157800 loss : 9.267 NLL : -9.267 KLD : 0.000 
iteration : 158000 loss : 18.241 NLL : -18.236 KLD : 0.005 
iteration : 158200 loss : 13.005 NLL : -13.005 KLD : 0.000 
iteration : 158400 loss : 13.569 NLL : -13.568 KLD : 0.002 
iteration : 158600 loss : 19.741 NLL : -19.740 KLD : 0.001 
iteration : 158800 loss : 18.087 NLL : -18.079 KLD : 0.008 
iteration : 159000 loss : 14.783 NLL : -14.783 KLD : 0.001 
iteration : 159200 loss : 24.191 NLL : -24.183 KLD : 0.008 
iteration : 159400 loss : 6.801 NLL : -6.796 KLD : 0.005 
iteration : 159600 loss : 6.001 NLL : -5.998 KLD : 0.004 
iteration : 159800 loss : 24.744 NLL : -24.739 KLD : 0.005 
iteration : 160000 loss : 10.543 NLL : -10.539 KLD : 0.004 
iteration : 160200 loss : 14.882 NLL : -14.881 KLD : 0.001 
iteration : 160400 loss : 13.225 NLL : -13.222 KLD : 0.003 
iteration : 160600 loss : 15.613 NLL : -15.610 KLD : 0.004 
iteration : 160800 loss : 17.071 NLL : -17.070 KLD : 0.002 
iteration : 161000 loss : 15.942 NLL : -15.941 KLD : 0.001 
iteration : 161200 loss : 5.171 NLL : -5.151 KLD : 0.021 
iteration : 161400 loss : 6.848 NLL : -6.846 KLD : 0.001 
iteration : 161600 loss : 13.764 NLL : -13.761 KLD : 0.003 
iteration : 161800 loss : 17.446 NLL : -17.435 KLD : 0.011 
iteration : 162000 loss : 12.664 NLL : -12.663 KLD : 0.002 
iteration : 162200 loss : 17.801 NLL : -17.792 KLD : 0.010 
iteration : 162400 loss : 6.962 NLL : -6.956 KLD : 0.006 
iteration : 162600 loss : 7.149 NLL : -7.147 KLD : 0.002 
iteration : 162800 loss : 11.620 NLL : -11.619 KLD : 0.001 
iteration : 163000 loss : 8.988 NLL : -8.986 KLD : 0.002 
iteration : 163200 loss : 10.129 NLL : -10.127 KLD : 0.001 
iteration : 163400 loss : 14.488 NLL : -14.487 KLD : 0.000 
iteration : 163600 loss : 7.467 NLL : -7.464 KLD : 0.003 
iteration : 163800 loss : 21.659 NLL : -21.649 KLD : 0.010 
iteration : 164000 loss : 24.238 NLL : -24.237 KLD : 0.001 
iteration : 164200 loss : 8.397 NLL : -8.374 KLD : 0.023 
iteration : 164400 loss : 17.870 NLL : -17.865 KLD : 0.005 
iteration : 164600 loss : 18.486 NLL : -18.485 KLD : 0.002 
iteration : 164800 loss : 11.455 NLL : -11.455 KLD : 0.000 
iteration : 165000 loss : 12.002 NLL : -12.000 KLD : 0.002 
iteration : 165200 loss : 16.406 NLL : -16.403 KLD : 0.003 
iteration : 165400 loss : 9.403 NLL : -9.403 KLD : 0.000 
iteration : 165600 loss : 14.389 NLL : -14.388 KLD : 0.002 
iteration : 165800 loss : 6.884 NLL : -6.879 KLD : 0.005 
iteration : 166000 loss : 5.130 NLL : -5.129 KLD : 0.001 
iteration : 166200 loss : 14.413 NLL : -14.410 KLD : 0.002 
iteration : 166400 loss : 15.927 NLL : -15.925 KLD : 0.002 
iteration : 166600 loss : 16.836 NLL : -16.835 KLD : 0.001 
iteration : 166800 loss : 16.966 NLL : -16.964 KLD : 0.002 
iteration : 167000 loss : 19.846 NLL : -19.842 KLD : 0.004 
iteration : 167200 loss : 17.100 NLL : -17.097 KLD : 0.003 
iteration : 167400 loss : 14.964 NLL : -14.952 KLD : 0.011 
iteration : 167600 loss : 12.305 NLL : -12.305 KLD : 0.000 
iteration : 167800 loss : 17.938 NLL : -17.935 KLD : 0.003 
iteration : 168000 loss : 12.592 NLL : -12.591 KLD : 0.001 
iteration : 168200 loss : 12.132 NLL : -12.130 KLD : 0.001 
iteration : 168400 loss : 15.803 NLL : -15.793 KLD : 0.009 
iteration : 168600 loss : 12.875 NLL : -12.871 KLD : 0.004 
iteration : 168800 loss : 11.672 NLL : -11.671 KLD : 0.001 
iteration : 169000 loss : 11.392 NLL : -11.391 KLD : 0.001 
iteration : 169200 loss : 18.093 NLL : -18.092 KLD : 0.001 
iteration : 169400 loss : 17.338 NLL : -17.336 KLD : 0.002 
iteration : 169600 loss : 19.742 NLL : -19.738 KLD : 0.003 
iteration : 169800 loss : 17.821 NLL : -17.820 KLD : 0.001 
iteration : 170000 loss : 10.851 NLL : -10.848 KLD : 0.003 
iteration : 170200 loss : 13.054 NLL : -13.053 KLD : 0.002 
iteration : 170400 loss : 18.867 NLL : -18.865 KLD : 0.002 
iteration : 170600 loss : 16.112 NLL : -16.110 KLD : 0.001 
iteration : 170800 loss : 9.274 NLL : -9.246 KLD : 0.028 
iteration : 171000 loss : 6.616 NLL : -6.614 KLD : 0.001 
iteration : 171200 loss : 11.281 NLL : -11.280 KLD : 0.000 
iteration : 171400 loss : 15.784 NLL : -15.784 KLD : 0.000 
iteration : 171600 loss : 8.459 NLL : -8.458 KLD : 0.001 
iteration : 171800 loss : 25.075 NLL : -25.064 KLD : 0.011 
iteration : 172000 loss : 7.826 NLL : -7.821 KLD : 0.004 
iteration : 172200 loss : 19.655 NLL : -19.651 KLD : 0.003 
iteration : 172400 loss : 20.255 NLL : -20.254 KLD : 0.001 
iteration : 172600 loss : 17.128 NLL : -17.127 KLD : 0.001 
iteration : 172800 loss : 7.660 NLL : -7.655 KLD : 0.005 
iteration : 173000 loss : 19.225 NLL : -19.221 KLD : 0.004 
iteration : 173200 loss : 11.760 NLL : -11.757 KLD : 0.004 
iteration : 173400 loss : 9.195 NLL : -9.191 KLD : 0.004 
iteration : 173600 loss : 19.208 NLL : -19.202 KLD : 0.006 
iteration : 173800 loss : 16.339 NLL : -16.336 KLD : 0.003 
iteration : 174000 loss : 9.403 NLL : -9.396 KLD : 0.007 
iteration : 174200 loss : 15.054 NLL : -15.053 KLD : 0.001 
iteration : 174400 loss : 11.131 NLL : -11.130 KLD : 0.001 
iteration : 174600 loss : 14.820 NLL : -14.818 KLD : 0.003 
iteration : 174800 loss : 11.964 NLL : -11.963 KLD : 0.001 
iteration : 175000 loss : 13.481 NLL : -13.481 KLD : 0.001 
iteration : 175200 loss : 21.636 NLL : -21.629 KLD : 0.007 
iteration : 175400 loss : 18.211 NLL : -18.210 KLD : 0.001 
iteration : 175600 loss : 16.326 NLL : -16.305 KLD : 0.022 
iteration : 175800 loss : 18.103 NLL : -18.100 KLD : 0.002 
iteration : 176000 loss : 9.315 NLL : -9.312 KLD : 0.004 
iteration : 176200 loss : 14.614 NLL : -14.610 KLD : 0.005 
iteration : 176400 loss : 22.989 NLL : -22.970 KLD : 0.019 
iteration : 176600 loss : 19.325 NLL : -19.319 KLD : 0.006 
iteration : 176800 loss : 14.744 NLL : -14.742 KLD : 0.002 
iteration : 177000 loss : 11.597 NLL : -11.596 KLD : 0.001 
iteration : 177200 loss : 20.168 NLL : -20.161 KLD : 0.007 
iteration : 177400 loss : 20.478 NLL : -20.475 KLD : 0.003 
iteration : 177600 loss : 14.733 NLL : -14.731 KLD : 0.003 
iteration : 177800 loss : 18.046 NLL : -18.046 KLD : 0.001 
iteration : 178000 loss : 19.547 NLL : -19.544 KLD : 0.003 
iteration : 178200 loss : 12.382 NLL : -12.367 KLD : 0.015 
iteration : 178400 loss : 8.149 NLL : -8.134 KLD : 0.016 
iteration : 178600 loss : 11.449 NLL : -11.448 KLD : 0.001 
iteration : 178800 loss : 18.981 NLL : -18.980 KLD : 0.002 
iteration : 179000 loss : 14.819 NLL : -14.810 KLD : 0.009 
iteration : 179200 loss : 22.562 NLL : -22.539 KLD : 0.023 
iteration : 179400 loss : 12.549 NLL : -12.543 KLD : 0.006 
iteration : 179600 loss : 26.340 NLL : -26.319 KLD : 0.021 
iteration : 179800 loss : 16.579 NLL : -16.548 KLD : 0.032 
iteration : 180000 loss : 12.061 NLL : -12.060 KLD : 0.001 
---------- Training loss 16.161 updated ! and save the model! (step:180000) ----------
---------- Training loss 13.413 updated ! and save the model! (step:180006) ----------
---------- Training loss 13.179 updated ! and save the model! (step:180054) ----------
---------- Training loss 12.470 updated ! and save the model! (step:180168) ----------
iteration : 180200 loss : 14.308 NLL : -14.307 KLD : 0.000 
---------- Training loss 11.057 updated ! and save the model! (step:180210) ----------
iteration : 180400 loss : 11.568 NLL : -11.567 KLD : 0.000 
---------- Training loss 10.404 updated ! and save the model! (step:180504) ----------
iteration : 180600 loss : 6.941 NLL : -6.936 KLD : 0.004 
iteration : 180800 loss : 15.198 NLL : -15.187 KLD : 0.011 
iteration : 181000 loss : 17.638 NLL : -17.638 KLD : 0.000 
iteration : 181200 loss : 15.996 NLL : -15.992 KLD : 0.004 
---------- Training loss 9.547 updated ! and save the model! (step:181224) ----------
iteration : 181400 loss : 18.384 NLL : -18.382 KLD : 0.003 
iteration : 181600 loss : 16.109 NLL : -16.108 KLD : 0.001 
iteration : 181800 loss : 15.631 NLL : -15.627 KLD : 0.004 
iteration : 182000 loss : 8.629 NLL : -8.628 KLD : 0.001 
iteration : 182200 loss : 19.734 NLL : -19.732 KLD : 0.002 
iteration : 182400 loss : 11.325 NLL : -11.324 KLD : 0.001 
iteration : 182600 loss : 12.928 NLL : -12.927 KLD : 0.001 
iteration : 182800 loss : 19.429 NLL : -19.420 KLD : 0.009 
iteration : 183000 loss : 21.827 NLL : -21.824 KLD : 0.003 
---------- Training loss 9.217 updated ! and save the model! (step:183144) ----------
iteration : 183200 loss : 17.493 NLL : -17.488 KLD : 0.005 
iteration : 183400 loss : 7.018 NLL : -7.016 KLD : 0.002 
iteration : 183600 loss : 7.832 NLL : -7.822 KLD : 0.011 
iteration : 183800 loss : 10.689 NLL : -10.688 KLD : 0.001 
iteration : 184000 loss : 12.561 NLL : -12.535 KLD : 0.026 
iteration : 184200 loss : 17.440 NLL : -17.439 KLD : 0.002 
iteration : 184400 loss : 13.916 NLL : -13.909 KLD : 0.006 
iteration : 184600 loss : 10.720 NLL : -10.719 KLD : 0.001 
iteration : 184800 loss : 16.164 NLL : -16.159 KLD : 0.005 
iteration : 185000 loss : 17.304 NLL : -17.303 KLD : 0.001 
iteration : 185200 loss : 19.448 NLL : -19.446 KLD : 0.002 
iteration : 185400 loss : 14.400 NLL : -14.400 KLD : 0.001 
iteration : 185600 loss : 15.164 NLL : -15.158 KLD : 0.005 
iteration : 185800 loss : 16.422 NLL : -16.416 KLD : 0.006 
iteration : 186000 loss : 14.249 NLL : -14.248 KLD : 0.002 
iteration : 186200 loss : 12.051 NLL : -12.046 KLD : 0.006 
iteration : 186400 loss : 13.927 NLL : -13.926 KLD : 0.001 
iteration : 186600 loss : 10.425 NLL : -10.423 KLD : 0.001 
iteration : 186800 loss : 17.000 NLL : -16.991 KLD : 0.009 
iteration : 187000 loss : 12.617 NLL : -12.608 KLD : 0.009 
iteration : 187200 loss : 15.374 NLL : -15.373 KLD : 0.001 
iteration : 187400 loss : 16.780 NLL : -16.779 KLD : 0.001 
iteration : 187600 loss : 15.226 NLL : -15.222 KLD : 0.004 
iteration : 187800 loss : 10.939 NLL : -10.933 KLD : 0.006 
---------- Training loss 9.127 updated ! and save the model! (step:187884) ----------
iteration : 188000 loss : 18.703 NLL : -18.651 KLD : 0.052 
iteration : 188200 loss : 4.416 NLL : -4.402 KLD : 0.014 
iteration : 188400 loss : 19.880 NLL : -19.876 KLD : 0.004 
iteration : 188600 loss : 18.541 NLL : -18.538 KLD : 0.002 
iteration : 188800 loss : 16.946 NLL : -16.945 KLD : 0.001 
iteration : 189000 loss : 21.484 NLL : -21.481 KLD : 0.003 
iteration : 189200 loss : 14.406 NLL : -14.404 KLD : 0.002 
iteration : 189400 loss : 19.561 NLL : -19.550 KLD : 0.012 
iteration : 189600 loss : 16.246 NLL : -16.245 KLD : 0.001 
iteration : 189800 loss : 12.591 NLL : -12.581 KLD : 0.010 
iteration : 190000 loss : 20.806 NLL : -20.794 KLD : 0.011 
iteration : 190200 loss : 11.165 NLL : -11.163 KLD : 0.002 
iteration : 190400 loss : 11.590 NLL : -11.584 KLD : 0.005 
iteration : 190600 loss : 10.832 NLL : -10.829 KLD : 0.004 
iteration : 190800 loss : 17.024 NLL : -17.022 KLD : 0.001 
iteration : 191000 loss : 22.171 NLL : -22.168 KLD : 0.003 
iteration : 191200 loss : 20.715 NLL : -20.711 KLD : 0.004 
iteration : 191400 loss : 16.935 NLL : -16.933 KLD : 0.002 
iteration : 191600 loss : 19.921 NLL : -19.918 KLD : 0.003 
iteration : 191800 loss : 8.817 NLL : -8.808 KLD : 0.009 
iteration : 192000 loss : 21.751 NLL : -21.751 KLD : 0.001 
iteration : 192200 loss : 15.678 NLL : -15.675 KLD : 0.002 
iteration : 192400 loss : 21.499 NLL : -21.496 KLD : 0.002 
iteration : 192600 loss : 16.190 NLL : -16.189 KLD : 0.001 
iteration : 192800 loss : 20.763 NLL : -20.761 KLD : 0.002 
iteration : 193000 loss : 12.134 NLL : -12.132 KLD : 0.002 
iteration : 193200 loss : 8.135 NLL : -8.121 KLD : 0.013 
iteration : 193400 loss : 9.006 NLL : -8.994 KLD : 0.011 
iteration : 193600 loss : 15.852 NLL : -15.850 KLD : 0.002 
iteration : 193800 loss : 17.938 NLL : -17.933 KLD : 0.005 
iteration : 194000 loss : 8.992 NLL : -8.965 KLD : 0.026 
iteration : 194200 loss : 19.872 NLL : -19.865 KLD : 0.007 
iteration : 194400 loss : 18.895 NLL : -18.890 KLD : 0.005 
iteration : 194600 loss : 14.454 NLL : -14.450 KLD : 0.004 
iteration : 194800 loss : 23.062 NLL : -23.059 KLD : 0.004 
iteration : 195000 loss : 10.703 NLL : -10.695 KLD : 0.008 
iteration : 195200 loss : 21.923 NLL : -21.918 KLD : 0.006 
iteration : 195400 loss : 14.525 NLL : -14.521 KLD : 0.004 
iteration : 195600 loss : 5.581 NLL : -5.574 KLD : 0.006 
iteration : 195800 loss : 17.390 NLL : -17.387 KLD : 0.003 
iteration : 196000 loss : 12.048 NLL : -12.046 KLD : 0.002 
iteration : 196200 loss : 8.300 NLL : -8.296 KLD : 0.004 
iteration : 196400 loss : 17.975 NLL : -17.974 KLD : 0.001 
iteration : 196600 loss : 24.074 NLL : -24.062 KLD : 0.012 
iteration : 196800 loss : 12.233 NLL : -12.232 KLD : 0.001 
iteration : 197000 loss : 19.032 NLL : -19.011 KLD : 0.021 /home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/home/mgyukim/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])

iteration : 197200 loss : 7.481 NLL : -7.479 KLD : 0.002 
iteration : 197400 loss : 13.809 NLL : -13.805 KLD : 0.004 
iteration : 197600 loss : 18.824 NLL : -18.810 KLD : 0.015 
iteration : 197800 loss : 10.914 NLL : -10.911 KLD : 0.004 
---------- Training loss 9.038 updated ! and save the model! (step:197892) ----------
iteration : 198000 loss : 11.770 NLL : -11.767 KLD : 0.004 
iteration : 198200 loss : 19.522 NLL : -19.502 KLD : 0.019 
iteration : 198400 loss : 7.580 NLL : -7.579 KLD : 0.001 
iteration : 198600 loss : 17.618 NLL : -17.603 KLD : 0.015 
iteration : 198800 loss : 8.261 NLL : -8.260 KLD : 0.001 
iteration : 199000 loss : 25.487 NLL : -25.478 KLD : 0.009 
iteration : 199200 loss : 18.107 NLL : -18.104 KLD : 0.003 
iteration : 199400 loss : 20.567 NLL : -20.555 KLD : 0.012 
iteration : 199600 loss : 12.198 NLL : -12.171 KLD : 0.026 
iteration : 199800 loss : 14.843 NLL : -14.841 KLD : 0.003 
iteration : 200000 loss : 11.488 NLL : -11.484 KLD : 0.004 
---------- Save the model! (step:None) ----------
