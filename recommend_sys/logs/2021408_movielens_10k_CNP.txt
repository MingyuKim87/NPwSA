---------- Training loss 65045.882 updated ! and save the model! (step:6) ----------
---------- Training loss 2724.562 updated ! and save the model! (step:12) ----------
---------- Training loss 289.319 updated ! and save the model! (step:18) ----------
---------- Training loss 103.369 updated ! and save the model! (step:24) ----------
---------- Training loss 85.350 updated ! and save the model! (step:30) ----------
---------- Training loss 73.386 updated ! and save the model! (step:36) ----------
---------- Training loss 68.815 updated ! and save the model! (step:60) ----------
---------- Training loss 64.462 updated ! and save the model! (step:66) ----------
---------- Training loss 63.376 updated ! and save the model! (step:144) ----------
---------- Training loss 60.783 updated ! and save the model! (step:156) ----------
iteration : 200 loss : 73.072 NLL : -73.072 
---------- Training loss 56.576 updated ! and save the model! (step:276) ----------
---------- Training loss 53.222 updated ! and save the model! (step:318) ----------
---------- Training loss 50.589 updated ! and save the model! (step:336) ----------
iteration : 400 loss : 52.326 NLL : -52.326 
---------- Training loss 47.226 updated ! and save the model! (step:414) ----------
---------- Training loss 45.363 updated ! and save the model! (step:450) ----------
iteration : 600 loss : 72.690 NLL : -72.690 
iteration : 800 loss : 38.190 NLL : -38.190 
---------- Training loss 43.829 updated ! and save the model! (step:804) ----------
iteration : 1000 loss : 49.130 NLL : -49.130 
iteration : 1200 loss : 73.588 NLL : -73.588 
---------- Training loss 42.849 updated ! and save the model! (step:1302) ----------
iteration : 1400 loss : 74.563 NLL : -74.563 
---------- Training loss 41.136 updated ! and save the model! (step:1518) ----------
iteration : 1600 loss : 49.918 NLL : -49.918 
iteration : 1800 loss : 46.735 NLL : -46.735 
---------- Training loss 40.274 updated ! and save the model! (step:1806) ----------
---------- Training loss 33.112 updated ! and save the model! (step:1818) ----------
iteration : 2000 loss : 98.652 NLL : -98.652 
iteration : 2200 loss : 65.359 NLL : -65.359 
iteration : 2400 loss : 86.729 NLL : -86.729 
iteration : 2600 loss : 17.371 NLL : -17.371 
iteration : 2800 loss : 36.940 NLL : -36.940 
iteration : 3000 loss : 32.061 NLL : -32.061 
iteration : 3200 loss : 31.048 NLL : -31.048 
iteration : 3400 loss : 63.403 NLL : -63.403 
iteration : 3600 loss : 54.193 NLL : -54.193 
iteration : 3800 loss : 36.336 NLL : -36.336 
iteration : 4000 loss : 54.030 NLL : -54.030 
---------- Training loss 32.914 updated ! and save the model! (step:4098) ----------
iteration : 4200 loss : 34.049 NLL : -34.049 
iteration : 4400 loss : 63.400 NLL : -63.400 
iteration : 4600 loss : 70.770 NLL : -70.770 
iteration : 4800 loss : 67.334 NLL : -67.334 
iteration : 5000 loss : 44.009 NLL : -44.009 
iteration : 5200 loss : 76.427 NLL : -76.427 
iteration : 5400 loss : 34.648 NLL : -34.648 
iteration : 5600 loss : 70.421 NLL : -70.421 
iteration : 5800 loss : 61.339 NLL : -61.339 
iteration : 6000 loss : 63.727 NLL : -63.727 
iteration : 6200 loss : 77.954 NLL : -77.954 
iteration : 6400 loss : 59.665 NLL : -59.665 
iteration : 6600 loss : 51.817 NLL : -51.817 
iteration : 6800 loss : 99.311 NLL : -99.311 
iteration : 7000 loss : 38.875 NLL : -38.875 
iteration : 7200 loss : 48.465 NLL : -48.465 
iteration : 7400 loss : 80.458 NLL : -80.458 
iteration : 7600 loss : 65.230 NLL : -65.230 
iteration : 7800 loss : 40.269 NLL : -40.269 
iteration : 8000 loss : 73.507 NLL : -73.507 
iteration : 8200 loss : 32.247 NLL : -32.247 
iteration : 8400 loss : 61.231 NLL : -61.231 
iteration : 8600 loss : 59.729 NLL : -59.729 
iteration : 8800 loss : 52.473 NLL : -52.473 
iteration : 9000 loss : 33.466 NLL : -33.466 
iteration : 9200 loss : 86.197 NLL : -86.197 
iteration : 9400 loss : 73.541 NLL : -73.541 
iteration : 9600 loss : 47.455 NLL : -47.455 
iteration : 9800 loss : 60.047 NLL : -60.047 
iteration : 10000 loss : 67.002 NLL : -67.002 
iteration : 10200 loss : 57.610 NLL : -57.610 
iteration : 10400 loss : 48.394 NLL : -48.394 
iteration : 10600 loss : 69.030 NLL : -69.030 
iteration : 10800 loss : 62.032 NLL : -62.032 
iteration : 11000 loss : 61.713 NLL : -61.713 
iteration : 11200 loss : 65.849 NLL : -65.849 
iteration : 11400 loss : 44.509 NLL : -44.509 
iteration : 11600 loss : 59.963 NLL : -59.963 
iteration : 11800 loss : 61.975 NLL : -61.975 
iteration : 12000 loss : 44.905 NLL : -44.905 
iteration : 12200 loss : 56.944 NLL : -56.944 
iteration : 12400 loss : 81.460 NLL : -81.460 
iteration : 12600 loss : 55.079 NLL : -55.079 
iteration : 12800 loss : 57.214 NLL : -57.214 
iteration : 13000 loss : 84.906 NLL : -84.906 
iteration : 13200 loss : 47.938 NLL : -47.938 
iteration : 13400 loss : 49.668 NLL : -49.668 
iteration : 13600 loss : 29.832 NLL : -29.832 
iteration : 13800 loss : 45.820 NLL : -45.820 
iteration : 14000 loss : 61.747 NLL : -61.747 
iteration : 14200 loss : 58.510 NLL : -58.510 
iteration : 14400 loss : 54.242 NLL : -54.242 
iteration : 14600 loss : 48.954 NLL : -48.954 
iteration : 14800 loss : 42.154 NLL : -42.154 
iteration : 15000 loss : 37.888 NLL : -37.888 
iteration : 15200 loss : 29.913 NLL : -29.913 
iteration : 15400 loss : 44.498 NLL : -44.498 
iteration : 15600 loss : 29.064 NLL : -29.064 
iteration : 15800 loss : 76.811 NLL : -76.811 
iteration : 16000 loss : 44.009 NLL : -44.009 
iteration : 16200 loss : 30.899 NLL : -30.899 
iteration : 16400 loss : 38.193 NLL : -38.193 
iteration : 16600 loss : 52.567 NLL : -52.567 
iteration : 16800 loss : 42.216 NLL : -42.216 
iteration : 17000 loss : 37.984 NLL : -37.984 
iteration : 17200 loss : 48.779 NLL : -48.779 
iteration : 17400 loss : 46.118 NLL : -46.118 
iteration : 17600 loss : 32.581 NLL : -32.581 
iteration : 17800 loss : 57.266 NLL : -57.266 
iteration : 18000 loss : 51.665 NLL : -51.665 
iteration : 18200 loss : 35.016 NLL : -35.016 
iteration : 18400 loss : 65.934 NLL : -65.934 
iteration : 18600 loss : 43.991 NLL : -43.991 
iteration : 18800 loss : 38.571 NLL : -38.571 
iteration : 19000 loss : 54.601 NLL : -54.601 
iteration : 19200 loss : 53.127 NLL : -53.127 
iteration : 19400 loss : 44.177 NLL : -44.177 
iteration : 19600 loss : 40.436 NLL : -40.436 
iteration : 19800 loss : 46.790 NLL : -46.790 
---------- Training loss 30.961 updated ! and save the model! (step:19890) ----------
iteration : 20000 loss : 47.994 NLL : -47.994 
iteration : 20200 loss : 47.285 NLL : -47.285 
iteration : 20400 loss : 60.196 NLL : -60.196 
iteration : 20600 loss : 68.251 NLL : -68.251 
iteration : 20800 loss : 60.323 NLL : -60.323 
iteration : 21000 loss : 35.027 NLL : -35.027 
iteration : 21200 loss : 58.309 NLL : -58.309 
iteration : 21400 loss : 43.790 NLL : -43.790 
iteration : 21600 loss : 47.454 NLL : -47.454 
iteration : 21800 loss : 35.317 NLL : -35.317 
---------- Training loss 30.878 updated ! and save the model! (step:21978) ----------
iteration : 22000 loss : 29.920 NLL : -29.920 
iteration : 22200 loss : 40.229 NLL : -40.229 
iteration : 22400 loss : 57.696 NLL : -57.696 
iteration : 22600 loss : 49.853 NLL : -49.853 
iteration : 22800 loss : 62.939 NLL : -62.939 
iteration : 23000 loss : 35.865 NLL : -35.865 
iteration : 23200 loss : 34.363 NLL : -34.363 
iteration : 23400 loss : 58.194 NLL : -58.194 
iteration : 23600 loss : 67.281 NLL : -67.281 
iteration : 23800 loss : 57.687 NLL : -57.687 
iteration : 24000 loss : 25.722 NLL : -25.722 
iteration : 24200 loss : 45.043 NLL : -45.043 
iteration : 24400 loss : 26.333 NLL : -26.333 
iteration : 24600 loss : 48.201 NLL : -48.201 
iteration : 24800 loss : 50.288 NLL : -50.288 
iteration : 25000 loss : 51.681 NLL : -51.681 
iteration : 25200 loss : 26.620 NLL : -26.620 
iteration : 25400 loss : 56.485 NLL : -56.485 
iteration : 25600 loss : 56.560 NLL : -56.560 
iteration : 25800 loss : 63.534 NLL : -63.534 
iteration : 26000 loss : 48.861 NLL : -48.861 
iteration : 26200 loss : 66.421 NLL : -66.421 
---------- Training loss 30.862 updated ! and save the model! (step:26394) ----------
iteration : 26400 loss : 34.981 NLL : -34.981 
iteration : 26600 loss : 50.113 NLL : -50.113 
iteration : 26800 loss : 93.207 NLL : -93.207 
iteration : 27000 loss : 54.131 NLL : -54.131 
iteration : 27200 loss : 84.122 NLL : -84.122 
iteration : 27400 loss : 47.698 NLL : -47.698 
iteration : 27600 loss : 24.179 NLL : -24.179 
iteration : 27800 loss : 46.555 NLL : -46.555 
iteration : 28000 loss : 45.290 NLL : -45.290 
iteration : 28200 loss : 88.090 NLL : -88.090 
iteration : 28400 loss : 58.501 NLL : -58.501 
iteration : 28600 loss : 31.720 NLL : -31.720 
iteration : 28800 loss : 39.959 NLL : -39.959 
iteration : 29000 loss : 74.830 NLL : -74.830 
iteration : 29200 loss : 41.652 NLL : -41.652 
iteration : 29400 loss : 46.433 NLL : -46.433 
iteration : 29600 loss : 98.510 NLL : -98.510 
iteration : 29800 loss : 33.014 NLL : -33.014 
iteration : 30000 loss : 48.243 NLL : -48.243 
---------- Training loss 55.877 updated ! and save the model! (step:30000) ----------
---------- Training loss 42.759 updated ! and save the model! (step:30006) ----------
---------- Training loss 41.730 updated ! and save the model! (step:30072) ----------
---------- Training loss 40.991 updated ! and save the model! (step:30078) ----------
---------- Training loss 38.313 updated ! and save the model! (step:30084) ----------
---------- Training loss 37.062 updated ! and save the model! (step:30126) ----------
---------- Training loss 36.761 updated ! and save the model! (step:30180) ----------
iteration : 30200 loss : 51.194 NLL : -51.194 
iteration : 30400 loss : 73.559 NLL : -73.559 
iteration : 30600 loss : 35.217 NLL : -35.217 
iteration : 30800 loss : 32.998 NLL : -32.998 
iteration : 31000 loss : 71.983 NLL : -71.983 
iteration : 31200 loss : 43.158 NLL : -43.158 
---------- Training loss 35.712 updated ! and save the model! (step:31242) ----------
---------- Training loss 35.177 updated ! and save the model! (step:31272) ----------
iteration : 31400 loss : 52.348 NLL : -52.348 
iteration : 31600 loss : 50.281 NLL : -50.281 
iteration : 31800 loss : 79.793 NLL : -79.793 
iteration : 32000 loss : 38.655 NLL : -38.655 
iteration : 32200 loss : 38.002 NLL : -38.002 
iteration : 32400 loss : 49.868 NLL : -49.868 
---------- Training loss 34.680 updated ! and save the model! (step:32550) ----------
iteration : 32600 loss : 40.737 NLL : -40.737 
iteration : 32800 loss : 60.919 NLL : -60.919 
iteration : 33000 loss : 62.630 NLL : -62.630 
iteration : 33200 loss : 52.815 NLL : -52.815 
iteration : 33400 loss : 56.673 NLL : -56.673 
iteration : 33600 loss : 33.368 NLL : -33.368 
---------- Training loss 31.152 updated ! and save the model! (step:33624) ----------
iteration : 33800 loss : 59.059 NLL : -59.059 
iteration : 34000 loss : 75.855 NLL : -75.855 
iteration : 34200 loss : 71.349 NLL : -71.349 
iteration : 34400 loss : 91.309 NLL : -91.309 
iteration : 34600 loss : 53.459 NLL : -53.459 
iteration : 34800 loss : 56.753 NLL : -56.753 
iteration : 35000 loss : 42.423 NLL : -42.423 
iteration : 35200 loss : 40.969 NLL : -40.969 
iteration : 35400 loss : 41.059 NLL : -41.059 
iteration : 35600 loss : 43.865 NLL : -43.865 
iteration : 35800 loss : 53.836 NLL : -53.836 
iteration : 36000 loss : 64.875 NLL : -64.875 
iteration : 36200 loss : 68.270 NLL : -68.270 
iteration : 36400 loss : 57.138 NLL : -57.138 
iteration : 36600 loss : 48.335 NLL : -48.335 
iteration : 36800 loss : 53.325 NLL : -53.325 
iteration : 37000 loss : 62.574 NLL : -62.574 
iteration : 37200 loss : 71.905 NLL : -71.905 
iteration : 37400 loss : 45.896 NLL : -45.896 
iteration : 37600 loss : 60.635 NLL : -60.635 
iteration : 37800 loss : 53.479 NLL : -53.479 
iteration : 38000 loss : 41.870 NLL : -41.870 
iteration : 38200 loss : 80.139 NLL : -80.139 
iteration : 38400 loss : 52.276 NLL : -52.276 
iteration : 38600 loss : 35.617 NLL : -35.617 
iteration : 38800 loss : 58.511 NLL : -58.511 
iteration : 39000 loss : 54.036 NLL : -54.036 
iteration : 39200 loss : 41.455 NLL : -41.455 
iteration : 39400 loss : 39.691 NLL : -39.691 
iteration : 39600 loss : 41.181 NLL : -41.181 
iteration : 39800 loss : 59.200 NLL : -59.200 
iteration : 40000 loss : 52.672 NLL : -52.672 
iteration : 40200 loss : 55.660 NLL : -55.660 
iteration : 40400 loss : 58.875 NLL : -58.875 
iteration : 40600 loss : 50.926 NLL : -50.926 
iteration : 40800 loss : 51.817 NLL : -51.817 
iteration : 41000 loss : 54.150 NLL : -54.150 
iteration : 41200 loss : 54.872 NLL : -54.872 
iteration : 41400 loss : 56.237 NLL : -56.237 
iteration : 41600 loss : 60.414 NLL : -60.414 
iteration : 41800 loss : 63.350 NLL : -63.350 
iteration : 42000 loss : 20.914 NLL : -20.914 
iteration : 42200 loss : 53.517 NLL : -53.517 
iteration : 42400 loss : 52.030 NLL : -52.030 
iteration : 42600 loss : 59.827 NLL : -59.827 
iteration : 42800 loss : 32.693 NLL : -32.693 
iteration : 43000 loss : 54.856 NLL : -54.856 
iteration : 43200 loss : 43.512 NLL : -43.512 
iteration : 43400 loss : 26.156 NLL : -26.156 
iteration : 43600 loss : 44.185 NLL : -44.185 
iteration : 43800 loss : 74.905 NLL : -74.905 
iteration : 44000 loss : 27.804 NLL : -27.804 
iteration : 44200 loss : 41.463 NLL : -41.463 
iteration : 44400 loss : 40.553 NLL : -40.553 
iteration : 44600 loss : 37.451 NLL : -37.451 
iteration : 44800 loss : 53.349 NLL : -53.349 
iteration : 45000 loss : 59.912 NLL : -59.912 
iteration : 45200 loss : 57.964 NLL : -57.964 
iteration : 45400 loss : 50.932 NLL : -50.932 
iteration : 45600 loss : 39.637 NLL : -39.637 
iteration : 45800 loss : 48.437 NLL : -48.437 
iteration : 46000 loss : 30.515 NLL : -30.515 
iteration : 46200 loss : 73.462 NLL : -73.462 
iteration : 46400 loss : 44.413 NLL : -44.413 
iteration : 46600 loss : 52.065 NLL : -52.065 
iteration : 46800 loss : 69.476 NLL : -69.476 
iteration : 47000 loss : 48.309 NLL : -48.309 
iteration : 47200 loss : 54.727 NLL : -54.727 
iteration : 47400 loss : 46.460 NLL : -46.460 
iteration : 47600 loss : 56.423 NLL : -56.423 
iteration : 47800 loss : 33.130 NLL : -33.130 
iteration : 48000 loss : 45.114 NLL : -45.114 
iteration : 48200 loss : 33.715 NLL : -33.715 
iteration : 48400 loss : 66.329 NLL : -66.329 
iteration : 48600 loss : 54.626 NLL : -54.626 
iteration : 48800 loss : 45.747 NLL : -45.747 
iteration : 49000 loss : 54.170 NLL : -54.170 
iteration : 49200 loss : 55.939 NLL : -55.939 
iteration : 49400 loss : 52.231 NLL : -52.231 
iteration : 49600 loss : 37.342 NLL : -37.342 
iteration : 49800 loss : 32.338 NLL : -32.338 
iteration : 50000 loss : 41.716 NLL : -41.716 
iteration : 50200 loss : 40.247 NLL : -40.247 
iteration : 50400 loss : 48.116 NLL : -48.116 
iteration : 50600 loss : 31.982 NLL : -31.982 
iteration : 50800 loss : 32.161 NLL : -32.161 
iteration : 51000 loss : 59.586 NLL : -59.586 
iteration : 51200 loss : 56.510 NLL : -56.510 
iteration : 51400 loss : 29.757 NLL : -29.757 
iteration : 51600 loss : 54.086 NLL : -54.086 
iteration : 51800 loss : 43.094 NLL : -43.094 
iteration : 52000 loss : 45.201 NLL : -45.201 
iteration : 52200 loss : 64.749 NLL : -64.749 
iteration : 52400 loss : 56.682 NLL : -56.682 
iteration : 52600 loss : 35.017 NLL : -35.017 
iteration : 52800 loss : 37.915 NLL : -37.915 
iteration : 53000 loss : 52.411 NLL : -52.411 
iteration : 53200 loss : 52.037 NLL : -52.037 
iteration : 53400 loss : 42.206 NLL : -42.206 
iteration : 53600 loss : 40.302 NLL : -40.302 
iteration : 53800 loss : 66.272 NLL : -66.272 
iteration : 54000 loss : 30.824 NLL : -30.824 
iteration : 54200 loss : 53.058 NLL : -53.058 
iteration : 54400 loss : 32.407 NLL : -32.407 
iteration : 54600 loss : 32.393 NLL : -32.393 
iteration : 54800 loss : 50.043 NLL : -50.043 
iteration : 55000 loss : 44.982 NLL : -44.982 
iteration : 55200 loss : 60.120 NLL : -60.120 
iteration : 55400 loss : 52.551 NLL : -52.551 
iteration : 55600 loss : 34.448 NLL : -34.448 
iteration : 55800 loss : 47.405 NLL : -47.405 
iteration : 56000 loss : 58.074 NLL : -58.074 
iteration : 56200 loss : 74.453 NLL : -74.453 
iteration : 56400 loss : 42.055 NLL : -42.055 
iteration : 56600 loss : 55.616 NLL : -55.616 
iteration : 56800 loss : 44.110 NLL : -44.110 
iteration : 57000 loss : 40.661 NLL : -40.661 
iteration : 57200 loss : 42.377 NLL : -42.377 
iteration : 57400 loss : 62.512 NLL : -62.512 
iteration : 57600 loss : 28.302 NLL : -28.302 
iteration : 57800 loss : 45.366 NLL : -45.366 
iteration : 58000 loss : 17.952 NLL : -17.952 
iteration : 58200 loss : 48.716 NLL : -48.716 
iteration : 58400 loss : 72.609 NLL : -72.609 
iteration : 58600 loss : 50.305 NLL : -50.305 
iteration : 58800 loss : 53.666 NLL : -53.666 
iteration : 59000 loss : 61.134 NLL : -61.134 
iteration : 59200 loss : 56.690 NLL : -56.690 
iteration : 59400 loss : 58.599 NLL : -58.599 
iteration : 59600 loss : 56.292 NLL : -56.292 
iteration : 59800 loss : 55.306 NLL : -55.306 
iteration : 60000 loss : 43.336 NLL : -43.336 
---------- Training loss 51.460 updated ! and save the model! (step:60000) ----------
---------- Training loss 44.963 updated ! and save the model! (step:60006) ----------
---------- Training loss 39.884 updated ! and save the model! (step:60018) ----------
---------- Training loss 39.592 updated ! and save the model! (step:60102) ----------
---------- Training loss 39.547 updated ! and save the model! (step:60114) ----------
---------- Training loss 38.338 updated ! and save the model! (step:60156) ----------
iteration : 60200 loss : 37.235 NLL : -37.235 
---------- Training loss 34.537 updated ! and save the model! (step:60216) ----------
iteration : 60400 loss : 26.457 NLL : -26.457 
iteration : 60600 loss : 51.666 NLL : -51.666 
iteration : 60800 loss : 49.775 NLL : -49.775 
iteration : 61000 loss : 58.926 NLL : -58.926 
iteration : 61200 loss : 55.258 NLL : -55.258 
iteration : 61400 loss : 56.202 NLL : -56.202 
iteration : 61600 loss : 46.329 NLL : -46.329 
iteration : 61800 loss : 56.315 NLL : -56.315 
---------- Training loss 32.159 updated ! and save the model! (step:61824) ----------
iteration : 62000 loss : 45.573 NLL : -45.573 
iteration : 62200 loss : 47.566 NLL : -47.566 
---------- Training loss 31.451 updated ! and save the model! (step:62292) ----------
iteration : 62400 loss : 87.522 NLL : -87.522 
iteration : 62600 loss : 51.565 NLL : -51.565 
iteration : 62800 loss : 39.397 NLL : -39.397 
iteration : 63000 loss : 48.735 NLL : -48.735 
iteration : 63200 loss : 49.964 NLL : -49.964 
---------- Training loss 29.820 updated ! and save the model! (step:63282) ----------
iteration : 63400 loss : 64.995 NLL : -64.995 
iteration : 63600 loss : 62.868 NLL : -62.868 
iteration : 63800 loss : 44.085 NLL : -44.085 
iteration : 64000 loss : 43.665 NLL : -43.665 
iteration : 64200 loss : 59.728 NLL : -59.728 
iteration : 64400 loss : 62.161 NLL : -62.161 
iteration : 64600 loss : 33.245 NLL : -33.245 
iteration : 64800 loss : 51.699 NLL : -51.699 
iteration : 65000 loss : 53.815 NLL : -53.815 
iteration : 65200 loss : 38.862 NLL : -38.862 
iteration : 65400 loss : 46.549 NLL : -46.549 
iteration : 65600 loss : 52.416 NLL : -52.416 
iteration : 65800 loss : 62.325 NLL : -62.325 
iteration : 66000 loss : 55.056 NLL : -55.056 
iteration : 66200 loss : 39.666 NLL : -39.666 
iteration : 66400 loss : 39.990 NLL : -39.990 
iteration : 66600 loss : 51.504 NLL : -51.504 
iteration : 66800 loss : 42.189 NLL : -42.189 
iteration : 67000 loss : 49.076 NLL : -49.076 
iteration : 67200 loss : 40.806 NLL : -40.806 
iteration : 67400 loss : 54.444 NLL : -54.444 
iteration : 67600 loss : 47.468 NLL : -47.468 
iteration : 67800 loss : 43.716 NLL : -43.716 
iteration : 68000 loss : 57.894 NLL : -57.894 
iteration : 68200 loss : 39.627 NLL : -39.627 
iteration : 68400 loss : 56.451 NLL : -56.451 
iteration : 68600 loss : 57.820 NLL : -57.820 
iteration : 68800 loss : 45.957 NLL : -45.957 
iteration : 69000 loss : 40.122 NLL : -40.122 
iteration : 69200 loss : 50.282 NLL : -50.282 
iteration : 69400 loss : 55.549 NLL : -55.549 
iteration : 69600 loss : 49.816 NLL : -49.816 
iteration : 69800 loss : 41.038 NLL : -41.038 
iteration : 70000 loss : 51.648 NLL : -51.648 
iteration : 70200 loss : 25.506 NLL : -25.506 
iteration : 70400 loss : 45.692 NLL : -45.692 
iteration : 70600 loss : 53.846 NLL : -53.846 
iteration : 70800 loss : 61.899 NLL : -61.899 
iteration : 71000 loss : 45.067 NLL : -45.067 
iteration : 71200 loss : 48.745 NLL : -48.745 
iteration : 71400 loss : 52.901 NLL : -52.901 
iteration : 71600 loss : 41.078 NLL : -41.078 
iteration : 71800 loss : 41.750 NLL : -41.750 
iteration : 72000 loss : 57.852 NLL : -57.852 
iteration : 72200 loss : 48.057 NLL : -48.057 
iteration : 72400 loss : 57.210 NLL : -57.210 
iteration : 72600 loss : 52.421 NLL : -52.421 
iteration : 72800 loss : 56.983 NLL : -56.983 
iteration : 73000 loss : 50.231 NLL : -50.231 
iteration : 73200 loss : 46.778 NLL : -46.778 
iteration : 73400 loss : 22.602 NLL : -22.602 
iteration : 73600 loss : 48.471 NLL : -48.471 
iteration : 73800 loss : 47.396 NLL : -47.396 
iteration : 74000 loss : 67.220 NLL : -67.220 
iteration : 74200 loss : 26.808 NLL : -26.808 
iteration : 74400 loss : 19.962 NLL : -19.962 
iteration : 74600 loss : 41.846 NLL : -41.846 
iteration : 74800 loss : 53.979 NLL : -53.979 
iteration : 75000 loss : 38.964 NLL : -38.964 
iteration : 75200 loss : 60.823 NLL : -60.823 
iteration : 75400 loss : 46.809 NLL : -46.809 
iteration : 75600 loss : 42.724 NLL : -42.724 
iteration : 75800 loss : 54.093 NLL : -54.093 
iteration : 76000 loss : 54.819 NLL : -54.819 
iteration : 76200 loss : 55.919 NLL : -55.919 
iteration : 76400 loss : 32.799 NLL : -32.799 
iteration : 76600 loss : 55.510 NLL : -55.510 
iteration : 76800 loss : 39.098 NLL : -39.098 
iteration : 77000 loss : 48.779 NLL : -48.779 
iteration : 77200 loss : 59.064 NLL : -59.064 
iteration : 77400 loss : 53.575 NLL : -53.575 
iteration : 77600 loss : 52.509 NLL : -52.509 
iteration : 77800 loss : 22.618 NLL : -22.618 
iteration : 78000 loss : 41.870 NLL : -41.870 
iteration : 78200 loss : 43.310 NLL : -43.310 
iteration : 78400 loss : 49.467 NLL : -49.467 
iteration : 78600 loss : 47.031 NLL : -47.031 
iteration : 78800 loss : 60.508 NLL : -60.508 
iteration : 79000 loss : 54.123 NLL : -54.123 
iteration : 79200 loss : 56.404 NLL : -56.404 
iteration : 79400 loss : 21.247 NLL : -21.247 
iteration : 79600 loss : 49.653 NLL : -49.653 
iteration : 79800 loss : 42.687 NLL : -42.687 
iteration : 80000 loss : 33.084 NLL : -33.084 
iteration : 80200 loss : 49.516 NLL : -49.516 
iteration : 80400 loss : 42.236 NLL : -42.236 
iteration : 80600 loss : 54.023 NLL : -54.023 
iteration : 80800 loss : 43.513 NLL : -43.513 
iteration : 81000 loss : 97.210 NLL : -97.210 
iteration : 81200 loss : 26.083 NLL : -26.083 
iteration : 81400 loss : 43.190 NLL : -43.190 
iteration : 81600 loss : 55.624 NLL : -55.624 
iteration : 81800 loss : 35.991 NLL : -35.991 
iteration : 82000 loss : 29.760 NLL : -29.760 
iteration : 82200 loss : 29.295 NLL : -29.295 
iteration : 82400 loss : 40.605 NLL : -40.605 
iteration : 82600 loss : 42.834 NLL : -42.834 
iteration : 82800 loss : 24.156 NLL : -24.156 
iteration : 83000 loss : 44.919 NLL : -44.919 
iteration : 83200 loss : 44.838 NLL : -44.838 
iteration : 83400 loss : 36.653 NLL : -36.653 
iteration : 83600 loss : 46.831 NLL : -46.831 
iteration : 83800 loss : 53.595 NLL : -53.595 
iteration : 84000 loss : 58.688 NLL : -58.688 
iteration : 84200 loss : 48.494 NLL : -48.494 
iteration : 84400 loss : 51.237 NLL : -51.237 
iteration : 84600 loss : 45.352 NLL : -45.352 
iteration : 84800 loss : 57.643 NLL : -57.643 
iteration : 85000 loss : 144.343 NLL : -144.343 
iteration : 85200 loss : 64.261 NLL : -64.261 
iteration : 85400 loss : 45.446 NLL : -45.446 
iteration : 85600 loss : 68.835 NLL : -68.835 
iteration : 85800 loss : 61.686 NLL : -61.686 
iteration : 86000 loss : 22.141 NLL : -22.141 
iteration : 86200 loss : 53.110 NLL : -53.110 
iteration : 86400 loss : 49.245 NLL : -49.245 
iteration : 86600 loss : 50.637 NLL : -50.637 
iteration : 86800 loss : 64.895 NLL : -64.895 
iteration : 87000 loss : 52.286 NLL : -52.286 
iteration : 87200 loss : 47.877 NLL : -47.877 
iteration : 87400 loss : 46.571 NLL : -46.571 
iteration : 87600 loss : 40.627 NLL : -40.627 
iteration : 87800 loss : 48.998 NLL : -48.998 
iteration : 88000 loss : 38.142 NLL : -38.142 
iteration : 88200 loss : 45.590 NLL : -45.590 
iteration : 88400 loss : 52.773 NLL : -52.773 
iteration : 88600 loss : 50.201 NLL : -50.201 
iteration : 88800 loss : 39.617 NLL : -39.617 
iteration : 89000 loss : 38.152 NLL : -38.152 
iteration : 89200 loss : 41.410 NLL : -41.410 
iteration : 89400 loss : 58.001 NLL : -58.001 
iteration : 89600 loss : 37.146 NLL : -37.146 
iteration : 89800 loss : 50.499 NLL : -50.499 
iteration : 90000 loss : 53.440 NLL : -53.440 
---------- Training loss 49.514 updated ! and save the model! (step:90000) ----------
---------- Training loss 48.832 updated ! and save the model! (step:90006) ----------
---------- Training loss 47.833 updated ! and save the model! (step:90018) ----------
---------- Training loss 46.262 updated ! and save the model! (step:90030) ----------
---------- Training loss 43.184 updated ! and save the model! (step:90054) ----------
---------- Training loss 40.320 updated ! and save the model! (step:90066) ----------
---------- Training loss 39.660 updated ! and save the model! (step:90132) ----------
iteration : 90200 loss : 33.119 NLL : -33.119 
---------- Training loss 38.506 updated ! and save the model! (step:90318) ----------
iteration : 90400 loss : 62.390 NLL : -62.390 
---------- Training loss 36.827 updated ! and save the model! (step:90480) ----------
iteration : 90600 loss : 51.779 NLL : -51.779 
---------- Training loss 35.926 updated ! and save the model! (step:90618) ----------
---------- Training loss 35.254 updated ! and save the model! (step:90708) ----------
iteration : 90800 loss : 53.524 NLL : -53.524 
iteration : 91000 loss : 45.432 NLL : -45.432 
iteration : 91200 loss : 44.792 NLL : -44.792 
iteration : 91400 loss : 48.412 NLL : -48.412 
iteration : 91600 loss : 64.409 NLL : -64.409 
iteration : 91800 loss : 40.304 NLL : -40.304 
iteration : 92000 loss : 53.067 NLL : -53.067 
iteration : 92200 loss : 61.158 NLL : -61.158 
iteration : 92400 loss : 42.019 NLL : -42.019 
iteration : 92600 loss : 55.625 NLL : -55.625 
iteration : 92800 loss : 59.135 NLL : -59.135 
iteration : 93000 loss : 48.260 NLL : -48.260 
iteration : 93200 loss : 42.138 NLL : -42.138 
iteration : 93400 loss : 66.529 NLL : -66.529 
iteration : 93600 loss : 52.146 NLL : -52.146 
iteration : 93800 loss : 34.987 NLL : -34.987 
iteration : 94000 loss : 49.848 NLL : -49.848 
iteration : 94200 loss : 50.389 NLL : -50.389 
iteration : 94400 loss : 55.793 NLL : -55.793 
---------- Training loss 33.345 updated ! and save the model! (step:94572) ----------
iteration : 94600 loss : 75.691 NLL : -75.691 
iteration : 94800 loss : 43.197 NLL : -43.197 
iteration : 95000 loss : 49.985 NLL : -49.985 
iteration : 95200 loss : 47.113 NLL : -47.113 
iteration : 95400 loss : 45.880 NLL : -45.880 
iteration : 95600 loss : 45.166 NLL : -45.166 
iteration : 95800 loss : 51.188 NLL : -51.188 
iteration : 96000 loss : 39.287 NLL : -39.287 
iteration : 96200 loss : 48.995 NLL : -48.995 
iteration : 96400 loss : 51.889 NLL : -51.889 
iteration : 96600 loss : 36.814 NLL : -36.814 
iteration : 96800 loss : 46.231 NLL : -46.231 
iteration : 97000 loss : 41.195 NLL : -41.195 
---------- Training loss 31.891 updated ! and save the model! (step:97194) ----------
iteration : 97200 loss : 39.807 NLL : -39.807 
iteration : 97400 loss : 46.460 NLL : -46.460 
iteration : 97600 loss : 40.890 NLL : -40.890 
iteration : 97800 loss : 62.051 NLL : -62.051 
iteration : 98000 loss : 38.783 NLL : -38.783 
iteration : 98200 loss : 48.187 NLL : -48.187 
iteration : 98400 loss : 22.078 NLL : -22.078 
iteration : 98600 loss : 56.552 NLL : -56.552 
iteration : 98800 loss : 19.425 NLL : -19.425 
iteration : 99000 loss : 43.739 NLL : -43.739 
iteration : 99200 loss : 50.257 NLL : -50.257 
iteration : 99400 loss : 60.546 NLL : -60.546 
iteration : 99600 loss : 50.759 NLL : -50.759 
iteration : 99800 loss : 21.903 NLL : -21.903 
iteration : 100000 loss : 48.607 NLL : -48.607 
iteration : 100200 loss : 37.681 NLL : -37.681 
iteration : 100400 loss : 42.112 NLL : -42.112 
iteration : 100600 loss : 46.419 NLL : -46.419 
iteration : 100800 loss : 53.317 NLL : -53.317 
iteration : 101000 loss : 46.785 NLL : -46.785 
iteration : 101200 loss : 45.822 NLL : -45.822 
iteration : 101400 loss : 39.471 NLL : -39.471 
iteration : 101600 loss : 40.439 NLL : -40.439 
iteration : 101800 loss : 49.168 NLL : -49.168 
iteration : 102000 loss : 56.787 NLL : -56.787 
iteration : 102200 loss : 44.519 NLL : -44.519 
iteration : 102400 loss : 43.887 NLL : -43.887 
iteration : 102600 loss : 48.708 NLL : -48.708 
iteration : 102800 loss : 45.878 NLL : -45.878 
iteration : 103000 loss : 95.673 NLL : -95.673 
iteration : 103200 loss : 61.065 NLL : -61.065 
iteration : 103400 loss : 22.643 NLL : -22.643 
iteration : 103600 loss : 50.388 NLL : -50.388 
iteration : 103800 loss : 65.425 NLL : -65.425 
iteration : 104000 loss : 42.820 NLL : -42.820 
iteration : 104200 loss : 29.208 NLL : -29.208 
iteration : 104400 loss : 38.938 NLL : -38.938 
iteration : 104600 loss : 48.950 NLL : -48.950 
iteration : 104800 loss : 22.512 NLL : -22.512 
iteration : 105000 loss : 51.889 NLL : -51.889 
iteration : 105200 loss : 33.854 NLL : -33.854 
iteration : 105400 loss : 53.796 NLL : -53.796 
iteration : 105600 loss : 61.864 NLL : -61.864 
iteration : 105800 loss : 21.456 NLL : -21.456 
iteration : 106000 loss : 52.125 NLL : -52.125 
iteration : 106200 loss : 57.754 NLL : -57.754 
iteration : 106400 loss : 52.537 NLL : -52.537 
iteration : 106600 loss : 59.325 NLL : -59.325 
iteration : 106800 loss : 42.826 NLL : -42.826 
iteration : 107000 loss : 58.799 NLL : -58.799 
iteration : 107200 loss : 45.186 NLL : -45.186 
iteration : 107400 loss : 49.292 NLL : -49.292 
iteration : 107600 loss : 41.859 NLL : -41.859 
iteration : 107800 loss : 48.949 NLL : -48.949 
iteration : 108000 loss : 46.899 NLL : -46.899 
iteration : 108200 loss : 44.449 NLL : -44.449 
iteration : 108400 loss : 53.277 NLL : -53.277 
iteration : 108600 loss : 32.319 NLL : -32.319 
iteration : 108800 loss : 48.466 NLL : -48.466 
iteration : 109000 loss : 60.799 NLL : -60.799 
iteration : 109200 loss : 53.489 NLL : -53.489 
iteration : 109400 loss : 123.021 NLL : -123.021 
iteration : 109600 loss : 43.305 NLL : -43.305 
iteration : 109800 loss : 48.076 NLL : -48.076 
iteration : 110000 loss : 53.194 NLL : -53.194 
iteration : 110200 loss : 18.776 NLL : -18.776 
iteration : 110400 loss : 44.134 NLL : -44.134 
iteration : 110600 loss : 46.899 NLL : -46.899 
iteration : 110800 loss : 33.447 NLL : -33.447 
iteration : 111000 loss : 45.549 NLL : -45.549 
iteration : 111200 loss : 53.089 NLL : -53.089 
iteration : 111400 loss : 58.711 NLL : -58.711 
iteration : 111600 loss : 45.691 NLL : -45.691 
iteration : 111800 loss : 40.002 NLL : -40.002 
iteration : 112000 loss : 40.521 NLL : -40.521 
iteration : 112200 loss : 30.462 NLL : -30.462 
iteration : 112400 loss : 51.572 NLL : -51.572 
iteration : 112600 loss : 44.429 NLL : -44.429 
iteration : 112800 loss : 54.161 NLL : -54.161 
iteration : 113000 loss : 58.155 NLL : -58.155 
iteration : 113200 loss : 46.327 NLL : -46.327 
iteration : 113400 loss : 46.241 NLL : -46.241 
iteration : 113600 loss : 49.322 NLL : -49.322 
iteration : 113800 loss : 20.678 NLL : -20.678 
iteration : 114000 loss : 53.862 NLL : -53.862 
iteration : 114200 loss : 34.653 NLL : -34.653 
iteration : 114400 loss : 38.855 NLL : -38.855 
iteration : 114600 loss : 54.456 NLL : -54.456 
iteration : 114800 loss : 62.967 NLL : -62.967 
iteration : 115000 loss : 58.439 NLL : -58.439 
iteration : 115200 loss : 60.870 NLL : -60.870 
iteration : 115400 loss : 66.953 NLL : -66.953 
iteration : 115600 loss : 47.137 NLL : -47.137 
iteration : 115800 loss : 58.800 NLL : -58.800 
iteration : 116000 loss : 35.584 NLL : -35.584 
iteration : 116200 loss : 20.890 NLL : -20.890 
iteration : 116400 loss : 57.608 NLL : -57.608 
iteration : 116600 loss : 31.294 NLL : -31.294 
iteration : 116800 loss : 37.809 NLL : -37.809 
iteration : 117000 loss : 33.719 NLL : -33.719 
iteration : 117200 loss : 43.035 NLL : -43.035 
iteration : 117400 loss : 51.932 NLL : -51.932 
iteration : 117600 loss : 41.252 NLL : -41.252 
iteration : 117800 loss : 52.971 NLL : -52.971 
iteration : 118000 loss : 46.675 NLL : -46.675 
iteration : 118200 loss : 46.504 NLL : -46.504 
iteration : 118400 loss : 33.634 NLL : -33.634 
iteration : 118600 loss : 53.449 NLL : -53.449 
iteration : 118800 loss : 60.560 NLL : -60.560 
iteration : 119000 loss : 49.008 NLL : -49.008 
iteration : 119200 loss : 31.314 NLL : -31.314 
iteration : 119400 loss : 56.800 NLL : -56.800 
iteration : 119600 loss : 72.752 NLL : -72.752 
iteration : 119800 loss : 66.777 NLL : -66.777 
iteration : 120000 loss : 32.309 NLL : -32.309 
---------- Training loss 44.216 updated ! and save the model! (step:120000) ----------
---------- Training loss 42.678 updated ! and save the model! (step:120012) ----------
---------- Training loss 39.501 updated ! and save the model! (step:120114) ----------
---------- Training loss 38.811 updated ! and save the model! (step:120150) ----------
iteration : 120200 loss : 38.593 NLL : -38.593 
---------- Training loss 37.948 updated ! and save the model! (step:120372) ----------
iteration : 120400 loss : 42.174 NLL : -42.174 
---------- Training loss 34.855 updated ! and save the model! (step:120468) ----------
iteration : 120600 loss : 53.863 NLL : -53.863 
iteration : 120800 loss : 59.894 NLL : -59.894 
iteration : 121000 loss : 53.051 NLL : -53.051 
iteration : 121200 loss : 33.069 NLL : -33.069 
iteration : 121400 loss : 53.078 NLL : -53.078 
---------- Training loss 33.621 updated ! and save the model! (step:121584) ----------
iteration : 121600 loss : 55.791 NLL : -55.791 
iteration : 121800 loss : 21.404 NLL : -21.404 
iteration : 122000 loss : 36.529 NLL : -36.529 
iteration : 122200 loss : 46.803 NLL : -46.803 
iteration : 122400 loss : 38.027 NLL : -38.027 
iteration : 122600 loss : 46.581 NLL : -46.581 
iteration : 122800 loss : 55.628 NLL : -55.628 
iteration : 123000 loss : 52.796 NLL : -52.796 
iteration : 123200 loss : 51.829 NLL : -51.829 
iteration : 123400 loss : 49.865 NLL : -49.865 
iteration : 123600 loss : 54.182 NLL : -54.182 
iteration : 123800 loss : 38.486 NLL : -38.486 
iteration : 124000 loss : 49.909 NLL : -49.909 
iteration : 124200 loss : 53.517 NLL : -53.517 
iteration : 124400 loss : 37.869 NLL : -37.869 
iteration : 124600 loss : 54.546 NLL : -54.546 
iteration : 124800 loss : 42.816 NLL : -42.816 
iteration : 125000 loss : 67.506 NLL : -67.506 
iteration : 125200 loss : 59.692 NLL : -59.692 
iteration : 125400 loss : 66.670 NLL : -66.670 
iteration : 125600 loss : 100.276 NLL : -100.276 
iteration : 125800 loss : 67.935 NLL : -67.935 
iteration : 126000 loss : 81.828 NLL : -81.828 
iteration : 126200 loss : 127.894 NLL : -127.894 
iteration : 126400 loss : 110.119 NLL : -110.119 
iteration : 126600 loss : 81.693 NLL : -81.693 
iteration : 126800 loss : 70.560 NLL : -70.560 
iteration : 127000 loss : 69.663 NLL : -69.663 
iteration : 127200 loss : 94.393 NLL : -94.393 
iteration : 127400 loss : 95.192 NLL : -95.192 
iteration : 127600 loss : 58.164 NLL : -58.164 
iteration : 127800 loss : 67.726 NLL : -67.726 
iteration : 128000 loss : 48.806 NLL : -48.806 
iteration : 128200 loss : 103.501 NLL : -103.501 
iteration : 128400 loss : 59.683 NLL : -59.683 
iteration : 128600 loss : 58.763 NLL : -58.763 
iteration : 128800 loss : 88.272 NLL : -88.272 
iteration : 129000 loss : 79.212 NLL : -79.212 
iteration : 129200 loss : 46.060 NLL : -46.060 
iteration : 129400 loss : 47.450 NLL : -47.450 
iteration : 129600 loss : 40.070 NLL : -40.070 
iteration : 129800 loss : 73.758 NLL : -73.758 
iteration : 130000 loss : 79.473 NLL : -79.473 
iteration : 130200 loss : 57.086 NLL : -57.086 
iteration : 130400 loss : 55.949 NLL : -55.949 
iteration : 130600 loss : 65.026 NLL : -65.026 
iteration : 130800 loss : 86.717 NLL : -86.717 
iteration : 131000 loss : 69.324 NLL : -69.324 
iteration : 131200 loss : 40.198 NLL : -40.198 
iteration : 131400 loss : 85.957 NLL : -85.957 
iteration : 131600 loss : 64.684 NLL : -64.684 
iteration : 131800 loss : 43.748 NLL : -43.748 
iteration : 132000 loss : 82.211 NLL : -82.211 
iteration : 132200 loss : 51.835 NLL : -51.835 
iteration : 132400 loss : 75.449 NLL : -75.449 
iteration : 132600 loss : 75.193 NLL : -75.193 
iteration : 132800 loss : 68.684 NLL : -68.684 
iteration : 133000 loss : 68.230 NLL : -68.230 
iteration : 133200 loss : 60.305 NLL : -60.305 
iteration : 133400 loss : 80.827 NLL : -80.827 
iteration : 133600 loss : 127.405 NLL : -127.405 
iteration : 133800 loss : 82.976 NLL : -82.976 
iteration : 134000 loss : 45.585 NLL : -45.585 
iteration : 134200 loss : 78.189 NLL : -78.189 
iteration : 134400 loss : 43.276 NLL : -43.276 
iteration : 134600 loss : 91.194 NLL : -91.194 
iteration : 134800 loss : 77.322 NLL : -77.322 
iteration : 135000 loss : 68.420 NLL : -68.420 
iteration : 135200 loss : 89.941 NLL : -89.941 
iteration : 135400 loss : 86.670 NLL : -86.670 
iteration : 135600 loss : 94.977 NLL : -94.977 
iteration : 135800 loss : 65.081 NLL : -65.081 
iteration : 136000 loss : 75.691 NLL : -75.691 
iteration : 136200 loss : 131.643 NLL : -131.643 
iteration : 136400 loss : 91.342 NLL : -91.342 
iteration : 136600 loss : 68.291 NLL : -68.291 
iteration : 136800 loss : 84.474 NLL : -84.474 
iteration : 137000 loss : 53.737 NLL : -53.737 
iteration : 137200 loss : 69.642 NLL : -69.642 
iteration : 137400 loss : 76.872 NLL : -76.872 
iteration : 137600 loss : 65.425 NLL : -65.425 
iteration : 137800 loss : 281.891 NLL : -281.891 
iteration : 138000 loss : 450.199 NLL : -450.199 
iteration : 138200 loss : 72.564 NLL : -72.564 
iteration : 138400 loss : 72.540 NLL : -72.540 
iteration : 138600 loss : 148.931 NLL : -148.931 
iteration : 138800 loss : 84.536 NLL : -84.536 
iteration : 139000 loss : 81.609 NLL : -81.609 
iteration : 139200 loss : 82.176 NLL : -82.176 
iteration : 139400 loss : 77.458 NLL : -77.458 
iteration : 139600 loss : 89.564 NLL : -89.564 
iteration : 139800 loss : 44.140 NLL : -44.140 
iteration : 140000 loss : 68.090 NLL : -68.090 
iteration : 140200 loss : 62.811 NLL : -62.811 
iteration : 140400 loss : 36.733 NLL : -36.733 
iteration : 140600 loss : 67.917 NLL : -67.917 
iteration : 140800 loss : 35.683 NLL : -35.683 
iteration : 141000 loss : 81.996 NLL : -81.996 
iteration : 141200 loss : 52.837 NLL : -52.837 
iteration : 141400 loss : 108.248 NLL : -108.248 
iteration : 141600 loss : 75.246 NLL : -75.246 
iteration : 141800 loss : 71.450 NLL : -71.450 
iteration : 142000 loss : 36.853 NLL : -36.853 
iteration : 142200 loss : 69.894 NLL : -69.894 
iteration : 142400 loss : 74.379 NLL : -74.379 
iteration : 142600 loss : 75.485 NLL : -75.485 
iteration : 142800 loss : 90.978 NLL : -90.978 
iteration : 143000 loss : 53.299 NLL : -53.299 
iteration : 143200 loss : 94.570 NLL : -94.570 
iteration : 143400 loss : 70.538 NLL : -70.538 
iteration : 143600 loss : 35.076 NLL : -35.076 
iteration : 143800 loss : 57.906 NLL : -57.906 
iteration : 144000 loss : 79.340 NLL : -79.340 
iteration : 144200 loss : 42.657 NLL : -42.657 
iteration : 144400 loss : 110.589 NLL : -110.589 
iteration : 144600 loss : 83.424 NLL : -83.424 
iteration : 144800 loss : 88.533 NLL : -88.533 
iteration : 145000 loss : 88.447 NLL : -88.447 
iteration : 145200 loss : 68.082 NLL : -68.082 
iteration : 145400 loss : 63.081 NLL : -63.081 
iteration : 145600 loss : 604.439 NLL : -604.439 
iteration : 145800 loss : 56.400 NLL : -56.400 
iteration : 146000 loss : 79.625 NLL : -79.625 
iteration : 146200 loss : 40.213 NLL : -40.213 
iteration : 146400 loss : 90.553 NLL : -90.553 
iteration : 146600 loss : 77.043 NLL : -77.043 
iteration : 146800 loss : 139.258 NLL : -139.258 
iteration : 147000 loss : 76.049 NLL : -76.049 
iteration : 147200 loss : 92.645 NLL : -92.645 
iteration : 147400 loss : 178.662 NLL : -178.662 
iteration : 147600 loss : 101.774 NLL : -101.774 
iteration : 147800 loss : 100.770 NLL : -100.770 
iteration : 148000 loss : 49.261 NLL : -49.261 
iteration : 148200 loss : 5400.617 NLL : -5400.617 
iteration : 148400 loss : 63.666 NLL : -63.666 
iteration : 148600 loss : 76.057 NLL : -76.057 
iteration : 148800 loss : 100.624 NLL : -100.624 
iteration : 149000 loss : 89.172 NLL : -89.172 
iteration : 149200 loss : 83.745 NLL : -83.745 
iteration : 149400 loss : 71.901 NLL : -71.901 
iteration : 149600 loss : 79.451 NLL : -79.451 
iteration : 149800 loss : 81.779 NLL : -81.779 
iteration : 150000 loss : 49.391 NLL : -49.391 
---------- Training loss 80.180 updated ! and save the model! (step:150000) ----------
---------- Training loss 77.698 updated ! and save the model! (step:150006) ----------
---------- Training loss 74.555 updated ! and save the model! (step:150012) ----------
---------- Training loss 73.935 updated ! and save the model! (step:150030) ----------
---------- Training loss 70.509 updated ! and save the model! (step:150036) ----------
---------- Training loss 70.380 updated ! and save the model! (step:150054) ----------
---------- Training loss 55.913 updated ! and save the model! (step:150066) ----------
iteration : 150200 loss : 54.719 NLL : -54.719 
iteration : 150400 loss : 97.340 NLL : -97.340 
---------- Training loss 55.013 updated ! and save the model! (step:150594) ----------
iteration : 150600 loss : 53.061 NLL : -53.061 
iteration : 150800 loss : 55.503 NLL : -55.503 
iteration : 151000 loss : 66.403 NLL : -66.403 
iteration : 151200 loss : 63.308 NLL : -63.308 
---------- Training loss 42.194 updated ! and save the model! (step:151200) ----------
iteration : 151400 loss : 96.046 NLL : -96.046 
iteration : 151600 loss : 81.328 NLL : -81.328 
iteration : 151800 loss : 79.752 NLL : -79.752 
iteration : 152000 loss : 86.817 NLL : -86.817 
iteration : 152200 loss : 99.013 NLL : -99.013 
iteration : 152400 loss : 51.921 NLL : -51.921 
iteration : 152600 loss : 121.488 NLL : -121.488 
iteration : 152800 loss : 65.165 NLL : -65.165 
iteration : 153000 loss : 69.945 NLL : -69.945 
iteration : 153200 loss : 1414.333 NLL : -1414.333 
iteration : 153400 loss : 68.071 NLL : -68.071 
iteration : 153600 loss : 66.573 NLL : -66.573 
iteration : 153800 loss : 79.151 NLL : -79.151 
iteration : 154000 loss : 82.563 NLL : -82.563 
iteration : 154200 loss : 402.044 NLL : -402.044 
iteration : 154400 loss : 53.831 NLL : -53.831 
iteration : 154600 loss : 110.518 NLL : -110.518 
iteration : 154800 loss : 88.879 NLL : -88.879 
iteration : 155000 loss : 146.227 NLL : -146.227 
iteration : 155200 loss : 73.438 NLL : -73.438 
iteration : 155400 loss : 96.726 NLL : -96.726 
iteration : 155600 loss : 91.117 NLL : -91.117 
iteration : 155800 loss : 72.482 NLL : -72.482 
iteration : 156000 loss : 122.144 NLL : -122.144 
iteration : 156200 loss : 37.544 NLL : -37.544 
iteration : 156400 loss : 97.807 NLL : -97.807 
iteration : 156600 loss : 100.829 NLL : -100.829 
iteration : 156800 loss : 105.745 NLL : -105.745 
iteration : 157000 loss : 52.049 NLL : -52.049 
iteration : 157200 loss : 90.513 NLL : -90.513 
iteration : 157400 loss : 98.488 NLL : -98.488 
iteration : 157600 loss : 79.690 NLL : -79.690 
iteration : 157800 loss : 127.946 NLL : -127.946 
iteration : 158000 loss : 77.563 NLL : -77.563 
iteration : 158200 loss : 118.366 NLL : -118.366 
iteration : 158400 loss : 60.128 NLL : -60.128 
iteration : 158600 loss : 3753.815 NLL : -3753.815 
iteration : 158800 loss : 3820.148 NLL : -3820.148 
iteration : 159000 loss : 4582.907 NLL : -4582.907 
iteration : 159200 loss : 1356.093 NLL : -1356.093 
iteration : 159400 loss : 86.011 NLL : -86.011 
iteration : 159600 loss : 70.435 NLL : -70.435 
iteration : 159800 loss : 82.629 NLL : -82.629 
iteration : 160000 loss : 169.698 NLL : -169.698 
iteration : 160200 loss : 88.024 NLL : -88.024 
iteration : 160400 loss : 97.930 NLL : -97.930 
iteration : 160600 loss : 110.780 NLL : -110.780 
iteration : 160800 loss : 98.554 NLL : -98.554 
iteration : 161000 loss : 74.176 NLL : -74.176 
iteration : 161200 loss : 62.912 NLL : -62.912 
iteration : 161400 loss : 83.405 NLL : -83.405 
iteration : 161600 loss : 104.080 NLL : -104.080 
iteration : 161800 loss : 94.512 NLL : -94.512 
iteration : 162000 loss : 64.183 NLL : -64.183 
iteration : 162200 loss : 93.498 NLL : -93.498 
iteration : 162400 loss : 86.928 NLL : -86.928 
iteration : 162600 loss : 114.656 NLL : -114.656 
iteration : 162800 loss : 77.886 NLL : -77.886 
iteration : 163000 loss : 86.701 NLL : -86.701 
iteration : 163200 loss : 47.906 NLL : -47.906 
iteration : 163400 loss : 94.985 NLL : -94.985 
iteration : 163600 loss : 85.419 NLL : -85.419 
iteration : 163800 loss : 58.711 NLL : -58.711 
iteration : 164000 loss : 81.769 NLL : -81.769 
iteration : 164200 loss : 189.602 NLL : -189.602 
iteration : 164400 loss : 162.641 NLL : -162.641 
iteration : 164600 loss : 178.120 NLL : -178.120 
iteration : 164800 loss : 161.496 NLL : -161.496 
iteration : 165000 loss : 196.872 NLL : -196.872 
iteration : 165200 loss : 170.267 NLL : -170.267 
iteration : 165400 loss : 210.272 NLL : -210.272 
iteration : 165600 loss : 167.733 NLL : -167.733 
iteration : 165800 loss : 191.471 NLL : -191.471 
iteration : 166000 loss : 121.345 NLL : -121.345 
iteration : 166200 loss : 130.619 NLL : -130.619 
iteration : 166400 loss : 160.430 NLL : -160.430 
iteration : 166600 loss : 183.792 NLL : -183.792 
iteration : 166800 loss : 202.293 NLL : -202.293 
iteration : 167000 loss : 97.421 NLL : -97.421 
iteration : 167200 loss : 161.778 NLL : -161.778 
iteration : 167400 loss : 181.611 NLL : -181.611 
iteration : 167600 loss : 126.840 NLL : -126.840 
iteration : 167800 loss : 191.387 NLL : -191.387 
iteration : 168000 loss : 166.046 NLL : -166.046 
iteration : 168200 loss : 84.120 NLL : -84.120 
iteration : 168400 loss : 112.558 NLL : -112.558 
iteration : 168600 loss : 176.647 NLL : -176.647 
iteration : 168800 loss : 146.065 NLL : -146.065 
iteration : 169000 loss : 165.740 NLL : -165.740 
iteration : 169200 loss : 193.094 NLL : -193.094 
iteration : 169400 loss : 169.904 NLL : -169.904 
iteration : 169600 loss : 197.625 NLL : -197.625 
iteration : 169800 loss : 138.449 NLL : -138.449 
iteration : 170000 loss : 182.763 NLL : -182.763 
iteration : 170200 loss : 210.432 NLL : -210.432 
iteration : 170400 loss : 176.297 NLL : -176.297 
iteration : 170600 loss : 201.460 NLL : -201.460 
iteration : 170800 loss : 195.952 NLL : -195.952 
iteration : 171000 loss : 167.276 NLL : -167.276 
iteration : 171200 loss : 177.938 NLL : -177.938 
iteration : 171400 loss : 253.813 NLL : -253.813 
iteration : 171600 loss : 248.516 NLL : -248.516 
iteration : 171800 loss : 219.755 NLL : -219.755 
iteration : 172000 loss : 194.565 NLL : -194.565 
iteration : 172200 loss : 205.126 NLL : -205.126 
iteration : 172400 loss : 175.574 NLL : -175.574 
iteration : 172600 loss : 142.599 NLL : -142.599 
iteration : 172800 loss : 202.698 NLL : -202.698 
iteration : 173000 loss : 252.017 NLL : -252.017 
iteration : 173200 loss : 132.281 NLL : -132.281 
iteration : 173400 loss : 199.834 NLL : -199.834 
iteration : 173600 loss : 255.093 NLL : -255.093 
iteration : 173800 loss : 110.541 NLL : -110.541 
iteration : 174000 loss : 165.666 NLL : -165.666 
iteration : 174200 loss : 178.649 NLL : -178.649 
iteration : 174400 loss : 163.767 NLL : -163.767 
iteration : 174600 loss : 123.764 NLL : -123.764 
iteration : 174800 loss : 219.957 NLL : -219.957 
iteration : 175000 loss : 134.535 NLL : -134.535 
iteration : 175200 loss : 233.424 NLL : -233.424 
iteration : 175400 loss : 147.854 NLL : -147.854 
iteration : 175600 loss : 236.177 NLL : -236.177 
iteration : 175800 loss : 216.271 NLL : -216.271 
iteration : 176000 loss : 215.418 NLL : -215.418 
iteration : 176200 loss : 257.438 NLL : -257.438 
iteration : 176400 loss : 241.637 NLL : -241.637 
iteration : 176600 loss : 231.561 NLL : -231.561 
iteration : 176800 loss : 162.451 NLL : -162.451 
iteration : 177000 loss : 162.230 NLL : -162.230 
iteration : 177200 loss : 94.015 NLL : -94.015 
iteration : 177400 loss : 182.089 NLL : -182.089 
iteration : 177600 loss : 229.659 NLL : -229.659 
iteration : 177800 loss : 183.744 NLL : -183.744 
iteration : 178000 loss : 120.887 NLL : -120.887 
iteration : 178200 loss : 210.888 NLL : -210.888 
iteration : 178400 loss : 76.930 NLL : -76.930 
iteration : 178600 loss : 81.137 NLL : -81.137 
iteration : 178800 loss : 73.465 NLL : -73.465 
iteration : 179000 loss : 76.392 NLL : -76.392 
iteration : 179200 loss : 70.155 NLL : -70.155 
iteration : 179400 loss : 56.497 NLL : -56.497 
iteration : 179600 loss : 90.593 NLL : -90.593 
iteration : 179800 loss : 80.451 NLL : -80.451 
iteration : 180000 loss : 102.078 NLL : -102.078 
---------- Training loss 96.700 updated ! and save the model! (step:180000) ----------
---------- Training loss 75.207 updated ! and save the model! (step:180006) ----------
---------- Training loss 71.365 updated ! and save the model! (step:180012) ----------
---------- Training loss 68.162 updated ! and save the model! (step:180078) ----------
---------- Training loss 66.633 updated ! and save the model! (step:180138) ----------
iteration : 180200 loss : 89.044 NLL : -89.044 
---------- Training loss 61.562 updated ! and save the model! (step:180204) ----------
iteration : 180400 loss : 106.213 NLL : -106.213 
iteration : 180600 loss : 51.579 NLL : -51.579 
---------- Training loss 56.111 updated ! and save the model! (step:180726) ----------
iteration : 180800 loss : 40.446 NLL : -40.446 
iteration : 181000 loss : 82.636 NLL : -82.636 
iteration : 181200 loss : 66.339 NLL : -66.339 
---------- Training loss 55.124 updated ! and save the model! (step:181338) ----------
iteration : 181400 loss : 79.472 NLL : -79.472 
iteration : 181600 loss : 71.718 NLL : -71.718 
iteration : 181800 loss : 72.515 NLL : -72.515 
iteration : 182000 loss : 73.741 NLL : -73.741 
iteration : 182200 loss : 94.517 NLL : -94.517 
iteration : 182400 loss : 66.858 NLL : -66.858 
iteration : 182600 loss : 67.850 NLL : -67.850 
iteration : 182800 loss : 70.408 NLL : -70.408 
iteration : 183000 loss : 82.002 NLL : -82.002 
iteration : 183200 loss : 59.031 NLL : -59.031 
iteration : 183400 loss : 82.745 NLL : -82.745 
iteration : 183600 loss : 76.631 NLL : -76.631 
iteration : 183800 loss : 61.499 NLL : -61.499 
iteration : 184000 loss : 86.411 NLL : -86.411 
iteration : 184200 loss : 66.404 NLL : -66.404 
iteration : 184400 loss : 68.144 NLL : -68.144 
iteration : 184600 loss : 48.654 NLL : -48.654 
iteration : 184800 loss : 114.687 NLL : -114.687 
---------- Training loss 52.297 updated ! and save the model! (step:184944) ----------
iteration : 185000 loss : 71.454 NLL : -71.454 
iteration : 185200 loss : 69.682 NLL : -69.682 
iteration : 185400 loss : 74.583 NLL : -74.583 
iteration : 185600 loss : 69.587 NLL : -69.587 
iteration : 185800 loss : 94.128 NLL : -94.128 
iteration : 186000 loss : 83.383 NLL : -83.383 
iteration : 186200 loss : 52.360 NLL : -52.360 
iteration : 186400 loss : 50.764 NLL : -50.764 
iteration : 186600 loss : 59.138 NLL : -59.138 
iteration : 186800 loss : 68.189 NLL : -68.189 
iteration : 187000 loss : 85.854 NLL : -85.854 
iteration : 187200 loss : 51.586 NLL : -51.586 
iteration : 187400 loss : 74.250 NLL : -74.250 
iteration : 187600 loss : 48.113 NLL : -48.113 
iteration : 187800 loss : 61.316 NLL : -61.316 
iteration : 188000 loss : 89.051 NLL : -89.051 
iteration : 188200 loss : 65.815 NLL : -65.815 
iteration : 188400 loss : 88.755 NLL : -88.755 
---------- Training loss 51.217 updated ! and save the model! (step:188460) ----------
iteration : 188600 loss : 72.021 NLL : -72.021 
iteration : 188800 loss : 68.338 NLL : -68.338 
iteration : 189000 loss : 55.755 NLL : -55.755 
iteration : 189200 loss : 70.520 NLL : -70.520 
iteration : 189400 loss : 71.092 NLL : -71.092 
iteration : 189600 loss : 66.795 NLL : -66.795 
iteration : 189800 loss : 38.548 NLL : -38.548 
iteration : 190000 loss : 86.986 NLL : -86.986 
iteration : 190200 loss : 64.491 NLL : -64.491 
---------- Training loss 50.699 updated ! and save the model! (step:190326) ----------
iteration : 190400 loss : 75.345 NLL : -75.345 
---------- Training loss 49.701 updated ! and save the model! (step:190464) ----------
iteration : 190600 loss : 55.226 NLL : -55.226 
iteration : 190800 loss : 94.190 NLL : -94.190 
iteration : 191000 loss : 40.194 NLL : -40.194 
iteration : 191200 loss : 62.161 NLL : -62.161 
iteration : 191400 loss : 46.146 NLL : -46.146 
iteration : 191600 loss : 62.242 NLL : -62.242 
iteration : 191800 loss : 70.920 NLL : -70.920 
iteration : 192000 loss : 72.770 NLL : -72.770 
iteration : 192200 loss : 59.738 NLL : -59.738 
iteration : 192400 loss : 47.973 NLL : -47.973 
iteration : 192600 loss : 53.593 NLL : -53.593 
iteration : 192800 loss : 92.221 NLL : -92.221 
iteration : 193000 loss : 78.447 NLL : -78.447 
iteration : 193200 loss : 86.231 NLL : -86.231 
iteration : 193400 loss : 38.478 NLL : -38.478 
iteration : 193600 loss : 88.444 NLL : -88.444 
iteration : 193800 loss : 79.591 NLL : -79.591 
iteration : 194000 loss : 77.557 NLL : -77.557 
iteration : 194200 loss : 78.339 NLL : -78.339 
iteration : 194400 loss : 71.511 NLL : -71.511 
iteration : 194600 loss : 88.288 NLL : -88.288 
iteration : 194800 loss : 85.368 NLL : -85.368 
iteration : 195000 loss : 64.333 NLL : -64.333 
iteration : 195200 loss : 36.692 NLL : -36.692 
iteration : 195400 loss : 69.987 NLL : -69.987 
iteration : 195600 loss : 74.875 NLL : -74.875 
iteration : 195800 loss : 93.951 NLL : -93.951 
iteration : 196000 loss : 55.761 NLL : -55.761 
---------- Training loss 46.633 updated ! and save the model! (step:196008) ----------
iteration : 196200 loss : 66.804 NLL : -66.804 
---------- Training loss 46.628 updated ! and save the model! (step:196218) ----------
iteration : 196400 loss : 88.398 NLL : -88.398 
iteration : 196600 loss : 59.100 NLL : -59.100 
iteration : 196800 loss : 71.026 NLL : -71.026 
iteration : 197000 loss : 104.193 NLL : -104.193 
iteration : 197200 loss : 77.465 NLL : -77.465 
iteration : 197400 loss : 78.313 NLL : -78.313 
iteration : 197600 loss : 97.162 NLL : -97.162 
iteration : 197800 loss : 94.423 NLL : -94.423 
iteration : 198000 loss : 44.043 NLL : -44.043 
iteration : 198200 loss : 96.293 NLL : -96.293 
iteration : 198400 loss : 82.809 NLL : -82.809 
iteration : 198600 loss : 82.811 NLL : -82.811 
iteration : 198800 loss : 52.706 NLL : -52.706 
iteration : 199000 loss : 38.413 NLL : -38.413 
iteration : 199200 loss : 87.432 NLL : -87.432 
iteration : 199400 loss : 46.707 NLL : -46.707 
iteration : 199600 loss : 54.829 NLL : -54.829 
iteration : 199800 loss : 78.576 NLL : -78.576 
iteration : 200000 loss : 101.259 NLL : -101.259 
---------- Save the model! (step:None) ----------
