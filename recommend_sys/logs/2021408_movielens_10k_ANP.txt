---------- Training loss 41330.796 updated ! and save the model! (step:6) ----------
---------- Training loss 1074.572 updated ! and save the model! (step:12) ----------
---------- Training loss 845.613 updated ! and save the model! (step:18) ----------
---------- Training loss 143.446 updated ! and save the model! (step:24) ----------
---------- Training loss 124.806 updated ! and save the model! (step:42) ----------
---------- Training loss 120.062 updated ! and save the model! (step:54) ----------
---------- Training loss 116.837 updated ! and save the model! (step:72) ----------
---------- Training loss 104.637 updated ! and save the model! (step:78) ----------
---------- Training loss 100.683 updated ! and save the model! (step:90) ----------
---------- Training loss 95.939 updated ! and save the model! (step:96) ----------
---------- Training loss 93.552 updated ! and save the model! (step:108) ----------
---------- Training loss 92.648 updated ! and save the model! (step:120) ----------
---------- Training loss 91.032 updated ! and save the model! (step:126) ----------
---------- Training loss 85.125 updated ! and save the model! (step:150) ----------
---------- Training loss 79.878 updated ! and save the model! (step:162) ----------
---------- Training loss 76.341 updated ! and save the model! (step:168) ----------
---------- Training loss 63.523 updated ! and save the model! (step:180) ----------
---------- Training loss 50.213 updated ! and save the model! (step:192) ----------
iteration : 200 loss : 90.830 NLL : -71.823 KLD : 19.007 
---------- Training loss 50.062 updated ! and save the model! (step:258) ----------
---------- Training loss 48.130 updated ! and save the model! (step:264) ----------
iteration : 400 loss : 52.108 NLL : -51.974 KLD : 0.133 
---------- Training loss 38.381 updated ! and save the model! (step:426) ----------
iteration : 600 loss : 51.774 NLL : -51.726 KLD : 0.048 
iteration : 800 loss : 54.529 NLL : -54.517 KLD : 0.012 
---------- Training loss 34.363 updated ! and save the model! (step:930) ----------
iteration : 1000 loss : 40.171 NLL : -40.150 KLD : 0.021 
iteration : 1200 loss : 49.535 NLL : -49.517 KLD : 0.018 
iteration : 1400 loss : 37.536 NLL : -37.470 KLD : 0.065 
iteration : 1600 loss : 54.423 NLL : -54.058 KLD : 0.365 
iteration : 1800 loss : 60.704 NLL : -60.329 KLD : 0.375 
iteration : 2000 loss : 129.068 NLL : -127.089 KLD : 1.978 
iteration : 2200 loss : 58.575 NLL : -58.501 KLD : 0.074 
iteration : 2400 loss : 61.841 NLL : -61.831 KLD : 0.010 
iteration : 2600 loss : 54.738 NLL : -54.727 KLD : 0.011 
iteration : 2800 loss : 55.807 NLL : -55.755 KLD : 0.052 
iteration : 3000 loss : 72.840 NLL : -72.775 KLD : 0.066 
iteration : 3200 loss : 85.472 NLL : -85.327 KLD : 0.145 
iteration : 3400 loss : 40.430 NLL : -40.332 KLD : 0.098 
iteration : 3600 loss : 54.999 NLL : -54.962 KLD : 0.037 
iteration : 3800 loss : 57.657 NLL : -57.647 KLD : 0.009 
iteration : 4000 loss : 51.550 NLL : -51.521 KLD : 0.029 
iteration : 4200 loss : 37.726 NLL : -37.714 KLD : 0.012 
---------- Training loss 30.907 updated ! and save the model! (step:4350) ----------
iteration : 4400 loss : 51.628 NLL : -51.612 KLD : 0.016 
iteration : 4600 loss : 55.918 NLL : -55.899 KLD : 0.019 
iteration : 4800 loss : 47.473 NLL : -47.467 KLD : 0.006 
iteration : 5000 loss : 49.808 NLL : -49.793 KLD : 0.014 
iteration : 5200 loss : 61.560 NLL : -61.524 KLD : 0.036 
iteration : 5400 loss : 53.371 NLL : -53.228 KLD : 0.143 
iteration : 5600 loss : 77.302 NLL : -77.281 KLD : 0.021 
iteration : 5800 loss : 70.583 NLL : -70.549 KLD : 0.034 
iteration : 6000 loss : 55.306 NLL : -55.294 KLD : 0.011 
iteration : 6200 loss : 69.239 NLL : -69.210 KLD : 0.029 
iteration : 6400 loss : 36.672 NLL : -36.655 KLD : 0.017 
iteration : 6600 loss : 48.660 NLL : -48.614 KLD : 0.046 
iteration : 6800 loss : 84.112 NLL : -83.599 KLD : 0.512 
iteration : 7000 loss : 56.161 NLL : -55.937 KLD : 0.224 
iteration : 7200 loss : 53.100 NLL : -53.069 KLD : 0.031 
iteration : 7400 loss : 64.033 NLL : -63.920 KLD : 0.114 
iteration : 7600 loss : 51.762 NLL : -51.476 KLD : 0.286 
iteration : 7800 loss : 25.361 NLL : -25.293 KLD : 0.068 
iteration : 8000 loss : 54.540 NLL : -54.442 KLD : 0.098 
iteration : 8200 loss : 34.570 NLL : -34.442 KLD : 0.128 
iteration : 8400 loss : 61.777 NLL : -61.747 KLD : 0.030 
iteration : 8600 loss : 68.821 NLL : -68.797 KLD : 0.024 
iteration : 8800 loss : 53.683 NLL : -53.650 KLD : 0.032 
iteration : 9000 loss : 47.897 NLL : -47.641 KLD : 0.256 
iteration : 9200 loss : 94.406 NLL : -94.161 KLD : 0.245 
iteration : 9400 loss : 33.814 NLL : -33.758 KLD : 0.056 
iteration : 9600 loss : 33.954 NLL : -33.136 KLD : 0.818 
iteration : 9800 loss : 54.626 NLL : -54.532 KLD : 0.094 
iteration : 10000 loss : 59.033 NLL : -58.325 KLD : 0.708 
iteration : 10200 loss : 36.832 NLL : -35.937 KLD : 0.895 
iteration : 10400 loss : 77.014 NLL : -76.794 KLD : 0.220 
iteration : 10600 loss : 85.538 NLL : -85.416 KLD : 0.122 
iteration : 10800 loss : 58.735 NLL : -58.700 KLD : 0.035 
iteration : 11000 loss : 48.965 NLL : -48.859 KLD : 0.106 
iteration : 11200 loss : 70.584 NLL : -70.371 KLD : 0.213 
iteration : 11400 loss : 102.995 NLL : -102.711 KLD : 0.284 
iteration : 11600 loss : 124.822 NLL : -122.500 KLD : 2.323 
iteration : 11800 loss : 115.736 NLL : -111.271 KLD : 4.465 
iteration : 12000 loss : 105.560 NLL : -105.184 KLD : 0.376 
iteration : 12200 loss : 169.645 NLL : -154.341 KLD : 15.304 
iteration : 12400 loss : 205.768 NLL : -190.740 KLD : 15.028 
iteration : 12600 loss : 148.795 NLL : -148.632 KLD : 0.163 
iteration : 12800 loss : 186.290 NLL : -185.804 KLD : 0.486 
iteration : 13000 loss : 105.970 NLL : -105.123 KLD : 0.847 
iteration : 13200 loss : 91.490 NLL : -91.149 KLD : 0.341 
iteration : 13400 loss : 51.142 NLL : -50.559 KLD : 0.583 
iteration : 13600 loss : 91.559 NLL : -90.012 KLD : 1.546 
iteration : 13800 loss : 69.251 NLL : -69.155 KLD : 0.096 
iteration : 14000 loss : 124.633 NLL : -122.105 KLD : 2.527 
iteration : 14200 loss : 145.397 NLL : -145.241 KLD : 0.156 
iteration : 14400 loss : 89.468 NLL : -89.297 KLD : 0.171 
iteration : 14600 loss : 85.198 NLL : -84.870 KLD : 0.328 
iteration : 14800 loss : 164.597 NLL : -156.861 KLD : 7.736 
iteration : 15000 loss : 114.279 NLL : -112.685 KLD : 1.594 
iteration : 15200 loss : 131.184 NLL : -126.221 KLD : 4.963 
iteration : 15400 loss : 85.144 NLL : -84.981 KLD : 0.162 
iteration : 15600 loss : 63.578 NLL : -62.960 KLD : 0.618 
iteration : 15800 loss : 57.872 NLL : -56.825 KLD : 1.046 
iteration : 16000 loss : 32.738 NLL : -31.760 KLD : 0.978 
iteration : 16200 loss : 85.429 NLL : -85.040 KLD : 0.389 
iteration : 16400 loss : 101.238 NLL : -101.162 KLD : 0.076 
iteration : 16600 loss : 73.782 NLL : -73.283 KLD : 0.498 
iteration : 16800 loss : 89.026 NLL : -88.726 KLD : 0.300 
iteration : 17000 loss : 82.228 NLL : -81.963 KLD : 0.265 
iteration : 17200 loss : 92.844 NLL : -92.145 KLD : 0.699 
iteration : 17400 loss : 66.167 NLL : -65.812 KLD : 0.355 
iteration : 17600 loss : 52.662 NLL : -52.473 KLD : 0.189 
iteration : 17800 loss : 158.298 NLL : -149.915 KLD : 8.383 
iteration : 18000 loss : 132.523 NLL : -129.898 KLD : 2.624 
iteration : 18200 loss : 74.154 NLL : -72.425 KLD : 1.729 
iteration : 18400 loss : 91.020 NLL : -90.207 KLD : 0.813 
iteration : 18600 loss : 131.787 NLL : -129.948 KLD : 1.839 
iteration : 18800 loss : 123.960 NLL : -123.714 KLD : 0.246 
iteration : 19000 loss : 136.428 NLL : -136.346 KLD : 0.082 
iteration : 19200 loss : 109.745 NLL : -109.181 KLD : 0.564 
iteration : 19400 loss : 90.688 NLL : -90.081 KLD : 0.607 
iteration : 19600 loss : 64.224 NLL : -62.789 KLD : 1.435 
iteration : 19800 loss : 127.054 NLL : -125.315 KLD : 1.739 
iteration : 20000 loss : 128.346 NLL : -127.806 KLD : 0.540 
iteration : 20200 loss : 144.192 NLL : -144.042 KLD : 0.150 
iteration : 20400 loss : 133.254 NLL : -127.013 KLD : 6.241 
iteration : 20600 loss : 168.690 NLL : -168.375 KLD : 0.315 
iteration : 20800 loss : 144.488 NLL : -144.321 KLD : 0.167 
iteration : 21000 loss : 155.689 NLL : -155.353 KLD : 0.336 
iteration : 21200 loss : 163.838 NLL : -163.800 KLD : 0.038 
iteration : 21400 loss : 121.253 NLL : -120.705 KLD : 0.548 
iteration : 21600 loss : 180.330 NLL : -180.310 KLD : 0.020 
iteration : 21800 loss : 139.295 NLL : -139.269 KLD : 0.025 
iteration : 22000 loss : 139.301 NLL : -139.252 KLD : 0.049 
iteration : 22200 loss : 134.091 NLL : -133.549 KLD : 0.542 
iteration : 22400 loss : 95.067 NLL : -94.852 KLD : 0.216 
iteration : 22600 loss : 104.877 NLL : -104.787 KLD : 0.090 
iteration : 22800 loss : 99.194 NLL : -99.144 KLD : 0.049 
iteration : 23000 loss : 75.525 NLL : -75.206 KLD : 0.320 
iteration : 23200 loss : 91.745 NLL : -91.666 KLD : 0.078 
iteration : 23400 loss : 222.342 NLL : -154.346 KLD : 67.995 
iteration : 23600 loss : 106.722 NLL : -88.429 KLD : 18.294 
iteration : 23800 loss : 92.830 NLL : -90.802 KLD : 2.028 
iteration : 24000 loss : 86.357 NLL : -85.615 KLD : 0.742 
iteration : 24200 loss : 124.973 NLL : -122.880 KLD : 2.093 
iteration : 24400 loss : 116.743 NLL : -116.170 KLD : 0.573 
iteration : 24600 loss : 95.529 NLL : -94.864 KLD : 0.665 
iteration : 24800 loss : 90.965 NLL : -89.525 KLD : 1.440 
iteration : 25000 loss : 65.123 NLL : -63.687 KLD : 1.436 
iteration : 25200 loss : 132.892 NLL : -104.162 KLD : 28.730 
iteration : 25400 loss : 72.064 NLL : -56.698 KLD : 15.365 
iteration : 25600 loss : 82.526 NLL : -76.328 KLD : 6.198 
iteration : 25800 loss : 46.896 NLL : -45.202 KLD : 1.694 
iteration : 26000 loss : 121.776 NLL : -120.663 KLD : 1.113 
iteration : 26200 loss : 58.588 NLL : -58.381 KLD : 0.207 
iteration : 26400 loss : 40.889 NLL : -39.369 KLD : 1.519 
iteration : 26600 loss : 63.759 NLL : -63.350 KLD : 0.409 
iteration : 26800 loss : 82.619 NLL : -80.279 KLD : 2.340 
iteration : 27000 loss : 92.950 NLL : -90.690 KLD : 2.260 
iteration : 27200 loss : 78.346 NLL : -78.106 KLD : 0.240 
iteration : 27400 loss : 124.587 NLL : -118.370 KLD : 6.217 
iteration : 27600 loss : 175.676 NLL : -136.048 KLD : 39.628 
iteration : 27800 loss : 115.923 NLL : -114.071 KLD : 1.853 
iteration : 28000 loss : 57.015 NLL : -55.772 KLD : 1.244 
iteration : 28200 loss : 102.127 NLL : -93.445 KLD : 8.682 
iteration : 28400 loss : 63.060 NLL : -61.232 KLD : 1.828 
iteration : 28600 loss : 72.001 NLL : -71.365 KLD : 0.637 
iteration : 28800 loss : 35.204 NLL : -34.314 KLD : 0.890 
iteration : 29000 loss : 62.770 NLL : -61.934 KLD : 0.836 
iteration : 29200 loss : 68.150 NLL : -67.663 KLD : 0.488 
iteration : 29400 loss : 56.316 NLL : -55.627 KLD : 0.689 
iteration : 29600 loss : 95.057 NLL : -94.265 KLD : 0.792 
iteration : 29800 loss : 69.302 NLL : -68.472 KLD : 0.830 
iteration : 30000 loss : 70.407 NLL : -67.926 KLD : 2.482 
---------- Training loss 64.672 updated ! and save the model! (step:30000) ----------
---------- Training loss 60.901 updated ! and save the model! (step:30024) ----------
---------- Training loss 57.080 updated ! and save the model! (step:30036) ----------
---------- Training loss 55.907 updated ! and save the model! (step:30048) ----------
---------- Training loss 55.870 updated ! and save the model! (step:30072) ----------
---------- Training loss 55.769 updated ! and save the model! (step:30102) ----------
---------- Training loss 46.280 updated ! and save the model! (step:30108) ----------
iteration : 30200 loss : 65.468 NLL : -65.177 KLD : 0.291 
---------- Training loss 44.891 updated ! and save the model! (step:30294) ----------
iteration : 30400 loss : 56.612 NLL : -55.131 KLD : 1.480 
iteration : 30600 loss : 113.024 NLL : -111.965 KLD : 1.059 
iteration : 30800 loss : 98.648 NLL : -96.939 KLD : 1.709 
iteration : 31000 loss : 67.454 NLL : -64.215 KLD : 3.239 
iteration : 31200 loss : 56.225 NLL : -55.264 KLD : 0.961 
iteration : 31400 loss : 55.961 NLL : -55.690 KLD : 0.271 
iteration : 31600 loss : 76.895 NLL : -76.699 KLD : 0.196 
iteration : 31800 loss : 107.623 NLL : -106.738 KLD : 0.884 
iteration : 32000 loss : 101.003 NLL : -100.228 KLD : 0.776 
iteration : 32200 loss : 98.419 NLL : -97.196 KLD : 1.223 
iteration : 32400 loss : 85.026 NLL : -83.624 KLD : 1.402 
iteration : 32600 loss : 67.795 NLL : -66.957 KLD : 0.838 
iteration : 32800 loss : 48.012 NLL : -46.560 KLD : 1.453 
iteration : 33000 loss : 112.391 NLL : -111.113 KLD : 1.278 
iteration : 33200 loss : 53.786 NLL : -53.269 KLD : 0.516 
---------- Training loss 43.624 updated ! and save the model! (step:33264) ----------
iteration : 33400 loss : 89.770 NLL : -89.486 KLD : 0.284 
iteration : 33600 loss : 84.840 NLL : -83.887 KLD : 0.952 
iteration : 33800 loss : 57.965 NLL : -57.516 KLD : 0.450 
iteration : 34000 loss : 84.712 NLL : -84.135 KLD : 0.577 
iteration : 34200 loss : 52.822 NLL : -52.566 KLD : 0.255 
iteration : 34400 loss : 64.025 NLL : -63.262 KLD : 0.763 
iteration : 34600 loss : 78.738 NLL : -77.823 KLD : 0.914 
iteration : 34800 loss : 114.607 NLL : -108.008 KLD : 6.599 
iteration : 35000 loss : 122.309 NLL : -60.665 KLD : 61.643 
iteration : 35200 loss : 54.211 NLL : -53.514 KLD : 0.697 
iteration : 35400 loss : 88.185 NLL : -86.296 KLD : 1.889 
iteration : 35600 loss : 54.320 NLL : -54.145 KLD : 0.176 
iteration : 35800 loss : 75.786 NLL : -69.359 KLD : 6.427 
iteration : 36000 loss : 65.140 NLL : -64.906 KLD : 0.234 
iteration : 36200 loss : 59.017 NLL : -58.887 KLD : 0.130 
iteration : 36400 loss : 113.210 NLL : -104.029 KLD : 9.182 
iteration : 36600 loss : 87.855 NLL : -82.217 KLD : 5.637 
iteration : 36800 loss : 78.123 NLL : -77.546 KLD : 0.578 
iteration : 37000 loss : 80.406 NLL : -80.243 KLD : 0.163 
iteration : 37200 loss : 64.245 NLL : -63.135 KLD : 1.110 
iteration : 37400 loss : 75.119 NLL : -74.902 KLD : 0.218 
iteration : 37600 loss : 75.179 NLL : -74.988 KLD : 0.191 
iteration : 37800 loss : 172.340 NLL : -171.275 KLD : 1.066 
iteration : 38000 loss : 96.668 NLL : -94.771 KLD : 1.898 
iteration : 38200 loss : 174.387 NLL : -173.657 KLD : 0.730 
iteration : 38400 loss : 116.226 NLL : -115.411 KLD : 0.816 
iteration : 38600 loss : 74.353 NLL : -69.356 KLD : 4.997 
iteration : 38800 loss : 142.944 NLL : -142.450 KLD : 0.495 
iteration : 39000 loss : 109.169 NLL : -104.787 KLD : 4.383 
iteration : 39200 loss : 147.594 NLL : -147.397 KLD : 0.197 
iteration : 39400 loss : 78.585 NLL : -78.414 KLD : 0.171 
iteration : 39600 loss : 138.468 NLL : -138.352 KLD : 0.116 
iteration : 39800 loss : 134.406 NLL : -132.625 KLD : 1.781 
iteration : 40000 loss : 59.859 NLL : -58.973 KLD : 0.886 
iteration : 40200 loss : 68.661 NLL : -68.151 KLD : 0.510 
iteration : 40400 loss : 107.279 NLL : -106.667 KLD : 0.612 
iteration : 40600 loss : 32.854 NLL : -32.406 KLD : 0.448 
iteration : 40800 loss : 160.287 NLL : -160.033 KLD : 0.253 
iteration : 41000 loss : 158.271 NLL : -158.214 KLD : 0.057 
iteration : 41200 loss : 151.395 NLL : -151.269 KLD : 0.125 
iteration : 41400 loss : 68.886 NLL : -68.456 KLD : 0.430 
iteration : 41600 loss : 103.674 NLL : -103.589 KLD : 0.085 
iteration : 41800 loss : 86.885 NLL : -85.574 KLD : 1.311 
iteration : 42000 loss : 97.332 NLL : -87.435 KLD : 9.897 
iteration : 42200 loss : 72.478 NLL : -72.061 KLD : 0.417 
iteration : 42400 loss : 66.187 NLL : -66.122 KLD : 0.065 
iteration : 42600 loss : 65.133 NLL : -64.759 KLD : 0.373 
iteration : 42800 loss : 85.602 NLL : -85.531 KLD : 0.070 
iteration : 43000 loss : 86.102 NLL : -85.678 KLD : 0.424 
---------- Training loss 42.649 updated ! and save the model! (step:43194) ----------
iteration : 43200 loss : 61.709 NLL : -61.040 KLD : 0.669 
iteration : 43400 loss : 60.083 NLL : -59.758 KLD : 0.325 
iteration : 43600 loss : 84.038 NLL : -83.939 KLD : 0.100 
iteration : 43800 loss : 42.773 NLL : -41.174 KLD : 1.599 
iteration : 44000 loss : 49.960 NLL : -49.775 KLD : 0.185 
iteration : 44200 loss : 71.999 NLL : -71.661 KLD : 0.338 
iteration : 44400 loss : 63.731 NLL : -63.230 KLD : 0.501 
iteration : 44600 loss : 53.397 NLL : -51.910 KLD : 1.487 
---------- Training loss 40.636 updated ! and save the model! (step:44730) ----------
iteration : 44800 loss : 138.097 NLL : -137.946 KLD : 0.151 
iteration : 45000 loss : 82.033 NLL : -81.960 KLD : 0.073 
iteration : 45200 loss : 89.859 NLL : -89.846 KLD : 0.013 
iteration : 45400 loss : 57.348 NLL : -57.288 KLD : 0.059 
iteration : 45600 loss : 68.826 NLL : -68.803 KLD : 0.023 
iteration : 45800 loss : 72.737 NLL : -72.732 KLD : 0.005 
iteration : 46000 loss : 56.667 NLL : -56.652 KLD : 0.015 
---------- Training loss 39.472 updated ! and save the model! (step:46104) ----------
iteration : 46200 loss : 112.240 NLL : -112.148 KLD : 0.092 
iteration : 46400 loss : 121.016 NLL : -121.015 KLD : 0.002 
iteration : 46600 loss : 66.351 NLL : -66.351 KLD : 0.000 
iteration : 46800 loss : 111.760 NLL : -111.759 KLD : 0.001 
iteration : 47000 loss : 124.451 NLL : -124.450 KLD : 0.000 
iteration : 47200 loss : 70.901 NLL : -70.901 KLD : 0.000 
iteration : 47400 loss : 76.242 NLL : -76.242 KLD : 0.000 
iteration : 47600 loss : 90.943 NLL : -90.943 KLD : 0.000 
iteration : 47800 loss : 44.702 NLL : -44.702 KLD : 0.000 
iteration : 48000 loss : 58.180 NLL : -58.178 KLD : 0.002 
iteration : 48200 loss : 72.834 NLL : -72.834 KLD : 0.000 
iteration : 48400 loss : 41.070 NLL : -40.773 KLD : 0.297 
iteration : 48600 loss : 69.802 NLL : -69.801 KLD : 0.001 
iteration : 48800 loss : 73.928 NLL : -73.928 KLD : 0.000 
iteration : 49000 loss : 50.619 NLL : -50.619 KLD : 0.000 
iteration : 49200 loss : 36.347 NLL : -36.347 KLD : 0.000 
iteration : 49400 loss : 120.778 NLL : -120.778 KLD : 0.000 
iteration : 49600 loss : 79.009 NLL : -79.009 KLD : 0.000 
iteration : 49800 loss : 37.456 NLL : -37.449 KLD : 0.007 
iteration : 50000 loss : 72.984 NLL : -72.983 KLD : 0.001 
iteration : 50200 loss : 73.241 NLL : -73.235 KLD : 0.006 
iteration : 50400 loss : 59.333 NLL : -59.322 KLD : 0.010 
iteration : 50600 loss : 40.971 NLL : -40.817 KLD : 0.153 
iteration : 50800 loss : 67.614 NLL : -67.371 KLD : 0.243 
iteration : 51000 loss : 40.716 NLL : -40.716 KLD : 0.000 
iteration : 51200 loss : 60.229 NLL : -60.229 KLD : 0.000 
iteration : 51400 loss : 66.501 NLL : -66.501 KLD : 0.000 
iteration : 51600 loss : 46.993 NLL : -46.993 KLD : 0.000 
iteration : 51800 loss : 38.654 NLL : -38.654 KLD : 0.000 
---------- Training loss 37.609 updated ! and save the model! (step:51834) ----------
iteration : 52000 loss : 50.132 NLL : -50.132 KLD : 0.000 
iteration : 52200 loss : 35.146 NLL : -35.146 KLD : 0.000 
iteration : 52400 loss : 76.318 NLL : -76.318 KLD : 0.000 
iteration : 52600 loss : 61.462 NLL : -61.462 KLD : 0.000 
iteration : 52800 loss : 32.389 NLL : -32.389 KLD : 0.000 
iteration : 53000 loss : 49.970 NLL : -49.970 KLD : 0.000 
iteration : 53200 loss : 59.461 NLL : -59.461 KLD : 0.000 
iteration : 53400 loss : 105.176 NLL : -105.176 KLD : 0.000 
iteration : 53600 loss : 44.492 NLL : -44.492 KLD : 0.000 
iteration : 53800 loss : 83.330 NLL : -83.330 KLD : 0.000 
iteration : 54000 loss : 50.790 NLL : -50.790 KLD : 0.000 
iteration : 54200 loss : 45.263 NLL : -45.263 KLD : 0.000 
iteration : 54400 loss : 68.760 NLL : -68.760 KLD : 0.000 
iteration : 54600 loss : 76.294 NLL : -76.294 KLD : 0.000 
iteration : 54800 loss : 62.371 NLL : -62.371 KLD : 0.000 
iteration : 55000 loss : 41.798 NLL : -41.798 KLD : 0.000 
---------- Training loss 36.766 updated ! and save the model! (step:55068) ----------
iteration : 55200 loss : 88.577 NLL : -88.577 KLD : 0.000 
iteration : 55400 loss : 58.412 NLL : -58.412 KLD : 0.000 
iteration : 55600 loss : 95.343 NLL : -95.343 KLD : 0.000 
iteration : 55800 loss : 48.671 NLL : -48.671 KLD : 0.000 
iteration : 56000 loss : 58.017 NLL : -58.017 KLD : 0.000 
---------- Training loss 34.535 updated ! and save the model! (step:56034) ----------
iteration : 56200 loss : 75.979 NLL : -75.979 KLD : 0.000 
iteration : 56400 loss : 96.610 NLL : -96.610 KLD : 0.000 
iteration : 56600 loss : 49.216 NLL : -49.216 KLD : 0.000 
iteration : 56800 loss : 44.410 NLL : -44.410 KLD : 0.000 
iteration : 57000 loss : 70.535 NLL : -70.535 KLD : 0.000 
iteration : 57200 loss : 82.149 NLL : -82.149 KLD : 0.000 
iteration : 57400 loss : 71.258 NLL : -71.258 KLD : 0.000 
iteration : 57600 loss : 50.643 NLL : -50.643 KLD : 0.000 
iteration : 57800 loss : 65.097 NLL : -65.097 KLD : 0.000 
iteration : 58000 loss : 62.149 NLL : -62.149 KLD : 0.000 
iteration : 58200 loss : 58.723 NLL : -58.723 KLD : 0.000 
iteration : 58400 loss : 99.710 NLL : -99.710 KLD : 0.000 
iteration : 58600 loss : 74.878 NLL : -74.878 KLD : 0.000 
iteration : 58800 loss : 54.967 NLL : -54.967 KLD : 0.000 
iteration : 59000 loss : 25.375 NLL : -25.375 KLD : 0.000 
iteration : 59200 loss : 67.996 NLL : -67.996 KLD : 0.000 
iteration : 59400 loss : 50.832 NLL : -50.832 KLD : 0.000 
iteration : 59600 loss : 39.891 NLL : -39.891 KLD : 0.000 
iteration : 59800 loss : 53.923 NLL : -53.923 KLD : 0.000 
iteration : 60000 loss : 39.331 NLL : -39.331 KLD : 0.000 
---------- Training loss 42.846 updated ! and save the model! (step:60000) ----------
---------- Training loss 41.209 updated ! and save the model! (step:60012) ----------
iteration : 60200 loss : 55.070 NLL : -55.070 KLD : 0.000 
---------- Training loss 39.391 updated ! and save the model! (step:60282) ----------
iteration : 60400 loss : 37.784 NLL : -37.784 KLD : 0.000 
---------- Training loss 39.330 updated ! and save the model! (step:60450) ----------
iteration : 60600 loss : 42.265 NLL : -42.265 KLD : 0.000 
iteration : 60800 loss : 53.530 NLL : -53.530 KLD : 0.000 
iteration : 61000 loss : 47.523 NLL : -47.523 KLD : 0.000 
iteration : 61200 loss : 33.958 NLL : -33.958 KLD : 0.000 
iteration : 61400 loss : 52.679 NLL : -52.679 KLD : 0.000 
iteration : 61600 loss : 42.517 NLL : -42.517 KLD : 0.000 
iteration : 61800 loss : 57.798 NLL : -57.798 KLD : 0.000 
iteration : 62000 loss : 29.417 NLL : -29.417 KLD : 0.000 
iteration : 62200 loss : 58.133 NLL : -58.133 KLD : 0.000 
---------- Training loss 37.426 updated ! and save the model! (step:62340) ----------
iteration : 62400 loss : 52.285 NLL : -52.285 KLD : 0.000 
---------- Training loss 34.306 updated ! and save the model! (step:62496) ----------
iteration : 62600 loss : 65.155 NLL : -65.155 KLD : 0.000 
iteration : 62800 loss : 65.519 NLL : -65.519 KLD : 0.000 
iteration : 63000 loss : 41.275 NLL : -41.275 KLD : 0.000 
iteration : 63200 loss : 61.985 NLL : -61.985 KLD : 0.000 
iteration : 63400 loss : 62.827 NLL : -62.827 KLD : 0.000 
iteration : 63600 loss : 48.421 NLL : -48.421 KLD : 0.000 
iteration : 63800 loss : 122.345 NLL : -122.345 KLD : 0.000 
iteration : 64000 loss : 60.508 NLL : -60.508 KLD : 0.000 
iteration : 64200 loss : 58.783 NLL : -58.783 KLD : 0.000 
iteration : 64400 loss : 89.374 NLL : -89.374 KLD : 0.000 
iteration : 64600 loss : 84.510 NLL : -84.510 KLD : 0.000 
iteration : 64800 loss : 94.089 NLL : -94.089 KLD : 0.000 
iteration : 65000 loss : 112.118 NLL : -112.118 KLD : 0.000 
iteration : 65200 loss : 106.259 NLL : -106.259 KLD : 0.000 
iteration : 65400 loss : 95.742 NLL : -95.742 KLD : 0.000 
iteration : 65600 loss : 102.078 NLL : -102.078 KLD : 0.000 
iteration : 65800 loss : 69.300 NLL : -69.300 KLD : 0.000 
iteration : 66000 loss : 83.313 NLL : -83.313 KLD : 0.000 
iteration : 66200 loss : 52.002 NLL : -52.002 KLD : 0.000 
iteration : 66400 loss : 63.774 NLL : -63.774 KLD : 0.000 
iteration : 66600 loss : 83.445 NLL : -83.445 KLD : 0.000 
iteration : 66800 loss : 40.870 NLL : -40.870 KLD : 0.000 
iteration : 67000 loss : 54.081 NLL : -54.081 KLD : 0.000 
iteration : 67200 loss : 61.533 NLL : -61.533 KLD : 0.000 
iteration : 67400 loss : 59.521 NLL : -59.521 KLD : 0.000 
iteration : 67600 loss : 67.223 NLL : -67.223 KLD : 0.000 
iteration : 67800 loss : 54.986 NLL : -54.986 KLD : 0.000 
iteration : 68000 loss : 64.271 NLL : -64.271 KLD : 0.000 
iteration : 68200 loss : 79.587 NLL : -79.587 KLD : 0.000 
iteration : 68400 loss : 41.990 NLL : -41.990 KLD : 0.000 
iteration : 68600 loss : 59.891 NLL : -59.891 KLD : 0.000 
iteration : 68800 loss : 44.901 NLL : -44.901 KLD : 0.000 
iteration : 69000 loss : 38.239 NLL : -38.239 KLD : 0.000 
iteration : 69200 loss : 46.120 NLL : -46.120 KLD : 0.000 
iteration : 69400 loss : 68.081 NLL : -68.081 KLD : 0.000 
iteration : 69600 loss : 92.413 NLL : -92.413 KLD : 0.000 
iteration : 69800 loss : 116.787 NLL : -116.787 KLD : 0.000 
iteration : 70000 loss : 96.710 NLL : -96.710 KLD : 0.000 
iteration : 70200 loss : 45.506 NLL : -45.506 KLD : 0.000 
iteration : 70400 loss : 66.780 NLL : -66.780 KLD : 0.000 
iteration : 70600 loss : 78.248 NLL : -78.248 KLD : 0.000 
iteration : 70800 loss : 49.920 NLL : -49.920 KLD : 0.000 
iteration : 71000 loss : 64.939 NLL : -64.939 KLD : 0.000 
iteration : 71200 loss : 65.111 NLL : -65.111 KLD : 0.000 
iteration : 71400 loss : 36.051 NLL : -36.051 KLD : 0.000 
iteration : 71600 loss : 63.881 NLL : -63.881 KLD : 0.000 
iteration : 71800 loss : 35.004 NLL : -35.004 KLD : 0.000 
iteration : 72000 loss : 52.268 NLL : -52.268 KLD : 0.000 
iteration : 72200 loss : 57.852 NLL : -57.852 KLD : 0.000 
iteration : 72400 loss : 84.967 NLL : -84.967 KLD : 0.000 
iteration : 72600 loss : 27.384 NLL : -27.384 KLD : 0.000 
iteration : 72800 loss : 49.011 NLL : -49.011 KLD : 0.000 
iteration : 73000 loss : 43.379 NLL : -43.379 KLD : 0.000 
iteration : 73200 loss : 62.603 NLL : -62.603 KLD : 0.000 
iteration : 73400 loss : 25.900 NLL : -25.900 KLD : 0.000 
iteration : 73600 loss : 44.727 NLL : -44.727 KLD : 0.000 
iteration : 73800 loss : 65.980 NLL : -65.980 KLD : 0.000 
iteration : 74000 loss : 36.689 NLL : -36.689 KLD : 0.000 
iteration : 74200 loss : 157.656 NLL : -157.656 KLD : 0.000 
iteration : 74400 loss : 58.519 NLL : -58.519 KLD : 0.000 
iteration : 74600 loss : 58.315 NLL : -58.315 KLD : 0.000 
iteration : 74800 loss : 52.468 NLL : -52.468 KLD : 0.000 
iteration : 75000 loss : 58.557 NLL : -58.557 KLD : 0.000 
iteration : 75200 loss : 43.068 NLL : -43.068 KLD : 0.000 
iteration : 75400 loss : 61.961 NLL : -61.961 KLD : 0.000 
iteration : 75600 loss : 46.945 NLL : -46.945 KLD : 0.000 
iteration : 75800 loss : 35.329 NLL : -35.329 KLD : 0.000 
iteration : 76000 loss : 97.423 NLL : -97.423 KLD : 0.000 
iteration : 76200 loss : 52.804 NLL : -52.804 KLD : 0.000 
iteration : 76400 loss : 104.734 NLL : -104.734 KLD : 0.000 
iteration : 76600 loss : 28.498 NLL : -28.498 KLD : 0.000 
iteration : 76800 loss : 74.673 NLL : -74.673 KLD : 0.000 
iteration : 77000 loss : 59.548 NLL : -59.548 KLD : 0.000 
iteration : 77200 loss : 65.313 NLL : -65.313 KLD : 0.000 
iteration : 77400 loss : 30.466 NLL : -30.466 KLD : 0.000 
iteration : 77600 loss : 60.579 NLL : -60.579 KLD : 0.000 
iteration : 77800 loss : 93.495 NLL : -93.495 KLD : 0.000 
iteration : 78000 loss : 53.443 NLL : -53.443 KLD : 0.000 
iteration : 78200 loss : 70.477 NLL : -70.477 KLD : 0.000 
iteration : 78400 loss : 63.250 NLL : -63.250 KLD : 0.000 
iteration : 78600 loss : 31.472 NLL : -31.472 KLD : 0.000 
iteration : 78800 loss : 32.934 NLL : -32.934 KLD : 0.000 
iteration : 79000 loss : 61.663 NLL : -61.663 KLD : 0.000 
iteration : 79200 loss : 40.615 NLL : -40.615 KLD : 0.000 
iteration : 79400 loss : 77.159 NLL : -77.159 KLD : 0.000 
iteration : 79600 loss : 98.568 NLL : -98.568 KLD : 0.000 
iteration : 79800 loss : 81.000 NLL : -81.000 KLD : 0.000 
iteration : 80000 loss : 58.965 NLL : -58.965 KLD : 0.000 
iteration : 80200 loss : 76.026 NLL : -76.026 KLD : 0.000 
iteration : 80400 loss : 33.181 NLL : -33.181 KLD : 0.000 
iteration : 80600 loss : 38.197 NLL : -38.197 KLD : 0.000 
iteration : 80800 loss : 29.282 NLL : -29.282 KLD : 0.000 
iteration : 81000 loss : 78.644 NLL : -78.644 KLD : 0.000 
iteration : 81200 loss : 64.671 NLL : -64.671 KLD : 0.000 
iteration : 81400 loss : 66.297 NLL : -66.297 KLD : 0.000 
iteration : 81600 loss : 127.719 NLL : -127.719 KLD : 0.000 
iteration : 81800 loss : 146.980 NLL : -146.980 KLD : 0.000 
iteration : 82000 loss : 120.637 NLL : -120.637 KLD : 0.000 
iteration : 82200 loss : 55.879 NLL : -55.879 KLD : 0.000 
iteration : 82400 loss : 52.138 NLL : -52.138 KLD : 0.000 
iteration : 82600 loss : 101.301 NLL : -101.301 KLD : 0.000 
iteration : 82800 loss : 49.561 NLL : -49.561 KLD : 0.000 
iteration : 83000 loss : 30.612 NLL : -30.612 KLD : 0.000 
iteration : 83200 loss : 42.877 NLL : -42.877 KLD : 0.000 
iteration : 83400 loss : 589.326 NLL : -589.326 KLD : 0.000 
iteration : 83600 loss : 121.677 NLL : -121.677 KLD : 0.000 
iteration : 83800 loss : 58.087 NLL : -58.087 KLD : 0.000 
iteration : 84000 loss : 59.693 NLL : -59.693 KLD : 0.000 
iteration : 84200 loss : 72.649 NLL : -72.649 KLD : 0.000 
iteration : 84400 loss : 63.207 NLL : -63.207 KLD : 0.000 
iteration : 84600 loss : 72.573 NLL : -72.573 KLD : 0.000 
iteration : 84800 loss : 59.080 NLL : -59.080 KLD : 0.000 
iteration : 85000 loss : 71.429 NLL : -71.429 KLD : 0.000 
iteration : 85200 loss : 47.918 NLL : -47.918 KLD : 0.000 
iteration : 85400 loss : 70.858 NLL : -70.858 KLD : 0.000 
iteration : 85600 loss : 42.263 NLL : -42.263 KLD : 0.000 
iteration : 85800 loss : 34.139 NLL : -34.139 KLD : 0.000 
iteration : 86000 loss : 56.040 NLL : -56.040 KLD : 0.000 
---------- Training loss 32.233 updated ! and save the model! (step:86040) ----------
iteration : 86200 loss : 57.385 NLL : -57.385 KLD : 0.000 
iteration : 86400 loss : 60.841 NLL : -60.841 KLD : 0.000 
iteration : 86600 loss : 38.573 NLL : -38.573 KLD : 0.000 
iteration : 86800 loss : 32.885 NLL : -32.885 KLD : 0.000 
iteration : 87000 loss : 62.509 NLL : -62.509 KLD : 0.000 
iteration : 87200 loss : 57.199 NLL : -57.199 KLD : 0.000 
iteration : 87400 loss : 77.876 NLL : -77.876 KLD : 0.000 
iteration : 87600 loss : 39.496 NLL : -39.496 KLD : 0.000 
iteration : 87800 loss : 57.609 NLL : -57.609 KLD : 0.000 
iteration : 88000 loss : 62.069 NLL : -62.069 KLD : 0.000 
iteration : 88200 loss : 120.425 NLL : -120.425 KLD : 0.000 
iteration : 88400 loss : 77.339 NLL : -77.339 KLD : 0.000 
iteration : 88600 loss : 52.341 NLL : -52.341 KLD : 0.000 
iteration : 88800 loss : 34.488 NLL : -34.488 KLD : 0.000 
iteration : 89000 loss : 592.304 NLL : -592.304 KLD : 0.000 
iteration : 89200 loss : 74.673 NLL : -74.673 KLD : 0.000 
iteration : 89400 loss : 67.449 NLL : -67.449 KLD : 0.000 
iteration : 89600 loss : 105.312 NLL : -105.312 KLD : 0.000 
iteration : 89800 loss : 76.630 NLL : -76.630 KLD : 0.000 
iteration : 90000 loss : 53.535 NLL : -53.535 KLD : 0.000 
---------- Training loss 67.218 updated ! and save the model! (step:90000) ----------
---------- Training loss 59.121 updated ! and save the model! (step:90024) ----------
iteration : 90200 loss : 65.250 NLL : -65.250 KLD : 0.000 
---------- Training loss 59.019 updated ! and save the model! (step:90204) ----------
iteration : 90400 loss : 58.223 NLL : -58.223 KLD : 0.000 
---------- Training loss 56.648 updated ! and save the model! (step:90444) ----------
iteration : 90600 loss : 65.667 NLL : -65.667 KLD : 0.000 
---------- Training loss 52.670 updated ! and save the model! (step:90696) ----------
---------- Training loss 52.233 updated ! and save the model! (step:90750) ----------
iteration : 90800 loss : 65.147 NLL : -65.147 KLD : 0.000 
iteration : 91000 loss : 58.782 NLL : -58.782 KLD : 0.000 
iteration : 91200 loss : 70.018 NLL : -70.018 KLD : 0.000 
iteration : 91400 loss : 89.459 NLL : -89.459 KLD : 0.000 
---------- Training loss 50.541 updated ! and save the model! (step:91440) ----------
iteration : 91600 loss : 62.580 NLL : -62.580 KLD : 0.000 
iteration : 91800 loss : 70.948 NLL : -70.948 KLD : 0.000 
iteration : 92000 loss : 69.126 NLL : -69.126 KLD : 0.000 
iteration : 92200 loss : 48.259 NLL : -48.259 KLD : 0.000 
iteration : 92400 loss : 73.602 NLL : -73.602 KLD : 0.000 
iteration : 92600 loss : 76.027 NLL : -76.027 KLD : 0.000 
---------- Training loss 43.211 updated ! and save the model! (step:92712) ----------
iteration : 92800 loss : 36.770 NLL : -36.770 KLD : 0.000 
iteration : 93000 loss : 73.668 NLL : -73.668 KLD : 0.000 
iteration : 93200 loss : 83.760 NLL : -83.760 KLD : 0.000 
iteration : 93400 loss : 69.366 NLL : -69.366 KLD : 0.000 
iteration : 93600 loss : 54.645 NLL : -54.645 KLD : 0.000 
iteration : 93800 loss : 43.027 NLL : -43.027 KLD : 0.000 
iteration : 94000 loss : 40.306 NLL : -40.306 KLD : 0.000 
iteration : 94200 loss : 71.951 NLL : -71.951 KLD : 0.000 
---------- Training loss 42.627 updated ! and save the model! (step:94242) ----------
iteration : 94400 loss : 71.171 NLL : -71.171 KLD : 0.000 
iteration : 94600 loss : 85.566 NLL : -85.566 KLD : 0.000 
iteration : 94800 loss : 75.471 NLL : -75.471 KLD : 0.000 
iteration : 95000 loss : 100.449 NLL : -100.449 KLD : 0.000 
iteration : 95200 loss : 115.646 NLL : -115.646 KLD : 0.000 
iteration : 95400 loss : 59.887 NLL : -59.887 KLD : 0.000 
iteration : 95600 loss : 66.183 NLL : -66.183 KLD : 0.000 
iteration : 95800 loss : 69.100 NLL : -69.100 KLD : 0.000 
iteration : 96000 loss : 59.465 NLL : -59.465 KLD : 0.000 
iteration : 96200 loss : 58.645 NLL : -58.645 KLD : 0.000 
iteration : 96400 loss : 118.377 NLL : -118.377 KLD : 0.000 
iteration : 96600 loss : 28.164 NLL : -28.164 KLD : 0.000 
iteration : 96800 loss : 120.105 NLL : -120.105 KLD : 0.000 
iteration : 97000 loss : 40.210 NLL : -40.210 KLD : 0.000 
iteration : 97200 loss : 70.214 NLL : -70.214 KLD : 0.000 
iteration : 97400 loss : 66.191 NLL : -66.191 KLD : 0.000 
iteration : 97600 loss : 22.473 NLL : -22.473 KLD : 0.000 
iteration : 97800 loss : 65.979 NLL : -65.979 KLD : 0.000 
iteration : 98000 loss : 44.366 NLL : -44.366 KLD : 0.000 
iteration : 98200 loss : 68.313 NLL : -68.313 KLD : 0.000 
iteration : 98400 loss : 74.884 NLL : -74.884 KLD : 0.000 
iteration : 98600 loss : 74.988 NLL : -74.988 KLD : 0.000 
iteration : 98800 loss : 58.886 NLL : -58.886 KLD : 0.000 
iteration : 99000 loss : 113.345 NLL : -113.345 KLD : 0.000 
iteration : 99200 loss : 90.187 NLL : -90.187 KLD : 0.000 
iteration : 99400 loss : 75.215 NLL : -75.215 KLD : 0.000 
iteration : 99600 loss : 38.845 NLL : -38.845 KLD : 0.000 
iteration : 99800 loss : 110.151 NLL : -110.151 KLD : 0.000 
iteration : 100000 loss : 102.284 NLL : -102.284 KLD : 0.000 
iteration : 100200 loss : 44.410 NLL : -44.410 KLD : 0.000 
iteration : 100400 loss : 76.734 NLL : -76.734 KLD : 0.000 
iteration : 100600 loss : 86.113 NLL : -86.113 KLD : 0.000 
iteration : 100800 loss : 88.801 NLL : -88.801 KLD : 0.000 
iteration : 101000 loss : 333.587 NLL : -333.587 KLD : 0.000 
iteration : 101200 loss : 119.752 NLL : -119.752 KLD : 0.000 
iteration : 101400 loss : 90.115 NLL : -90.115 KLD : 0.000 
iteration : 101600 loss : 6119.315 NLL : -6119.315 KLD : 0.000 
iteration : 101800 loss : 86.270 NLL : -86.270 KLD : 0.000 
iteration : 102000 loss : 48.192 NLL : -48.192 KLD : 0.000 
iteration : 102200 loss : 87.293 NLL : -87.293 KLD : 0.000 
iteration : 102400 loss : 145.752 NLL : -145.752 KLD : 0.000 
iteration : 102600 loss : 67.728 NLL : -67.728 KLD : 0.000 
iteration : 102800 loss : 88.778 NLL : -88.778 KLD : 0.000 
iteration : 103000 loss : 102.424 NLL : -102.424 KLD : 0.000 
iteration : 103200 loss : 126.599 NLL : -126.599 KLD : 0.000 
iteration : 103400 loss : 118.767 NLL : -118.767 KLD : 0.000 
iteration : 103600 loss : 75.517 NLL : -75.517 KLD : 0.000 
iteration : 103800 loss : 75.815 NLL : -75.815 KLD : 0.000 
iteration : 104000 loss : 69.010 NLL : -69.010 KLD : 0.000 
iteration : 104200 loss : 90.723 NLL : -90.723 KLD : 0.000 
iteration : 104400 loss : 83.590 NLL : -83.590 KLD : 0.000 
iteration : 104600 loss : 94.214 NLL : -94.214 KLD : 0.000 
iteration : 104800 loss : 8332.835 NLL : -8332.835 KLD : 0.000 
iteration : 105000 loss : 92.345 NLL : -92.345 KLD : 0.000 
iteration : 105200 loss : 58.287 NLL : -58.287 KLD : 0.000 
iteration : 105400 loss : 77.996 NLL : -77.996 KLD : 0.000 
iteration : 105600 loss : 589.404 NLL : -589.404 KLD : 0.000 
iteration : 105800 loss : 127.910 NLL : -127.910 KLD : 0.000 
iteration : 106000 loss : 65.771 NLL : -65.771 KLD : 0.000 
iteration : 106200 loss : 88.712 NLL : -88.712 KLD : 0.000 
iteration : 106400 loss : 42.162 NLL : -42.162 KLD : 0.000 
iteration : 106600 loss : 86.172 NLL : -86.172 KLD : 0.000 
iteration : 106800 loss : 78.404 NLL : -78.404 KLD : 0.000 
iteration : 107000 loss : 52.558 NLL : -52.558 KLD : 0.000 
iteration : 107200 loss : 83.744 NLL : -83.744 KLD : 0.000 
iteration : 107400 loss : 55.556 NLL : -55.556 KLD : 0.000 
iteration : 107600 loss : 95.683 NLL : -95.683 KLD : 0.000 
iteration : 107800 loss : 80.535 NLL : -80.535 KLD : 0.000 
iteration : 108000 loss : 60.389 NLL : -60.389 KLD : 0.000 
iteration : 108200 loss : 83.704 NLL : -83.704 KLD : 0.000 
iteration : 108400 loss : 57.826 NLL : -57.826 KLD : 0.000 
iteration : 108600 loss : 89.839 NLL : -89.839 KLD : 0.000 
iteration : 108800 loss : 103.223 NLL : -103.223 KLD : 0.000 
iteration : 109000 loss : 125.977 NLL : -125.977 KLD : 0.000 
iteration : 109200 loss : 158.981 NLL : -158.981 KLD : 0.000 
iteration : 109400 loss : 126.816 NLL : -126.816 KLD : 0.000 
iteration : 109600 loss : 85.066 NLL : -85.066 KLD : 0.000 
iteration : 109800 loss : 57.749 NLL : -57.749 KLD : 0.000 
iteration : 110000 loss : 67.873 NLL : -67.873 KLD : 0.000 
iteration : 110200 loss : 56.135 NLL : -56.135 KLD : 0.000 
iteration : 110400 loss : 76.954 NLL : -76.954 KLD : 0.000 
iteration : 110600 loss : 78.218 NLL : -78.218 KLD : 0.000 
iteration : 110800 loss : 62.983 NLL : -62.983 KLD : 0.000 
iteration : 111000 loss : 53.299 NLL : -53.299 KLD : 0.000 
iteration : 111200 loss : 93.145 NLL : -93.145 KLD : 0.000 
iteration : 111400 loss : 80.839 NLL : -80.839 KLD : 0.000 
iteration : 111600 loss : 54.021 NLL : -54.021 KLD : 0.000 
iteration : 111800 loss : 86.239 NLL : -86.239 KLD : 0.000 
iteration : 112000 loss : 84.316 NLL : -84.316 KLD : 0.000 
iteration : 112200 loss : 64.577 NLL : -64.577 KLD : 0.000 
iteration : 112400 loss : 66.732 NLL : -66.732 KLD : 0.000 
iteration : 112600 loss : 120.688 NLL : -120.688 KLD : 0.000 
iteration : 112800 loss : 79.442 NLL : -79.442 KLD : 0.000 
iteration : 113000 loss : 82.766 NLL : -82.766 KLD : 0.000 
iteration : 113200 loss : 55.875 NLL : -55.875 KLD : 0.000 
iteration : 113400 loss : 79.232 NLL : -79.232 KLD : 0.000 
iteration : 113600 loss : 63.928 NLL : -63.928 KLD : 0.000 
iteration : 113800 loss : 70.640 NLL : -70.640 KLD : 0.000 
iteration : 114000 loss : 53.817 NLL : -53.817 KLD : 0.000 
iteration : 114200 loss : 71.296 NLL : -71.296 KLD : 0.000 
iteration : 114400 loss : 72.854 NLL : -72.854 KLD : 0.000 
iteration : 114600 loss : 118.223 NLL : -118.223 KLD : 0.000 
iteration : 114800 loss : 138.616 NLL : -138.616 KLD : 0.000 
iteration : 115000 loss : 53.076 NLL : -53.076 KLD : 0.000 
iteration : 115200 loss : 73.745 NLL : -73.745 KLD : 0.000 
iteration : 115400 loss : 67.941 NLL : -67.941 KLD : 0.000 
iteration : 115600 loss : 56.804 NLL : -56.804 KLD : 0.000 
iteration : 115800 loss : 64.272 NLL : -64.272 KLD : 0.000 
iteration : 116000 loss : 83.495 NLL : -83.495 KLD : 0.000 
iteration : 116200 loss : 76.811 NLL : -76.811 KLD : 0.000 
iteration : 116400 loss : 64.844 NLL : -64.844 KLD : 0.000 
iteration : 116600 loss : 44.509 NLL : -44.509 KLD : 0.000 
iteration : 116800 loss : 91.089 NLL : -91.089 KLD : 0.000 
iteration : 117000 loss : 67.220 NLL : -67.220 KLD : 0.000 
iteration : 117200 loss : 86.116 NLL : -86.116 KLD : 0.000 
iteration : 117400 loss : 65.434 NLL : -65.434 KLD : 0.000 
iteration : 117600 loss : 70.958 NLL : -70.958 KLD : 0.000 
iteration : 117800 loss : 61.478 NLL : -61.478 KLD : 0.000 
iteration : 118000 loss : 127.658 NLL : -127.658 KLD : 0.000 
iteration : 118200 loss : 29.966 NLL : -29.966 KLD : 0.000 
iteration : 118400 loss : 63.910 NLL : -63.910 KLD : 0.000 
---------- Training loss 42.123 updated ! and save the model! (step:118506) ----------
iteration : 118600 loss : 73.567 NLL : -73.567 KLD : 0.000 
iteration : 118800 loss : 63.491 NLL : -63.491 KLD : 0.000 
iteration : 119000 loss : 93.847 NLL : -93.847 KLD : 0.000 
iteration : 119200 loss : 109.473 NLL : -109.473 KLD : 0.000 
iteration : 119400 loss : 80.380 NLL : -80.380 KLD : 0.000 
iteration : 119600 loss : 57.453 NLL : -57.453 KLD : 0.000 
iteration : 119800 loss : 78.062 NLL : -78.062 KLD : 0.000 
iteration : 120000 loss : 52.698 NLL : -52.698 KLD : 0.000 
---------- Training loss 58.744 updated ! and save the model! (step:120000) ----------
iteration : 120200 loss : 82.048 NLL : -82.048 KLD : 0.000 
---------- Training loss 56.753 updated ! and save the model! (step:120270) ----------
iteration : 120400 loss : 32.803 NLL : -32.803 KLD : 0.000 
iteration : 120600 loss : 94.071 NLL : -94.071 KLD : 0.000 
iteration : 120800 loss : 37.578 NLL : -37.578 KLD : 0.000 
---------- Training loss 53.934 updated ! and save the model! (step:120948) ----------
iteration : 121000 loss : 41.520 NLL : -41.520 KLD : 0.000 
iteration : 121200 loss : 85.882 NLL : -85.882 KLD : 0.000 
iteration : 121400 loss : 85.592 NLL : -85.592 KLD : 0.000 
iteration : 121600 loss : 47.233 NLL : -47.233 KLD : 0.000 
iteration : 121800 loss : 92.972 NLL : -92.972 KLD : 0.000 
iteration : 122000 loss : 48.052 NLL : -48.052 KLD : 0.000 
iteration : 122200 loss : 99.263 NLL : -99.263 KLD : 0.000 
iteration : 122400 loss : 44.074 NLL : -44.074 KLD : 0.000 
iteration : 122600 loss : 80.636 NLL : -80.636 KLD : 0.000 
iteration : 122800 loss : 59.406 NLL : -59.406 KLD : 0.000 
iteration : 123000 loss : 75.697 NLL : -75.697 KLD : 0.000 
iteration : 123200 loss : 82.343 NLL : -82.343 KLD : 0.000 
iteration : 123400 loss : 413.595 NLL : -413.595 KLD : 0.000 
iteration : 123600 loss : 52.832 NLL : -52.832 KLD : 0.000 
iteration : 123800 loss : 130.930 NLL : -130.930 KLD : 0.000 
iteration : 124000 loss : 81.234 NLL : -81.234 KLD : 0.000 
iteration : 124200 loss : 141.144 NLL : -141.144 KLD : 0.000 
iteration : 124400 loss : 159.445 NLL : -159.445 KLD : 0.000 
---------- Training loss 52.790 updated ! and save the model! (step:124590) ----------
iteration : 124600 loss : 87.756 NLL : -87.756 KLD : 0.000 
iteration : 124800 loss : 105.544 NLL : -105.544 KLD : 0.000 
iteration : 125000 loss : 42.760 NLL : -42.760 KLD : 0.000 
iteration : 125200 loss : 43.434 NLL : -43.434 KLD : 0.000 
iteration : 125400 loss : 102.435 NLL : -102.435 KLD : 0.000 
iteration : 125600 loss : 79.478 NLL : -79.478 KLD : 0.000 
iteration : 125800 loss : 76.832 NLL : -76.832 KLD : 0.000 
iteration : 126000 loss : 86.596 NLL : -86.596 KLD : 0.000 
iteration : 126200 loss : 89.303 NLL : -89.303 KLD : 0.000 
iteration : 126400 loss : 78.672 NLL : -78.672 KLD : 0.000 
iteration : 126600 loss : 82.661 NLL : -82.661 KLD : 0.000 
iteration : 126800 loss : 89.339 NLL : -89.339 KLD : 0.000 
iteration : 127000 loss : 110.291 NLL : -110.291 KLD : 0.000 
iteration : 127200 loss : 77.322 NLL : -77.322 KLD : 0.000 
iteration : 127400 loss : 64.318 NLL : -64.318 KLD : 0.000 
iteration : 127600 loss : 66.599 NLL : -66.599 KLD : 0.000 
iteration : 127800 loss : 68.927 NLL : -68.927 KLD : 0.000 
iteration : 128000 loss : 141.076 NLL : -141.076 KLD : 0.000 
iteration : 128200 loss : 129.041 NLL : -129.041 KLD : 0.000 
iteration : 128400 loss : 89.225 NLL : -89.225 KLD : 0.000 
iteration : 128600 loss : 56.380 NLL : -56.380 KLD : 0.000 
iteration : 128800 loss : 155.691 NLL : -155.691 KLD : 0.000 
iteration : 129000 loss : 92.556 NLL : -92.556 KLD : 0.000 
iteration : 129200 loss : 89.573 NLL : -89.573 KLD : 0.000 
iteration : 129400 loss : 131.843 NLL : -131.843 KLD : 0.000 
iteration : 129600 loss : 92.424 NLL : -92.424 KLD : 0.000 
iteration : 129800 loss : 82.702 NLL : -82.702 KLD : 0.000 
iteration : 130000 loss : 119.587 NLL : -119.587 KLD : 0.000 
iteration : 130200 loss : 71.537 NLL : -71.537 KLD : 0.000 
iteration : 130400 loss : 96.567 NLL : -96.567 KLD : 0.000 
iteration : 130600 loss : 78.717 NLL : -78.717 KLD : 0.000 
iteration : 130800 loss : 81.408 NLL : -81.408 KLD : 0.000 
iteration : 131000 loss : 63.383 NLL : -63.383 KLD : 0.000 
iteration : 131200 loss : 77.701 NLL : -77.701 KLD : 0.000 
iteration : 131400 loss : 89.622 NLL : -89.622 KLD : 0.000 
iteration : 131600 loss : 88.078 NLL : -88.078 KLD : 0.000 
iteration : 131800 loss : 99.280 NLL : -99.280 KLD : 0.000 
iteration : 132000 loss : 104.208 NLL : -104.208 KLD : 0.000 
iteration : 132200 loss : 83.796 NLL : -83.796 KLD : 0.000 
iteration : 132400 loss : 54.241 NLL : -54.241 KLD : 0.000 
iteration : 132600 loss : 49.717 NLL : -49.717 KLD : 0.000 
iteration : 132800 loss : 74.138 NLL : -74.138 KLD : 0.000 
iteration : 133000 loss : 92.001 NLL : -92.001 KLD : 0.000 
iteration : 133200 loss : 103.355 NLL : -103.355 KLD : 0.000 
iteration : 133400 loss : 88.778 NLL : -88.778 KLD : 0.000 
iteration : 133600 loss : 98.469 NLL : -98.469 KLD : 0.000 
iteration : 133800 loss : 98.494 NLL : -98.494 KLD : 0.000 
iteration : 134000 loss : 80.645 NLL : -80.645 KLD : 0.000 
iteration : 134200 loss : 84.793 NLL : -84.793 KLD : 0.000 
iteration : 134400 loss : 86.745 NLL : -86.745 KLD : 0.000 
iteration : 134600 loss : 122.148 NLL : -122.148 KLD : 0.000 
iteration : 134800 loss : 89.062 NLL : -89.062 KLD : 0.000 
---------- Training loss 48.792 updated ! and save the model! (step:134934) ----------
iteration : 135000 loss : 67.451 NLL : -67.451 KLD : 0.000 
iteration : 135200 loss : 68.451 NLL : -68.451 KLD : 0.000 
iteration : 135400 loss : 53.862 NLL : -53.862 KLD : 0.000 
iteration : 135600 loss : 100.592 NLL : -100.592 KLD : 0.000 
---------- Training loss 47.675 updated ! and save the model! (step:135762) ----------
iteration : 135800 loss : 36.365 NLL : -36.365 KLD : 0.000 
iteration : 136000 loss : 88.773 NLL : -88.773 KLD : 0.000 
iteration : 136200 loss : 68.927 NLL : -68.927 KLD : 0.000 
---------- Training loss 47.630 updated ! and save the model! (step:136284) ----------
iteration : 136400 loss : 72.221 NLL : -72.221 KLD : 0.000 
---------- Training loss 45.559 updated ! and save the model! (step:136566) ----------
iteration : 136600 loss : 60.051 NLL : -60.051 KLD : 0.000 
iteration : 136800 loss : 72.856 NLL : -72.856 KLD : 0.000 
iteration : 137000 loss : 72.081 NLL : -72.081 KLD : 0.000 
iteration : 137200 loss : 44.853 NLL : -44.853 KLD : 0.000 
iteration : 137400 loss : 57.989 NLL : -57.989 KLD : 0.000 
iteration : 137600 loss : 72.330 NLL : -72.330 KLD : 0.000 
iteration : 137800 loss : 69.199 NLL : -69.199 KLD : 0.000 
iteration : 138000 loss : 52.652 NLL : -52.652 KLD : 0.000 
---------- Training loss 45.018 updated ! and save the model! (step:138084) ----------
iteration : 138200 loss : 47.234 NLL : -47.234 KLD : 0.000 
iteration : 138400 loss : 71.217 NLL : -71.217 KLD : 0.000 
iteration : 138600 loss : 65.047 NLL : -65.047 KLD : 0.000 
iteration : 138800 loss : 60.794 NLL : -60.794 KLD : 0.000 
iteration : 139000 loss : 44.089 NLL : -44.089 KLD : 0.000 
iteration : 139200 loss : 54.611 NLL : -54.611 KLD : 0.000 
iteration : 139400 loss : 46.698 NLL : -46.698 KLD : 0.000 
---------- Training loss 43.944 updated ! and save the model! (step:139428) ----------
iteration : 139600 loss : 60.125 NLL : -60.125 KLD : 0.000 
iteration : 139800 loss : 71.395 NLL : -71.395 KLD : 0.000 
iteration : 140000 loss : 69.270 NLL : -69.270 KLD : 0.000 
iteration : 140200 loss : 67.511 NLL : -67.511 KLD : 0.000 
iteration : 140400 loss : 67.293 NLL : -67.293 KLD : 0.000 
iteration : 140600 loss : 219.302 NLL : -219.302 KLD : 0.000 
iteration : 140800 loss : 125.302 NLL : -125.302 KLD : 0.000 
iteration : 141000 loss : 93.412 NLL : -93.412 KLD : 0.000 
iteration : 141200 loss : 83.223 NLL : -83.223 KLD : 0.000 
iteration : 141400 loss : 97.881 NLL : -97.881 KLD : 0.000 
iteration : 141600 loss : 96.884 NLL : -96.884 KLD : 0.000 
iteration : 141800 loss : 53.701 NLL : -53.701 KLD : 0.000 
iteration : 142000 loss : 84.442 NLL : -84.442 KLD : 0.000 
iteration : 142200 loss : 84.745 NLL : -84.745 KLD : 0.000 
iteration : 142400 loss : 69.922 NLL : -69.922 KLD : 0.000 
iteration : 142600 loss : 94.681 NLL : -94.681 KLD : 0.000 
iteration : 142800 loss : 73.523 NLL : -73.523 KLD : 0.000 
iteration : 143000 loss : 128.061 NLL : -128.061 KLD : 0.000 
iteration : 143200 loss : 80.294 NLL : -80.294 KLD : 0.000 
iteration : 143400 loss : 48.411 NLL : -48.411 KLD : 0.000 
iteration : 143600 loss : 87.049 NLL : -87.049 KLD : 0.000 
iteration : 143800 loss : 59.078 NLL : -59.078 KLD : 0.000 
iteration : 144000 loss : 92.302 NLL : -92.302 KLD : 0.000 
iteration : 144200 loss : 95.435 NLL : -95.435 KLD : 0.000 
iteration : 144400 loss : 54.886 NLL : -54.886 KLD : 0.000 
iteration : 144600 loss : 57.876 NLL : -57.876 KLD : 0.000 
iteration : 144800 loss : 84.621 NLL : -84.621 KLD : 0.000 
iteration : 145000 loss : 58.684 NLL : -58.684 KLD : 0.000 
iteration : 145200 loss : 75.593 NLL : -75.593 KLD : 0.000 
iteration : 145400 loss : 84.912 NLL : -84.912 KLD : 0.000 
iteration : 145600 loss : 46.105 NLL : -46.105 KLD : 0.000 
iteration : 145800 loss : 68.943 NLL : -68.943 KLD : 0.000 
iteration : 146000 loss : 87.591 NLL : -87.591 KLD : 0.000 
iteration : 146200 loss : 116.754 NLL : -116.754 KLD : 0.000 
iteration : 146400 loss : 52.927 NLL : -52.927 KLD : 0.000 
iteration : 146600 loss : 51.291 NLL : -51.291 KLD : 0.000 
iteration : 146800 loss : 48.865 NLL : -48.865 KLD : 0.000 
iteration : 147000 loss : 37.397 NLL : -37.397 KLD : 0.000 
iteration : 147200 loss : 70.821 NLL : -70.821 KLD : 0.000 
iteration : 147400 loss : 32.119 NLL : -32.119 KLD : 0.000 
iteration : 147600 loss : 65.826 NLL : -65.826 KLD : 0.000 
iteration : 147800 loss : 80.598 NLL : -80.598 KLD : 0.000 
iteration : 148000 loss : 106.475 NLL : -106.475 KLD : 0.000 
iteration : 148200 loss : 75.748 NLL : -75.748 KLD : 0.000 
iteration : 148400 loss : 58.278 NLL : -58.278 KLD : 0.000 
iteration : 148600 loss : 92.579 NLL : -92.579 KLD : 0.000 
iteration : 148800 loss : 86.051 NLL : -86.051 KLD : 0.000 
iteration : 149000 loss : 90.468 NLL : -90.468 KLD : 0.000 
iteration : 149200 loss : 105.438 NLL : -105.438 KLD : 0.000 
iteration : 149400 loss : 69.681 NLL : -69.681 KLD : 0.000 
iteration : 149600 loss : 41.512 NLL : -41.512 KLD : 0.000 
iteration : 149800 loss : 184.257 NLL : -184.257 KLD : 0.000 
iteration : 150000 loss : 81.804 NLL : -81.804 KLD : 0.000 
---------- Training loss 70.940 updated ! and save the model! (step:150000) ----------
---------- Training loss 64.623 updated ! and save the model! (step:150012) ----------
---------- Training loss 59.383 updated ! and save the model! (step:150042) ----------
iteration : 150200 loss : 85.502 NLL : -85.502 KLD : 0.000 
iteration : 150400 loss : 75.282 NLL : -75.282 KLD : 0.000 
---------- Training loss 55.656 updated ! and save the model! (step:150450) ----------
iteration : 150600 loss : 86.531 NLL : -86.531 KLD : 0.000 
iteration : 150800 loss : 59.782 NLL : -59.782 KLD : 0.000 
iteration : 151000 loss : 87.202 NLL : -87.202 KLD : 0.000 
iteration : 151200 loss : 88.631 NLL : -88.631 KLD : 0.000 
iteration : 151400 loss : 80.733 NLL : -80.733 KLD : 0.000 
iteration : 151600 loss : 70.271 NLL : -70.271 KLD : 0.000 
iteration : 151800 loss : 94.268 NLL : -94.268 KLD : 0.000 
iteration : 152000 loss : 105.009 NLL : -105.009 KLD : 0.000 
---------- Training loss 54.634 updated ! and save the model! (step:152082) ----------
iteration : 152200 loss : 74.348 NLL : -74.348 KLD : 0.000 
iteration : 152400 loss : 86.878 NLL : -86.878 KLD : 0.000 
iteration : 152600 loss : 41.043 NLL : -41.043 KLD : 0.000 
iteration : 152800 loss : 56.891 NLL : -56.891 KLD : 0.000 
iteration : 153000 loss : 107.723 NLL : -107.723 KLD : 0.000 
iteration : 153200 loss : 89.601 NLL : -89.601 KLD : 0.000 
iteration : 153400 loss : 223.770 NLL : -223.770 KLD : 0.000 
iteration : 153600 loss : 61.569 NLL : -61.569 KLD : 0.000 
iteration : 153800 loss : 104.436 NLL : -104.436 KLD : 0.000 
iteration : 154000 loss : 84.039 NLL : -84.039 KLD : 0.000 
iteration : 154200 loss : 51.014 NLL : -51.014 KLD : 0.000 
iteration : 154400 loss : 77.763 NLL : -77.763 KLD : 0.000 
iteration : 154600 loss : 45.796 NLL : -45.796 KLD : 0.000 
iteration : 154800 loss : 81.470 NLL : -81.470 KLD : 0.000 
iteration : 155000 loss : 114.620 NLL : -114.620 KLD : 0.000 
iteration : 155200 loss : 94.253 NLL : -94.253 KLD : 0.000 
iteration : 155400 loss : 39.866 NLL : -39.866 KLD : 0.000 
iteration : 155600 loss : 68.937 NLL : -68.937 KLD : 0.000 
iteration : 155800 loss : 90.658 NLL : -90.658 KLD : 0.000 
iteration : 156000 loss : 66.138 NLL : -66.138 KLD : 0.000 
iteration : 156200 loss : 65.530 NLL : -65.530 KLD : 0.000 
---------- Training loss 54.449 updated ! and save the model! (step:156384) ----------
iteration : 156400 loss : 81.848 NLL : -81.848 KLD : 0.000 
iteration : 156600 loss : 113.249 NLL : -113.249 KLD : 0.000 
iteration : 156800 loss : 58.756 NLL : -58.756 KLD : 0.000 
iteration : 157000 loss : 90.788 NLL : -90.788 KLD : 0.000 
iteration : 157200 loss : 71.375 NLL : -71.375 KLD : 0.000 
iteration : 157400 loss : 340.203 NLL : -340.203 KLD : 0.000 
iteration : 157600 loss : 91.271 NLL : -91.271 KLD : 0.000 
iteration : 157800 loss : 132.867 NLL : -132.867 KLD : 0.000 
iteration : 158000 loss : 215.573 NLL : -215.573 KLD : 0.000 
iteration : 158200 loss : 83.718 NLL : -83.718 KLD : 0.000 
iteration : 158400 loss : 43.922 NLL : -43.922 KLD : 0.000 
iteration : 158600 loss : 79.621 NLL : -79.621 KLD : 0.000 
iteration : 158800 loss : 89.031 NLL : -89.031 KLD : 0.000 
iteration : 159000 loss : 103.211 NLL : -103.211 KLD : 0.000 
iteration : 159200 loss : 49.760 NLL : -49.760 KLD : 0.000 
iteration : 159400 loss : 88.801 NLL : -88.801 KLD : 0.000 
iteration : 159600 loss : 100.583 NLL : -100.583 KLD : 0.000 
iteration : 159800 loss : 88.485 NLL : -88.485 KLD : 0.000 
iteration : 160000 loss : 70.642 NLL : -70.642 KLD : 0.000 
iteration : 160200 loss : 66.946 NLL : -66.946 KLD : 0.000 
iteration : 160400 loss : 95.919 NLL : -95.919 KLD : 0.000 
iteration : 160600 loss : 85.848 NLL : -85.848 KLD : 0.000 
iteration : 160800 loss : 69.979 NLL : -69.979 KLD : 0.000 
iteration : 161000 loss : 45.313 NLL : -45.313 KLD : 0.000 
iteration : 161200 loss : 71.637 NLL : -71.637 KLD : 0.000 
iteration : 161400 loss : 86.181 NLL : -86.181 KLD : 0.000 
iteration : 161600 loss : 75.575 NLL : -75.575 KLD : 0.000 
iteration : 161800 loss : 89.656 NLL : -89.656 KLD : 0.000 
iteration : 162000 loss : 79.509 NLL : -79.509 KLD : 0.000 
iteration : 162200 loss : 91.077 NLL : -91.077 KLD : 0.000 
iteration : 162400 loss : 71.328 NLL : -71.328 KLD : 0.000 
iteration : 162600 loss : 80.378 NLL : -80.378 KLD : 0.000 
iteration : 162800 loss : 50.161 NLL : -50.161 KLD : 0.000 
iteration : 163000 loss : 91.519 NLL : -91.519 KLD : 0.000 
iteration : 163200 loss : 64.312 NLL : -64.312 KLD : 0.000 
iteration : 163400 loss : 95.072 NLL : -95.072 KLD : 0.000 
iteration : 163600 loss : 84.045 NLL : -84.045 KLD : 0.000 
iteration : 163800 loss : 122.878 NLL : -122.878 KLD : 0.000 
iteration : 164000 loss : 98.818 NLL : -98.818 KLD : 0.000 
iteration : 164200 loss : 73.297 NLL : -73.297 KLD : 0.000 
iteration : 164400 loss : 82.913 NLL : -82.913 KLD : 0.000 
iteration : 164600 loss : 76.518 NLL : -76.518 KLD : 0.000 
iteration : 164800 loss : 74.690 NLL : -74.690 KLD : 0.000 
---------- Training loss 49.723 updated ! and save the model! (step:164958) ----------
iteration : 165000 loss : 48.601 NLL : -48.601 KLD : 0.000 
iteration : 165200 loss : 87.687 NLL : -87.687 KLD : 0.000 
iteration : 165400 loss : 90.673 NLL : -90.673 KLD : 0.000 
iteration : 165600 loss : 103.741 NLL : -103.741 KLD : 0.000 
iteration : 165800 loss : 78.605 NLL : -78.605 KLD : 0.000 
iteration : 166000 loss : 125.941 NLL : -125.941 KLD : 0.000 
iteration : 166200 loss : 103.813 NLL : -103.813 KLD : 0.000 
iteration : 166400 loss : 74.787 NLL : -74.787 KLD : 0.000 
iteration : 166600 loss : 60.672 NLL : -60.672 KLD : 0.000 
iteration : 166800 loss : 75.075 NLL : -75.075 KLD : 0.000 
iteration : 167000 loss : 65.486 NLL : -65.486 KLD : 0.000 
iteration : 167200 loss : 89.939 NLL : -89.939 KLD : 0.000 
iteration : 167400 loss : 88.970 NLL : -88.970 KLD : 0.000 
iteration : 167600 loss : 54.483 NLL : -54.483 KLD : 0.000 
iteration : 167800 loss : 80.400 NLL : -80.400 KLD : 0.000 
iteration : 168000 loss : 72.322 NLL : -72.322 KLD : 0.000 
iteration : 168200 loss : 86.255 NLL : -86.255 KLD : 0.000 
iteration : 168400 loss : 81.236 NLL : -81.236 KLD : 0.000 
iteration : 168600 loss : 64.865 NLL : -64.865 KLD : 0.000 
iteration : 168800 loss : 75.378 NLL : -75.378 KLD : 0.000 
iteration : 169000 loss : 89.058 NLL : -89.058 KLD : 0.000 
iteration : 169200 loss : 39.212 NLL : -39.212 KLD : 0.000 
iteration : 169400 loss : 86.222 NLL : -86.222 KLD : 0.000 
iteration : 169600 loss : 53.356 NLL : -53.356 KLD : 0.000 
iteration : 169800 loss : 65.145 NLL : -65.145 KLD : 0.000 
iteration : 170000 loss : 66.174 NLL : -66.174 KLD : 0.000 
iteration : 170200 loss : 69.622 NLL : -69.622 KLD : 0.000 
iteration : 170400 loss : 100.214 NLL : -100.214 KLD : 0.000 
iteration : 170600 loss : 62.388 NLL : -62.388 KLD : 0.000 
iteration : 170800 loss : 82.664 NLL : -82.664 KLD : 0.000 
iteration : 171000 loss : 85.598 NLL : -85.598 KLD : 0.000 
iteration : 171200 loss : 86.821 NLL : -86.821 KLD : 0.000 
iteration : 171400 loss : 65.133 NLL : -65.133 KLD : 0.000 
iteration : 171600 loss : 35.362 NLL : -35.362 KLD : 0.000 
iteration : 171800 loss : 58.847 NLL : -58.847 KLD : 0.000 
iteration : 172000 loss : 61.896 NLL : -61.896 KLD : 0.000 
iteration : 172200 loss : 54.671 NLL : -54.671 KLD : 0.000 
iteration : 172400 loss : 411.442 NLL : -411.442 KLD : 0.000 
iteration : 172600 loss : 83.473 NLL : -83.473 KLD : 0.000 
iteration : 172800 loss : 85.602 NLL : -85.602 KLD : 0.000 
iteration : 173000 loss : 93.374 NLL : -93.374 KLD : 0.000 
iteration : 173200 loss : 44.900 NLL : -44.900 KLD : 0.000 
iteration : 173400 loss : 78.052 NLL : -78.052 KLD : 0.000 
iteration : 173600 loss : 51.301 NLL : -51.301 KLD : 0.000 
iteration : 173800 loss : 55.750 NLL : -55.750 KLD : 0.000 
iteration : 174000 loss : 69.859 NLL : -69.859 KLD : 0.000 
iteration : 174200 loss : 68.506 NLL : -68.506 KLD : 0.000 
iteration : 174400 loss : 77.101 NLL : -77.101 KLD : 0.000 
iteration : 174600 loss : 72.463 NLL : -72.463 KLD : 0.000 
iteration : 174800 loss : 86.873 NLL : -86.873 KLD : 0.000 
iteration : 175000 loss : 57.582 NLL : -57.582 KLD : 0.000 
iteration : 175200 loss : 88.103 NLL : -88.103 KLD : 0.000 
iteration : 175400 loss : 85.240 NLL : -85.240 KLD : 0.000 
iteration : 175600 loss : 93.012 NLL : -93.012 KLD : 0.000 
---------- Training loss 49.576 updated ! and save the model! (step:175722) ----------
iteration : 175800 loss : 56.890 NLL : -56.890 KLD : 0.000 
iteration : 176000 loss : 86.935 NLL : -86.935 KLD : 0.000 
iteration : 176200 loss : 98.482 NLL : -98.482 KLD : 0.000 
iteration : 176400 loss : 93.334 NLL : -93.334 KLD : 0.000 
iteration : 176600 loss : 88.479 NLL : -88.479 KLD : 0.000 
iteration : 176800 loss : 53.918 NLL : -53.918 KLD : 0.000 
iteration : 177000 loss : 82.922 NLL : -82.922 KLD : 0.000 
iteration : 177200 loss : 91.920 NLL : -91.920 KLD : 0.000 
iteration : 177400 loss : 76.727 NLL : -76.727 KLD : 0.000 
iteration : 177600 loss : 80.452 NLL : -80.452 KLD : 0.000 
iteration : 177800 loss : 69.310 NLL : -69.310 KLD : 0.000 
iteration : 178000 loss : 53.772 NLL : -53.772 KLD : 0.000 
iteration : 178200 loss : 76.657 NLL : -76.657 KLD : 0.000 
iteration : 178400 loss : 92.011 NLL : -92.011 KLD : 0.000 
iteration : 178600 loss : 69.629 NLL : -69.629 KLD : 0.000 
iteration : 178800 loss : 50.220 NLL : -50.220 KLD : 0.000 
iteration : 179000 loss : 83.704 NLL : -83.704 KLD : 0.000 
iteration : 179200 loss : 78.299 NLL : -78.299 KLD : 0.000 
iteration : 179400 loss : 96.786 NLL : -96.786 KLD : 0.000 
iteration : 179600 loss : 175.651 NLL : -175.651 KLD : 0.000 
iteration : 179800 loss : 96.924 NLL : -96.924 KLD : 0.000 
iteration : 180000 loss : 46.204 NLL : -46.204 KLD : 0.000 
---------- Training loss 79.884 updated ! and save the model! (step:180000) ----------
---------- Training loss 78.816 updated ! and save the model! (step:180006) ----------
---------- Training loss 70.221 updated ! and save the model! (step:180012) ----------
---------- Training loss 65.627 updated ! and save the model! (step:180078) ----------
iteration : 180200 loss : 83.686 NLL : -83.686 KLD : 0.000 
---------- Training loss 62.333 updated ! and save the model! (step:180228) ----------
---------- Training loss 59.928 updated ! and save the model! (step:180246) ----------
---------- Training loss 59.001 updated ! and save the model! (step:180300) ----------
---------- Training loss 58.752 updated ! and save the model! (step:180324) ----------
iteration : 180400 loss : 51.170 NLL : -51.170 KLD : 0.000 
iteration : 180600 loss : 85.719 NLL : -85.719 KLD : 0.000 
---------- Training loss 56.107 updated ! and save the model! (step:180732) ----------
iteration : 180800 loss : 76.305 NLL : -76.305 KLD : 0.000 
iteration : 181000 loss : 90.773 NLL : -90.773 KLD : 0.000 
---------- Training loss 54.107 updated ! and save the model! (step:181098) ----------
iteration : 181200 loss : 63.378 NLL : -63.378 KLD : 0.000 
---------- Training loss 48.896 updated ! and save the model! (step:181302) ----------
iteration : 181400 loss : 62.247 NLL : -62.247 KLD : 0.000 
iteration : 181600 loss : 88.400 NLL : -88.400 KLD : 0.000 
iteration : 181800 loss : 78.053 NLL : -78.053 KLD : 0.000 
iteration : 182000 loss : 79.634 NLL : -79.634 KLD : 0.000 
iteration : 182200 loss : 85.485 NLL : -85.485 KLD : 0.000 
---------- Training loss 48.713 updated ! and save the model! (step:182304) ----------
iteration : 182400 loss : 51.151 NLL : -51.151 KLD : 0.000 
iteration : 182600 loss : 107.461 NLL : -107.461 KLD : 0.000 
iteration : 182800 loss : 41.142 NLL : -41.142 KLD : 0.000 
iteration : 183000 loss : 47.245 NLL : -47.245 KLD : 0.000 
iteration : 183200 loss : 70.708 NLL : -70.708 KLD : 0.000 
iteration : 183400 loss : 80.108 NLL : -80.108 KLD : 0.000 
iteration : 183600 loss : 83.212 NLL : -83.212 KLD : 0.000 
iteration : 183800 loss : 69.133 NLL : -69.133 KLD : 0.000 
iteration : 184000 loss : 30.163 NLL : -30.163 KLD : 0.000 
iteration : 184200 loss : 44.397 NLL : -44.397 KLD : 0.000 
iteration : 184400 loss : 77.677 NLL : -77.677 KLD : 0.000 
iteration : 184600 loss : 58.754 NLL : -58.754 KLD : 0.000 
iteration : 184800 loss : 53.777 NLL : -53.777 KLD : 0.000 
iteration : 185000 loss : 56.679 NLL : -56.679 KLD : 0.000 
iteration : 185200 loss : 78.995 NLL : -78.995 KLD : 0.000 
iteration : 185400 loss : 65.601 NLL : -65.601 KLD : 0.000 
iteration : 185600 loss : 31.305 NLL : -31.305 KLD : 0.000 
iteration : 185800 loss : 44.152 NLL : -44.152 KLD : 0.000 
iteration : 186000 loss : 51.193 NLL : -51.193 KLD : 0.000 
---------- Training loss 48.203 updated ! and save the model! (step:186090) ----------
iteration : 186200 loss : 68.778 NLL : -68.778 KLD : 0.000 
iteration : 186400 loss : 49.392 NLL : -49.392 KLD : 0.000 
iteration : 186600 loss : 88.083 NLL : -88.083 KLD : 0.000 
iteration : 186800 loss : 51.308 NLL : -51.308 KLD : 0.000 
iteration : 187000 loss : 77.919 NLL : -77.919 KLD : 0.000 
---------- Training loss 47.028 updated ! and save the model! (step:187080) ----------
iteration : 187200 loss : 58.042 NLL : -58.042 KLD : 0.000 
iteration : 187400 loss : 45.032 NLL : -45.032 KLD : 0.000 
iteration : 187600 loss : 76.245 NLL : -76.245 KLD : 0.000 
iteration : 187800 loss : 66.667 NLL : -66.667 KLD : 0.000 
iteration : 188000 loss : 111.162 NLL : -111.162 KLD : 0.000 
iteration : 188200 loss : 66.075 NLL : -66.075 KLD : 0.000 
iteration : 188400 loss : 64.940 NLL : -64.940 KLD : 0.000 
iteration : 188600 loss : 65.301 NLL : -65.301 KLD : 0.000 
iteration : 188800 loss : 55.996 NLL : -55.996 KLD : 0.000 
iteration : 189000 loss : 42.885 NLL : -42.885 KLD : 0.000 
iteration : 189200 loss : 65.792 NLL : -65.792 KLD : 0.000 
iteration : 189400 loss : 78.706 NLL : -78.706 KLD : 0.000 
iteration : 189600 loss : 86.430 NLL : -86.430 KLD : 0.000 
iteration : 189800 loss : 126.073 NLL : -126.073 KLD : 0.000 
iteration : 190000 loss : 79.819 NLL : -79.819 KLD : 0.000 
iteration : 190200 loss : 896.741 NLL : -896.741 KLD : 0.000 
iteration : 190400 loss : 420.148 NLL : -420.148 KLD : 0.000 
iteration : 190600 loss : 104.393 NLL : -104.393 KLD : 0.000 
iteration : 190800 loss : 86.749 NLL : -86.749 KLD : 0.000 
iteration : 191000 loss : 84.918 NLL : -84.918 KLD : 0.000 
iteration : 191200 loss : 136.746 NLL : -136.746 KLD : 0.000 
iteration : 191400 loss : 92.239 NLL : -92.239 KLD : 0.000 
iteration : 191600 loss : 106.738 NLL : -106.738 KLD : 0.000 
iteration : 191800 loss : 48.560 NLL : -48.560 KLD : 0.000 
iteration : 192000 loss : 61.227 NLL : -61.227 KLD : 0.000 
iteration : 192200 loss : 48.059 NLL : -48.059 KLD : 0.000 
iteration : 192400 loss : 110.888 NLL : -110.888 KLD : 0.000 
iteration : 192600 loss : 81.542 NLL : -81.542 KLD : 0.000 
iteration : 192800 loss : 80.085 NLL : -80.085 KLD : 0.000 
iteration : 193000 loss : 46.894 NLL : -46.894 KLD : 0.000 
iteration : 193200 loss : 70.307 NLL : -70.307 KLD : 0.000 
iteration : 193400 loss : 60.955 NLL : -60.955 KLD : 0.000 
iteration : 193600 loss : 77.825 NLL : -77.825 KLD : 0.000 
iteration : 193800 loss : 73.577 NLL : -73.577 KLD : 0.000 
iteration : 194000 loss : 87.000 NLL : -87.000 KLD : 0.000 
iteration : 194200 loss : 39.058 NLL : -39.058 KLD : 0.000 
iteration : 194400 loss : 44.681 NLL : -44.681 KLD : 0.000 
iteration : 194600 loss : 56.294 NLL : -56.294 KLD : 0.000 
iteration : 194800 loss : 43.614 NLL : -43.614 KLD : 0.000 
iteration : 195000 loss : 51.722 NLL : -51.722 KLD : 0.000 
iteration : 195200 loss : 76.566 NLL : -76.566 KLD : 0.000 
iteration : 195400 loss : 43.808 NLL : -43.808 KLD : 0.000 
iteration : 195600 loss : 40.362 NLL : -40.362 KLD : 0.000 
iteration : 195800 loss : 75.067 NLL : -75.067 KLD : 0.000 
iteration : 196000 loss : 64.993 NLL : -64.993 KLD : 0.000 
iteration : 196200 loss : 59.001 NLL : -59.001 KLD : 0.000 
iteration : 196400 loss : 74.925 NLL : -74.925 KLD : 0.000 
iteration : 196600 loss : 62.982 NLL : -62.982 KLD : 0.000 
iteration : 196800 loss : 78.120 NLL : -78.120 KLD : 0.000 
iteration : 197000 loss : 146.427 NLL : -146.427 KLD : 0.000 
iteration : 197200 loss : 90.522 NLL : -90.522 KLD : 0.000 
iteration : 197400 loss : 82.140 NLL : -82.140 KLD : 0.000 
iteration : 197600 loss : 123.325 NLL : -123.325 KLD : 0.000 
iteration : 197800 loss : 90.321 NLL : -90.321 KLD : 0.000 
iteration : 198000 loss : 69.666 NLL : -69.666 KLD : 0.000 
iteration : 198200 loss : 142.701 NLL : -142.701 KLD : 0.000 
iteration : 198400 loss : 172.769 NLL : -172.769 KLD : 0.000 
iteration : 198600 loss : 148.776 NLL : -148.776 KLD : 0.000 
iteration : 198800 loss : 54.834 NLL : -54.834 KLD : 0.000 
iteration : 199000 loss : 64.688 NLL : -64.688 KLD : 0.000 
iteration : 199200 loss : 36.478 NLL : -36.478 KLD : 0.000 
iteration : 199400 loss : 55.945 NLL : -55.945 KLD : 0.000 
iteration : 199600 loss : 94.796 NLL : -94.796 KLD : 0.000 
iteration : 199800 loss : 207.258 NLL : -207.258 KLD : 0.000 
iteration : 200000 loss : 186.098 NLL : -186.098 KLD : 0.000 
---------- Save the model! (step:None) ----------
