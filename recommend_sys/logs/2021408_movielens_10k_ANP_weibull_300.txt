---------- Training loss 56384.926 updated ! and save the model! (step:6) ----------
---------- Training loss 1315.227 updated ! and save the model! (step:12) ----------
---------- Training loss 358.296 updated ! and save the model! (step:18) ----------
---------- Training loss 350.736 updated ! and save the model! (step:24) ----------
---------- Training loss 145.979 updated ! and save the model! (step:36) ----------
---------- Training loss 108.929 updated ! and save the model! (step:48) ----------
---------- Training loss 92.026 updated ! and save the model! (step:54) ----------
---------- Training loss 88.517 updated ! and save the model! (step:66) ----------
---------- Training loss 85.551 updated ! and save the model! (step:72) ----------
---------- Training loss 84.114 updated ! and save the model! (step:102) ----------
---------- Training loss 81.729 updated ! and save the model! (step:114) ----------
---------- Training loss 80.720 updated ! and save the model! (step:162) ----------
---------- Training loss 77.058 updated ! and save the model! (step:174) ----------
---------- Training loss 67.849 updated ! and save the model! (step:198) ----------
iteration : 200 loss : 57.650 NLL : -48.894 KLD : 2.106 KLD_attention : 6.650 
---------- Training loss 56.752 updated ! and save the model! (step:210) ----------
---------- Training loss 56.280 updated ! and save the model! (step:288) ----------
---------- Training loss 47.007 updated ! and save the model! (step:318) ----------
---------- Training loss 45.305 updated ! and save the model! (step:348) ----------
iteration : 400 loss : 35.163 NLL : -28.708 KLD : 0.134 KLD_attention : 6.321 
iteration : 600 loss : 55.216 NLL : -48.367 KLD : 0.028 KLD_attention : 6.821 
iteration : 800 loss : 66.843 NLL : -59.945 KLD : 0.016 KLD_attention : 6.882 
iteration : 1000 loss : 70.073 NLL : -63.545 KLD : 0.055 KLD_attention : 6.472 
---------- Training loss 43.329 updated ! and save the model! (step:1062) ----------
iteration : 1200 loss : 35.649 NLL : -29.465 KLD : 0.042 KLD_attention : 6.142 
iteration : 1400 loss : 50.218 NLL : -43.985 KLD : 0.051 KLD_attention : 6.182 
---------- Training loss 42.466 updated ! and save the model! (step:1452) ----------
iteration : 1600 loss : 49.931 NLL : -43.199 KLD : 0.062 KLD_attention : 6.669 
iteration : 1800 loss : 99.076 NLL : -91.934 KLD : 0.623 KLD_attention : 6.520 
iteration : 2000 loss : 42.544 NLL : -35.946 KLD : 0.042 KLD_attention : 6.556 
iteration : 2200 loss : 50.405 NLL : -43.594 KLD : 0.023 KLD_attention : 6.788 
iteration : 2400 loss : 88.103 NLL : -81.070 KLD : 0.155 KLD_attention : 6.878 
iteration : 2600 loss : 46.025 NLL : -39.519 KLD : 0.032 KLD_attention : 6.474 
iteration : 2800 loss : 82.576 NLL : -75.262 KLD : 1.059 KLD_attention : 6.256 
iteration : 3000 loss : 103.139 NLL : -96.207 KLD : 0.208 KLD_attention : 6.723 
iteration : 3200 loss : 55.954 NLL : -49.484 KLD : 0.030 KLD_attention : 6.440 
iteration : 3400 loss : 69.876 NLL : -62.489 KLD : 1.422 KLD_attention : 5.965 
iteration : 3600 loss : 84.308 NLL : -77.558 KLD : 0.030 KLD_attention : 6.720 
iteration : 3800 loss : 100.032 NLL : -93.607 KLD : 0.202 KLD_attention : 6.222 
iteration : 4000 loss : 47.060 NLL : -40.549 KLD : 0.051 KLD_attention : 6.460 
iteration : 4200 loss : 48.209 NLL : -41.563 KLD : 0.061 KLD_attention : 6.585 
---------- Training loss 41.373 updated ! and save the model! (step:4380) ----------
iteration : 4400 loss : 62.319 NLL : -55.354 KLD : 0.022 KLD_attention : 6.943 
iteration : 4600 loss : 52.689 NLL : -45.991 KLD : 0.023 KLD_attention : 6.674 
---------- Training loss 37.369 updated ! and save the model! (step:4668) ----------
iteration : 4800 loss : 71.160 NLL : -64.352 KLD : 0.016 KLD_attention : 6.792 
iteration : 5000 loss : 40.890 NLL : -34.402 KLD : 0.332 KLD_attention : 6.156 
iteration : 5200 loss : 46.066 NLL : -39.597 KLD : 0.084 KLD_attention : 6.385 
iteration : 5400 loss : 49.855 NLL : -43.370 KLD : 0.050 KLD_attention : 6.434 
iteration : 5600 loss : 68.130 NLL : -61.222 KLD : 0.020 KLD_attention : 6.889 
iteration : 5800 loss : 53.333 NLL : -46.869 KLD : 0.454 KLD_attention : 6.010 
iteration : 6000 loss : 57.914 NLL : -51.339 KLD : 0.017 KLD_attention : 6.557 
iteration : 6200 loss : 62.027 NLL : -55.327 KLD : 0.016 KLD_attention : 6.683 
iteration : 6400 loss : 24.957 NLL : -18.668 KLD : 0.059 KLD_attention : 6.230 
iteration : 6600 loss : 65.993 NLL : -59.080 KLD : 0.023 KLD_attention : 6.891 
iteration : 6800 loss : 45.186 NLL : -39.175 KLD : 0.027 KLD_attention : 5.985 
iteration : 7000 loss : 72.892 NLL : -66.291 KLD : 0.028 KLD_attention : 6.573 
iteration : 7200 loss : 49.162 NLL : -42.543 KLD : 0.020 KLD_attention : 6.599 
iteration : 7400 loss : 69.893 NLL : -63.465 KLD : 0.140 KLD_attention : 6.287 
iteration : 7600 loss : 70.390 NLL : -63.329 KLD : 0.012 KLD_attention : 7.049 
iteration : 7800 loss : 61.726 NLL : -54.888 KLD : 0.013 KLD_attention : 6.825 
iteration : 8000 loss : 43.848 NLL : -37.146 KLD : 0.075 KLD_attention : 6.627 
iteration : 8200 loss : 36.737 NLL : -30.193 KLD : 0.032 KLD_attention : 6.512 
iteration : 8400 loss : 84.728 NLL : -77.498 KLD : 0.031 KLD_attention : 7.199 
iteration : 8600 loss : 62.950 NLL : -56.694 KLD : 0.013 KLD_attention : 6.243 
iteration : 8800 loss : 61.577 NLL : -54.938 KLD : 0.025 KLD_attention : 6.614 
iteration : 9000 loss : 101.283 NLL : -95.125 KLD : 0.050 KLD_attention : 6.108 
iteration : 9200 loss : 80.595 NLL : -73.486 KLD : 0.104 KLD_attention : 7.006 
iteration : 9400 loss : 85.238 NLL : -78.216 KLD : 0.168 KLD_attention : 6.855 
iteration : 9600 loss : 63.480 NLL : -56.486 KLD : 0.125 KLD_attention : 6.870 
iteration : 9800 loss : 47.403 NLL : -40.468 KLD : 0.014 KLD_attention : 6.920 
iteration : 10000 loss : 66.517 NLL : -59.740 KLD : 0.014 KLD_attention : 6.763 
iteration : 10200 loss : 62.392 NLL : -55.766 KLD : 0.170 KLD_attention : 6.457 
iteration : 10400 loss : 43.876 NLL : -37.207 KLD : 0.007 KLD_attention : 6.662 
iteration : 10600 loss : 96.701 NLL : -89.618 KLD : 0.004 KLD_attention : 7.079 
iteration : 10800 loss : 63.314 NLL : -56.371 KLD : 0.004 KLD_attention : 6.939 
iteration : 11000 loss : 61.108 NLL : -54.879 KLD : 0.031 KLD_attention : 6.198 
iteration : 11200 loss : 53.211 NLL : -46.217 KLD : 0.004 KLD_attention : 6.990 
iteration : 11400 loss : 88.001 NLL : -81.224 KLD : 0.087 KLD_attention : 6.689 
iteration : 11600 loss : 71.091 NLL : -62.885 KLD : 0.478 KLD_attention : 7.727 
iteration : 11800 loss : 100.044 NLL : -91.445 KLD : 0.131 KLD_attention : 8.468 
iteration : 12000 loss : 75.255 NLL : -64.895 KLD : 0.149 KLD_attention : 10.212 
iteration : 12200 loss : 82.534 NLL : -63.702 KLD : 0.365 KLD_attention : 18.467 
iteration : 12400 loss : 124.605 NLL : -113.583 KLD : 0.218 KLD_attention : 10.804 
iteration : 12600 loss : 186.967 NLL : -172.341 KLD : 3.822 KLD_attention : 10.804 
iteration : 12800 loss : 115.280 NLL : -104.174 KLD : 0.302 KLD_attention : 10.804 
iteration : 13000 loss : 79.884 NLL : -69.562 KLD : 0.110 KLD_attention : 10.212 
iteration : 13200 loss : 84.064 NLL : -73.130 KLD : 0.129 KLD_attention : 10.804 
iteration : 13400 loss : 64.727 NLL : -53.815 KLD : 0.108 KLD_attention : 10.804 
iteration : 13600 loss : 67.346 NLL : -54.606 KLD : 0.157 KLD_attention : 12.583 
iteration : 13800 loss : 60.591 NLL : -42.687 KLD : 2.442 KLD_attention : 15.462 
iteration : 14000 loss : 66.826 NLL : -54.099 KLD : 0.143 KLD_attention : 12.583 
iteration : 14200 loss : 87.528 NLL : -75.570 KLD : 0.157 KLD_attention : 11.801 
iteration : 14400 loss : 104.972 NLL : -88.008 KLD : 1.502 KLD_attention : 15.462 
iteration : 14600 loss : 61.158 NLL : -45.007 KLD : 0.689 KLD_attention : 15.462 
iteration : 14800 loss : 68.989 NLL : -58.926 KLD : 0.062 KLD_attention : 10.001 
iteration : 15000 loss : 58.230 NLL : -48.348 KLD : 0.052 KLD_attention : 9.830 
iteration : 15200 loss : 76.497 NLL : -66.576 KLD : 0.090 KLD_attention : 9.830 
iteration : 15400 loss : 68.887 NLL : -58.825 KLD : 0.060 KLD_attention : 10.001 
iteration : 15600 loss : 76.676 NLL : -65.339 KLD : 0.105 KLD_attention : 11.232 
iteration : 15800 loss : 86.912 NLL : -74.840 KLD : 0.271 KLD_attention : 11.801 
iteration : 16000 loss : 85.784 NLL : -74.133 KLD : 0.419 KLD_attention : 11.232 
iteration : 16200 loss : 105.904 NLL : -93.065 KLD : 0.256 KLD_attention : 12.583 
iteration : 16400 loss : 121.826 NLL : -110.916 KLD : 0.106 KLD_attention : 10.804 
iteration : 16600 loss : 65.050 NLL : -52.288 KLD : 0.178 KLD_attention : 12.583 
iteration : 16800 loss : 54.324 NLL : -38.711 KLD : 0.151 KLD_attention : 15.462 
iteration : 17000 loss : 65.140 NLL : -53.291 KLD : 0.048 KLD_attention : 11.801 
iteration : 17200 loss : 103.229 NLL : -88.288 KLD : 4.467 KLD_attention : 10.473 
iteration : 17400 loss : 70.576 NLL : -42.182 KLD : 9.928 KLD_attention : 18.467 
iteration : 17600 loss : 109.762 NLL : -95.358 KLD : 0.690 KLD_attention : 13.714 
iteration : 17800 loss : 90.325 NLL : -78.230 KLD : 0.863 KLD_attention : 11.232 
iteration : 18000 loss : 80.252 NLL : -69.983 KLD : 0.058 KLD_attention : 10.212 
iteration : 18200 loss : 77.421 NLL : -63.402 KLD : 0.305 KLD_attention : 13.714 
iteration : 18400 loss : 57.955 NLL : -41.514 KLD : 0.979 KLD_attention : 15.462 
iteration : 18600 loss : 93.654 NLL : -83.545 KLD : 0.108 KLD_attention : 10.001 
iteration : 18800 loss : 73.747 NLL : -58.012 KLD : 0.274 KLD_attention : 15.462 
iteration : 19000 loss : 36.748 NLL : -21.114 KLD : 0.172 KLD_attention : 15.462 
iteration : 19200 loss : 62.375 NLL : -51.072 KLD : 0.071 KLD_attention : 11.232 
iteration : 19400 loss : 50.575 NLL : -38.641 KLD : 0.133 KLD_attention : 11.801 
iteration : 19600 loss : 72.510 NLL : -56.803 KLD : 0.245 KLD_attention : 15.462 
iteration : 19800 loss : 46.085 NLL : -32.249 KLD : 0.122 KLD_attention : 13.714 
iteration : 20000 loss : 82.981 NLL : -72.470 KLD : 0.038 KLD_attention : 10.473 
iteration : 20200 loss : 96.204 NLL : -80.555 KLD : 0.188 KLD_attention : 15.462 
iteration : 20400 loss : 66.399 NLL : -55.836 KLD : 0.090 KLD_attention : 10.473 
iteration : 20600 loss : 110.624 NLL : -100.565 KLD : 0.058 KLD_attention : 10.001 
iteration : 20800 loss : 83.015 NLL : -73.168 KLD : 0.017 KLD_attention : 9.830 
iteration : 21000 loss : 61.487 NLL : -50.178 KLD : 0.078 KLD_attention : 11.232 
iteration : 21200 loss : 85.119 NLL : -74.589 KLD : 0.057 KLD_attention : 10.473 
iteration : 21400 loss : 75.006 NLL : -65.166 KLD : 0.009 KLD_attention : 9.830 
iteration : 21600 loss : 93.768 NLL : -83.419 KLD : 0.137 KLD_attention : 10.212 
iteration : 21800 loss : 49.849 NLL : -34.054 KLD : 0.333 KLD_attention : 15.462 
iteration : 22000 loss : 49.994 NLL : -39.505 KLD : 0.016 KLD_attention : 10.473 
iteration : 22200 loss : 74.638 NLL : -64.529 KLD : 0.108 KLD_attention : 10.001 
iteration : 22400 loss : 96.608 NLL : -86.768 KLD : 0.010 KLD_attention : 9.830 
iteration : 22600 loss : 52.712 NLL : -42.876 KLD : 0.006 KLD_attention : 9.830 
iteration : 22800 loss : 110.336 NLL : -84.624 KLD : 13.128 KLD_attention : 12.583 
iteration : 23000 loss : 101.613 NLL : -88.509 KLD : 0.521 KLD_attention : 12.583 
iteration : 23200 loss : 84.486 NLL : -73.916 KLD : 0.097 KLD_attention : 10.473 
iteration : 23400 loss : 56.010 NLL : -40.199 KLD : 0.349 KLD_attention : 15.462 
iteration : 23600 loss : 114.647 NLL : -98.111 KLD : 1.075 KLD_attention : 15.462 
iteration : 23800 loss : 54.249 NLL : -42.776 KLD : 0.241 KLD_attention : 11.232 
iteration : 24000 loss : 88.055 NLL : -77.991 KLD : 0.062 KLD_attention : 10.001 
iteration : 24200 loss : 40.339 NLL : -21.239 KLD : 0.633 KLD_attention : 18.467 
iteration : 24400 loss : 69.635 NLL : -57.767 KLD : 0.636 KLD_attention : 11.232 
iteration : 24600 loss : 80.543 NLL : -64.643 KLD : 0.437 KLD_attention : 15.462 
iteration : 24800 loss : 73.990 NLL : -63.964 KLD : 0.024 KLD_attention : 10.001 
iteration : 25000 loss : 68.032 NLL : -52.393 KLD : 0.177 KLD_attention : 15.462 
iteration : 25200 loss : 89.953 NLL : -78.610 KLD : 0.110 KLD_attention : 11.232 
iteration : 25400 loss : 43.350 NLL : -26.884 KLD : 1.004 KLD_attention : 15.462 
iteration : 25600 loss : 46.125 NLL : -33.416 KLD : 0.126 KLD_attention : 12.583 
iteration : 25800 loss : 85.076 NLL : -73.787 KLD : 0.057 KLD_attention : 11.232 
iteration : 26000 loss : 67.699 NLL : -56.763 KLD : 0.464 KLD_attention : 10.473 
iteration : 26200 loss : 82.189 NLL : -72.140 KLD : 0.047 KLD_attention : 10.001 
iteration : 26400 loss : 58.942 NLL : -33.076 KLD : 7.399 KLD_attention : 18.467 
iteration : 26600 loss : 97.362 NLL : -86.824 KLD : 0.065 KLD_attention : 10.473 
iteration : 26800 loss : 61.778 NLL : -47.946 KLD : 0.119 KLD_attention : 13.714 
iteration : 27000 loss : 63.875 NLL : -53.845 KLD : 0.030 KLD_attention : 10.001 
iteration : 27200 loss : 61.412 NLL : -50.587 KLD : 0.020 KLD_attention : 10.804 
iteration : 27400 loss : 104.643 NLL : -94.797 KLD : 0.016 KLD_attention : 9.830 
iteration : 27600 loss : 93.203 NLL : -82.967 KLD : 0.025 KLD_attention : 10.212 
iteration : 27800 loss : 99.345 NLL : -89.332 KLD : 0.012 KLD_attention : 10.001 
iteration : 28000 loss : 119.485 NLL : -108.179 KLD : 0.073 KLD_attention : 11.232 
iteration : 28200 loss : 63.510 NLL : -53.500 KLD : 0.009 KLD_attention : 10.001 
iteration : 28400 loss : 59.902 NLL : -47.307 KLD : 0.012 KLD_attention : 12.583 
iteration : 28600 loss : 59.179 NLL : -43.698 KLD : 0.019 KLD_attention : 15.462 
iteration : 28800 loss : 36.670 NLL : -24.080 KLD : 0.007 KLD_attention : 12.583 
iteration : 29000 loss : 64.173 NLL : -53.953 KLD : 0.009 KLD_attention : 10.212 
iteration : 29200 loss : 293.522 NLL : -275.028 KLD : 0.027 KLD_attention : 18.467 
iteration : 29400 loss : 73.372 NLL : -57.895 KLD : 0.015 KLD_attention : 15.462 
iteration : 29600 loss : 74.163 NLL : -62.927 KLD : 0.004 KLD_attention : 11.232 
iteration : 29800 loss : 68.124 NLL : -57.911 KLD : 0.001 KLD_attention : 10.212 
iteration : 30000 loss : 66.128 NLL : -55.653 KLD : 0.001 KLD_attention : 10.473 
---------- Training loss 55.815 updated ! and save the model! (step:30000) ----------
---------- Training loss 53.375 updated ! and save the model! (step:30030) ----------
---------- Training loss 52.124 updated ! and save the model! (step:30186) ----------
---------- Training loss 49.945 updated ! and save the model! (step:30198) ----------
iteration : 30200 loss : 75.111 NLL : -61.395 KLD : 0.002 KLD_attention : 13.714 
iteration : 30400 loss : 66.355 NLL : -55.123 KLD : 0.000 KLD_attention : 11.232 
iteration : 30600 loss : 120.297 NLL : -109.823 KLD : 0.001 KLD_attention : 10.473 
iteration : 30800 loss : 63.700 NLL : -48.238 KLD : 0.001 KLD_attention : 15.462 
iteration : 31000 loss : 117.904 NLL : -105.321 KLD : 0.000 KLD_attention : 12.583 
iteration : 31200 loss : 50.545 NLL : -39.312 KLD : 0.000 KLD_attention : 11.232 
iteration : 31400 loss : 89.389 NLL : -78.157 KLD : 0.000 KLD_attention : 11.232 
iteration : 31600 loss : 93.616 NLL : -83.786 KLD : 0.000 KLD_attention : 9.830 
iteration : 31800 loss : 59.035 NLL : -49.205 KLD : 0.000 KLD_attention : 9.830 
---------- Training loss 46.068 updated ! and save the model! (step:31860) ----------
iteration : 32000 loss : 55.824 NLL : -45.351 KLD : 0.000 KLD_attention : 10.473 
iteration : 32200 loss : 48.116 NLL : -29.649 KLD : 0.000 KLD_attention : 18.467 
iteration : 32400 loss : 45.791 NLL : -27.325 KLD : 0.000 KLD_attention : 18.467 
iteration : 32600 loss : 75.391 NLL : -59.929 KLD : 0.000 KLD_attention : 15.462 
iteration : 32800 loss : 86.933 NLL : -75.132 KLD : 0.000 KLD_attention : 11.801 
iteration : 33000 loss : 66.396 NLL : -56.185 KLD : 0.000 KLD_attention : 10.212 
iteration : 33200 loss : 58.190 NLL : -48.188 KLD : 0.000 KLD_attention : 10.001 
iteration : 33400 loss : 69.629 NLL : -58.825 KLD : 0.000 KLD_attention : 10.804 
iteration : 33600 loss : 39.709 NLL : -27.909 KLD : 0.000 KLD_attention : 11.801 
iteration : 33800 loss : 109.877 NLL : -100.047 KLD : 0.000 KLD_attention : 9.830 
iteration : 34000 loss : 65.252 NLL : -52.669 KLD : 0.000 KLD_attention : 12.583 
iteration : 34200 loss : 65.170 NLL : -49.708 KLD : 0.000 KLD_attention : 15.462 
iteration : 34400 loss : 75.964 NLL : -64.164 KLD : 0.000 KLD_attention : 11.801 
iteration : 34600 loss : 57.999 NLL : -47.787 KLD : 0.000 KLD_attention : 10.212 
iteration : 34800 loss : 61.580 NLL : -51.579 KLD : 0.000 KLD_attention : 10.001 
iteration : 35000 loss : 143.044 NLL : -130.461 KLD : 0.000 KLD_attention : 12.583 
iteration : 35200 loss : 62.465 NLL : -51.992 KLD : 0.000 KLD_attention : 10.473 
iteration : 35400 loss : 72.435 NLL : -61.962 KLD : 0.000 KLD_attention : 10.473 
iteration : 35600 loss : 135.604 NLL : -125.774 KLD : 0.000 KLD_attention : 9.830 
iteration : 35800 loss : 59.807 NLL : -49.977 KLD : 0.000 KLD_attention : 9.830 
iteration : 36000 loss : 66.687 NLL : -56.857 KLD : 0.000 KLD_attention : 9.830 
iteration : 36200 loss : 119.190 NLL : -108.979 KLD : 0.000 KLD_attention : 10.212 
iteration : 36400 loss : 152.100 NLL : -133.633 KLD : 0.000 KLD_attention : 18.467 
iteration : 36600 loss : 153.309 NLL : -143.479 KLD : 0.000 KLD_attention : 9.830 
iteration : 36800 loss : 115.145 NLL : -103.344 KLD : 0.000 KLD_attention : 11.801 
iteration : 37000 loss : 201.731 NLL : -190.499 KLD : 0.000 KLD_attention : 11.232 
iteration : 37200 loss : 195.225 NLL : -179.763 KLD : 0.000 KLD_attention : 15.462 
iteration : 37400 loss : 119.937 NLL : -107.353 KLD : 0.000 KLD_attention : 12.583 
iteration : 37600 loss : 176.383 NLL : -165.910 KLD : 0.000 KLD_attention : 10.473 
iteration : 37800 loss : 51045.930 NLL : -51035.457 KLD : 0.000 KLD_attention : 10.473 
iteration : 38000 loss : 123.554 NLL : -111.753 KLD : 0.000 KLD_attention : 11.801 
iteration : 38200 loss : 149.905 NLL : -138.104 KLD : 0.000 KLD_attention : 11.801 
iteration : 38400 loss : 185.426 NLL : -174.621 KLD : 0.000 KLD_attention : 10.804 
iteration : 38600 loss : 157.142 NLL : -146.337 KLD : 0.000 KLD_attention : 10.804 
iteration : 38800 loss : 124.625 NLL : -114.152 KLD : 0.000 KLD_attention : 10.473 
iteration : 39000 loss : 161.830 NLL : -149.246 KLD : 0.000 KLD_attention : 12.583 
iteration : 39200 loss : 167.255 NLL : -155.454 KLD : 0.000 KLD_attention : 11.801 
iteration : 39400 loss : 132.785 NLL : -114.318 KLD : 0.000 KLD_attention : 18.467 
iteration : 39600 loss : 119.223 NLL : -108.750 KLD : 0.000 KLD_attention : 10.473 
iteration : 39800 loss : 134.903 NLL : -124.430 KLD : 0.000 KLD_attention : 10.473 
iteration : 40000 loss : 121.311 NLL : -110.079 KLD : 0.000 KLD_attention : 11.232 
iteration : 40200 loss : 89.147 NLL : -76.564 KLD : 0.000 KLD_attention : 12.583 
iteration : 40400 loss : 810.336 NLL : -791.869 KLD : 0.000 KLD_attention : 18.467 
iteration : 40600 loss : 115.735 NLL : -105.524 KLD : 0.000 KLD_attention : 10.212 
iteration : 40800 loss : 134.238 NLL : -121.655 KLD : 0.000 KLD_attention : 12.583 
iteration : 41000 loss : 89.860 NLL : -76.146 KLD : 0.000 KLD_attention : 13.714 
iteration : 41200 loss : 155.685 NLL : -145.856 KLD : 0.000 KLD_attention : 9.830 
iteration : 41400 loss : 145.921 NLL : -135.920 KLD : 0.000 KLD_attention : 10.001 
iteration : 41600 loss : 155.716 NLL : -145.242 KLD : 0.000 KLD_attention : 10.473 
iteration : 41800 loss : 159.138 NLL : -140.672 KLD : 0.000 KLD_attention : 18.467 
iteration : 42000 loss : 140.464 NLL : -126.750 KLD : 0.000 KLD_attention : 13.714 
iteration : 42200 loss : 138.629 NLL : -127.397 KLD : 0.000 KLD_attention : 11.232 
iteration : 42400 loss : 151.585 NLL : -141.112 KLD : 0.000 KLD_attention : 10.473 
iteration : 42600 loss : 146.665 NLL : -136.192 KLD : 0.000 KLD_attention : 10.473 
iteration : 42800 loss : 153.689 NLL : -143.216 KLD : 0.000 KLD_attention : 10.473 
iteration : 43000 loss : 172.130 NLL : -156.668 KLD : 0.000 KLD_attention : 15.462 
iteration : 43200 loss : 140.010 NLL : -129.799 KLD : 0.000 KLD_attention : 10.212 
iteration : 43400 loss : 109.016 NLL : -93.554 KLD : 0.000 KLD_attention : 15.462 
iteration : 43600 loss : 116.162 NLL : -100.700 KLD : 0.000 KLD_attention : 15.462 
iteration : 43800 loss : 122.540 NLL : -112.066 KLD : 0.000 KLD_attention : 10.473 
iteration : 44000 loss : 167.071 NLL : -156.859 KLD : 0.000 KLD_attention : 10.212 
iteration : 44200 loss : 147.673 NLL : -137.672 KLD : 0.000 KLD_attention : 10.001 
iteration : 44400 loss : 156.343 NLL : -146.513 KLD : 0.000 KLD_attention : 9.830 
iteration : 44600 loss : 156.167 NLL : -145.694 KLD : 0.000 KLD_attention : 10.473 
iteration : 44800 loss : 135.902 NLL : -125.690 KLD : 0.000 KLD_attention : 10.212 
iteration : 45000 loss : 118.216 NLL : -107.412 KLD : 0.000 KLD_attention : 10.804 
iteration : 45200 loss : 116.304 NLL : -105.071 KLD : 0.000 KLD_attention : 11.232 
iteration : 45400 loss : 148.620 NLL : -138.791 KLD : 0.000 KLD_attention : 9.830 
iteration : 45600 loss : 157.611 NLL : -145.811 KLD : 0.000 KLD_attention : 11.801 
iteration : 45800 loss : 107.999 NLL : -94.285 KLD : 0.000 KLD_attention : 13.714 
iteration : 46000 loss : 164.570 NLL : -154.740 KLD : 0.000 KLD_attention : 9.830 
iteration : 46200 loss : 124.700 NLL : -109.238 KLD : 0.000 KLD_attention : 15.462 
iteration : 46400 loss : 166.652 NLL : -156.822 KLD : 0.000 KLD_attention : 9.830 
iteration : 46600 loss : 121.703 NLL : -103.236 KLD : 0.000 KLD_attention : 18.467 
iteration : 46800 loss : 147.580 NLL : -134.997 KLD : 0.000 KLD_attention : 12.583 
iteration : 47000 loss : 118.354 NLL : -102.892 KLD : 0.000 KLD_attention : 15.462 
iteration : 47200 loss : 140.499 NLL : -128.698 KLD : 0.000 KLD_attention : 11.801 
iteration : 47400 loss : 187.581 NLL : -175.781 KLD : 0.000 KLD_attention : 11.801 
iteration : 47600 loss : 24288.781 NLL : -24277.549 KLD : 0.000 KLD_attention : 11.232 
iteration : 47800 loss : 83.567 NLL : -69.853 KLD : 0.000 KLD_attention : 13.714 
iteration : 48000 loss : 111.416 NLL : -100.183 KLD : 0.000 KLD_attention : 11.232 
iteration : 48200 loss : 85.693 NLL : -67.226 KLD : 0.000 KLD_attention : 18.467 
iteration : 48400 loss : 113.190 NLL : -102.717 KLD : 0.000 KLD_attention : 10.473 
iteration : 48600 loss : 136.965 NLL : -125.733 KLD : 0.000 KLD_attention : 11.232 
iteration : 48800 loss : 203.507 NLL : -190.924 KLD : 0.000 KLD_attention : 12.583 
iteration : 49000 loss : 158.667 NLL : -146.866 KLD : 0.000 KLD_attention : 11.801 
iteration : 49200 loss : 207.276 NLL : -195.475 KLD : 0.000 KLD_attention : 11.801 
iteration : 49400 loss : 207.102 NLL : -196.629 KLD : 0.000 KLD_attention : 10.473 
iteration : 49600 loss : 191.624 NLL : -181.795 KLD : 0.000 KLD_attention : 9.830 
iteration : 49800 loss : 168.272 NLL : -152.810 KLD : 0.000 KLD_attention : 15.462 
iteration : 50000 loss : 138.114 NLL : -127.310 KLD : 0.000 KLD_attention : 10.804 
iteration : 50200 loss : 197.176 NLL : -187.346 KLD : 0.000 KLD_attention : 9.830 
iteration : 50400 loss : 132.161 NLL : -119.578 KLD : 0.000 KLD_attention : 12.583 
iteration : 50600 loss : 154.282 NLL : -143.478 KLD : 0.000 KLD_attention : 10.804 
iteration : 50800 loss : 183.177 NLL : -173.347 KLD : 0.000 KLD_attention : 9.830 
iteration : 51000 loss : 146.913 NLL : -131.451 KLD : 0.000 KLD_attention : 15.462 
iteration : 51200 loss : 209.741 NLL : -199.740 KLD : 0.000 KLD_attention : 10.001 
iteration : 51400 loss : 157.087 NLL : -143.374 KLD : 0.000 KLD_attention : 13.714 
iteration : 51600 loss : 195.931 NLL : -183.348 KLD : 0.000 KLD_attention : 12.583 
iteration : 51800 loss : 213.706 NLL : -201.123 KLD : 0.000 KLD_attention : 12.583 
iteration : 52000 loss : 164.546 NLL : -149.084 KLD : 0.000 KLD_attention : 15.462 
iteration : 52200 loss : 98.284 NLL : -87.480 KLD : 0.000 KLD_attention : 10.804 
iteration : 52400 loss : 495.805 NLL : -477.338 KLD : 0.000 KLD_attention : 18.467 
iteration : 52600 loss : 128.903 NLL : -116.319 KLD : 0.000 KLD_attention : 12.583 
iteration : 52800 loss : 106.401 NLL : -92.687 KLD : 0.000 KLD_attention : 13.714 
iteration : 53000 loss : 199.900 NLL : -188.099 KLD : 0.000 KLD_attention : 11.801 
iteration : 53200 loss : 230.775 NLL : -220.773 KLD : 0.000 KLD_attention : 10.001 
iteration : 53400 loss : 128.946 NLL : -117.145 KLD : 0.000 KLD_attention : 11.801 
iteration : 53600 loss : 149.607 NLL : -137.806 KLD : 0.000 KLD_attention : 11.801 
iteration : 53800 loss : 227.401 NLL : -217.190 KLD : 0.000 KLD_attention : 10.212 
iteration : 54000 loss : 175.443 NLL : -161.730 KLD : 0.000 KLD_attention : 13.714 
iteration : 54200 loss : 181.101 NLL : -169.300 KLD : 0.000 KLD_attention : 11.801 
iteration : 54400 loss : 147.840 NLL : -137.036 KLD : 0.000 KLD_attention : 10.804 
iteration : 54600 loss : 147.431 NLL : -134.848 KLD : 0.000 KLD_attention : 12.583 
iteration : 54800 loss : 198.164 NLL : -186.931 KLD : 0.000 KLD_attention : 11.232 
iteration : 55000 loss : 109.885 NLL : -96.172 KLD : 0.000 KLD_attention : 13.714 
iteration : 55200 loss : 138.253 NLL : -126.453 KLD : 0.000 KLD_attention : 11.801 
iteration : 55400 loss : 1085.507 NLL : -1070.045 KLD : 0.000 KLD_attention : 15.462 
iteration : 55600 loss : 470.684 NLL : -455.222 KLD : 0.000 KLD_attention : 15.462 
iteration : 55800 loss : 74.397 NLL : -60.684 KLD : 0.000 KLD_attention : 13.714 
iteration : 56000 loss : 168.853 NLL : -158.380 KLD : 0.000 KLD_attention : 10.473 
iteration : 56200 loss : 223.014 NLL : -211.782 KLD : 0.000 KLD_attention : 11.232 
iteration : 56400 loss : 201.153 NLL : -187.440 KLD : 0.000 KLD_attention : 13.714 
iteration : 56600 loss : 183.757 NLL : -165.290 KLD : 0.000 KLD_attention : 18.467 
iteration : 56800 loss : 92.801 NLL : -82.328 KLD : 0.000 KLD_attention : 10.473 
iteration : 57000 loss : 81.345 NLL : -71.133 KLD : 0.000 KLD_attention : 10.212 
iteration : 57200 loss : 101.402 NLL : -91.572 KLD : 0.000 KLD_attention : 9.830 
iteration : 57400 loss : 234.815 NLL : -224.342 KLD : 0.000 KLD_attention : 10.473 
iteration : 57600 loss : 152.849 NLL : -139.135 KLD : 0.000 KLD_attention : 13.714 
iteration : 57800 loss : 170.129 NLL : -159.656 KLD : 0.000 KLD_attention : 10.473 
iteration : 58000 loss : 171.527 NLL : -161.316 KLD : 0.000 KLD_attention : 10.212 
iteration : 58200 loss : 149.522 NLL : -134.060 KLD : 0.000 KLD_attention : 15.462 
iteration : 58400 loss : 178.924 NLL : -166.341 KLD : 0.000 KLD_attention : 12.583 
iteration : 58600 loss : 177.017 NLL : -166.805 KLD : 0.000 KLD_attention : 10.212 
iteration : 58800 loss : 85.160 NLL : -69.698 KLD : 0.000 KLD_attention : 15.462 
iteration : 59000 loss : 90.014 NLL : -74.552 KLD : 0.000 KLD_attention : 15.462 
iteration : 59200 loss : 99.241 NLL : -89.240 KLD : 0.000 KLD_attention : 10.001 
iteration : 59400 loss : 66.324 NLL : -52.611 KLD : 0.000 KLD_attention : 13.714 
iteration : 59600 loss : 86.865 NLL : -77.035 KLD : 0.000 KLD_attention : 9.830 
iteration : 59800 loss : 78.836 NLL : -68.835 KLD : 0.000 KLD_attention : 10.001 
iteration : 60000 loss : 66.416 NLL : -55.184 KLD : 0.000 KLD_attention : 11.232 
---------- Training loss 73.567 updated ! and save the model! (step:60000) ----------
iteration : 60200 loss : 88.990 NLL : -75.276 KLD : 0.000 KLD_attention : 13.714 
iteration : 60400 loss : 106.646 NLL : -96.645 KLD : 0.000 KLD_attention : 10.001 
iteration : 60600 loss : 284.744 NLL : -269.282 KLD : 0.000 KLD_attention : 15.462 
---------- Training loss 73.422 updated ! and save the model! (step:60744) ----------
iteration : 60800 loss : 9428.924 NLL : -9413.462 KLD : 0.000 KLD_attention : 15.462 
iteration : 61000 loss : 3587.263 NLL : -3571.801 KLD : 0.000 KLD_attention : 15.462 
iteration : 61200 loss : 6182.546 NLL : -6168.833 KLD : 0.000 KLD_attention : 13.714 
iteration : 61400 loss : 4504.472 NLL : -4490.759 KLD : 0.000 KLD_attention : 13.714 
iteration : 61600 loss : 3568.928 NLL : -3558.927 KLD : 0.000 KLD_attention : 10.001 
iteration : 61800 loss : 4012.740 NLL : -3999.026 KLD : 0.000 KLD_attention : 13.714 
iteration : 62000 loss : 9765.170 NLL : -9754.365 KLD : 0.000 KLD_attention : 10.804 
iteration : 62200 loss : 3601.494 NLL : -3591.021 KLD : 0.000 KLD_attention : 10.473 
iteration : 62400 loss : 4798.759 NLL : -4787.955 KLD : 0.000 KLD_attention : 10.804 
iteration : 62600 loss : 3325.297 NLL : -3314.824 KLD : 0.000 KLD_attention : 10.473 
iteration : 62800 loss : 5744.397 NLL : -5733.924 KLD : 0.000 KLD_attention : 10.473 
iteration : 63000 loss : 3098.541 NLL : -3087.737 KLD : 0.000 KLD_attention : 10.804 
iteration : 63200 loss : 2342.091 NLL : -2331.880 KLD : 0.000 KLD_attention : 10.212 
iteration : 63400 loss : 2067.945 NLL : -2052.483 KLD : 0.000 KLD_attention : 15.462 
iteration : 63600 loss : 2331.185 NLL : -2319.384 KLD : 0.000 KLD_attention : 11.801 
iteration : 63800 loss : 2558.588 NLL : -2540.121 KLD : 0.000 KLD_attention : 18.467 
iteration : 64000 loss : 1542.710 NLL : -1532.709 KLD : 0.000 KLD_attention : 10.001 
iteration : 64200 loss : 2549.789 NLL : -2537.989 KLD : 0.000 KLD_attention : 11.801 
iteration : 64400 loss : 2203.311 NLL : -2189.598 KLD : 0.000 KLD_attention : 13.714 
iteration : 64600 loss : 2177.545 NLL : -2167.072 KLD : 0.000 KLD_attention : 10.473 
iteration : 64800 loss : 2274.453 NLL : -2263.649 KLD : 0.000 KLD_attention : 10.804 
iteration : 65000 loss : 1737.221 NLL : -1726.748 KLD : 0.000 KLD_attention : 10.473 
iteration : 65200 loss : 1587.583 NLL : -1572.121 KLD : 0.000 KLD_attention : 15.462 
iteration : 65400 loss : 1278.627 NLL : -1266.826 KLD : 0.000 KLD_attention : 11.801 
iteration : 65600 loss : 495.321 NLL : -484.088 KLD : 0.000 KLD_attention : 11.232 
iteration : 65800 loss : 941.372 NLL : -925.910 KLD : 0.000 KLD_attention : 15.462 
iteration : 66000 loss : 2810.999 NLL : -2792.532 KLD : 0.000 KLD_attention : 18.467 
iteration : 66200 loss : 904.572 NLL : -892.771 KLD : 0.000 KLD_attention : 11.801 
iteration : 66400 loss : 875.117 NLL : -863.885 KLD : 0.000 KLD_attention : 11.232 
iteration : 66600 loss : 1296.803 NLL : -1281.341 KLD : 0.000 KLD_attention : 15.462 
iteration : 66800 loss : 1625.059 NLL : -1606.592 KLD : 0.000 KLD_attention : 18.467 
iteration : 67000 loss : 1483.469 NLL : -1472.665 KLD : 0.000 KLD_attention : 10.804 
iteration : 67200 loss : 1529.084 NLL : -1518.611 KLD : 0.000 KLD_attention : 10.473 
iteration : 67400 loss : 1796.722 NLL : -1784.921 KLD : 0.000 KLD_attention : 11.801 
iteration : 67600 loss : 1835.417 NLL : -1819.955 KLD : 0.000 KLD_attention : 15.462 
iteration : 67800 loss : 716.779 NLL : -704.195 KLD : 0.000 KLD_attention : 12.583 
iteration : 68000 loss : 672.303 NLL : -666.141 KLD : 0.000 KLD_attention : 6.162 
iteration : 68200 loss : 646.833 NLL : -640.773 KLD : 0.000 KLD_attention : 6.060 
iteration : 68400 loss : 1525.183 NLL : -1518.558 KLD : 0.000 KLD_attention : 6.625 
iteration : 68600 loss : 643.143 NLL : -636.952 KLD : 0.000 KLD_attention : 6.192 
iteration : 68800 loss : 1365.120 NLL : -1359.189 KLD : 0.000 KLD_attention : 5.930 
iteration : 69000 loss : 1175.523 NLL : -1169.208 KLD : 0.000 KLD_attention : 6.315 
iteration : 69200 loss : 1232.963 NLL : -1226.170 KLD : 0.000 KLD_attention : 6.793 
iteration : 69400 loss : 515.635 NLL : -509.443 KLD : 0.000 KLD_attention : 6.192 
iteration : 69600 loss : 965.950 NLL : -959.083 KLD : 0.000 KLD_attention : 6.867 
iteration : 69800 loss : 478.189 NLL : -471.998 KLD : 0.000 KLD_attention : 6.192 
iteration : 70000 loss : 2031.087 NLL : -2024.772 KLD : 0.000 KLD_attention : 6.315 
iteration : 70200 loss : 414.389 NLL : -408.459 KLD : 0.000 KLD_attention : 5.930 
iteration : 70400 loss : 1137.079 NLL : -1131.149 KLD : 0.000 KLD_attention : 5.930 
iteration : 70600 loss : 818.867 NLL : -812.807 KLD : 0.000 KLD_attention : 6.059 
iteration : 70800 loss : 1390.742 NLL : -1384.117 KLD : 0.000 KLD_attention : 6.625 
iteration : 71000 loss : 785.527 NLL : -779.099 KLD : 0.000 KLD_attention : 6.428 
iteration : 71200 loss : 669.635 NLL : -662.767 KLD : 0.000 KLD_attention : 6.867 
iteration : 71400 loss : 555.275 NLL : -548.482 KLD : 0.000 KLD_attention : 6.793 
iteration : 71600 loss : 630.746 NLL : -623.878 KLD : 0.000 KLD_attention : 6.867 
iteration : 71800 loss : 522.340 NLL : -516.281 KLD : 0.000 KLD_attention : 6.059 
iteration : 72000 loss : 895.428 NLL : -888.491 KLD : 0.000 KLD_attention : 6.937 
iteration : 72200 loss : 496.948 NLL : -490.155 KLD : 0.000 KLD_attention : 6.793 
iteration : 72400 loss : 493.078 NLL : -487.019 KLD : 0.000 KLD_attention : 6.059 
iteration : 72600 loss : 605.986 NLL : -600.056 KLD : 0.000 KLD_attention : 5.930 
iteration : 72800 loss : 1167.319 NLL : -1161.004 KLD : 0.000 KLD_attention : 6.315 
iteration : 73000 loss : 788.328 NLL : -781.702 KLD : 0.000 KLD_attention : 6.625 
iteration : 73200 loss : 591.498 NLL : -584.872 KLD : 0.000 KLD_attention : 6.625 
iteration : 73400 loss : 336.764 NLL : -330.051 KLD : 0.000 KLD_attention : 6.712 
iteration : 73600 loss : 446.520 NLL : -440.460 KLD : 0.000 KLD_attention : 6.059 
iteration : 73800 loss : 432.218 NLL : -425.903 KLD : 0.000 KLD_attention : 6.315 
iteration : 74000 loss : 409.954 NLL : -403.242 KLD : 0.000 KLD_attention : 6.712 
iteration : 74200 loss : 857.065 NLL : -850.637 KLD : 0.000 KLD_attention : 6.428 
iteration : 74400 loss : 437.290 NLL : -431.230 KLD : 0.000 KLD_attention : 6.059 
iteration : 74600 loss : 431.478 NLL : -425.419 KLD : 0.000 KLD_attention : 6.059 
iteration : 74800 loss : 258.634 NLL : -252.320 KLD : 0.000 KLD_attention : 6.315 
iteration : 75000 loss : 342.503 NLL : -335.636 KLD : 0.000 KLD_attention : 6.867 
iteration : 75200 loss : 547.128 NLL : -540.503 KLD : 0.000 KLD_attention : 6.625 
iteration : 75400 loss : 168.755 NLL : -162.695 KLD : 0.000 KLD_attention : 6.059 
iteration : 75600 loss : 516.305 NLL : -509.368 KLD : 0.000 KLD_attention : 6.937 
iteration : 75800 loss : 355.362 NLL : -348.934 KLD : 0.000 KLD_attention : 6.428 
iteration : 76000 loss : 466.752 NLL : -459.960 KLD : 0.000 KLD_attention : 6.793 
iteration : 76200 loss : 623.473 NLL : -617.158 KLD : 0.000 KLD_attention : 6.315 
iteration : 76400 loss : 542.793 NLL : -536.168 KLD : 0.000 KLD_attention : 6.625 
iteration : 76600 loss : 246.489 NLL : -240.297 KLD : 0.000 KLD_attention : 6.192 
iteration : 76800 loss : 624.530 NLL : -617.738 KLD : 0.000 KLD_attention : 6.793 
iteration : 77000 loss : 512.388 NLL : -506.458 KLD : 0.000 KLD_attention : 5.930 
iteration : 77200 loss : 520.552 NLL : -513.615 KLD : 0.000 KLD_attention : 6.937 
iteration : 77400 loss : 514.290 NLL : -507.578 KLD : 0.000 KLD_attention : 6.712 
iteration : 77600 loss : 652.176 NLL : -645.550 KLD : 0.000 KLD_attention : 6.625 
iteration : 77800 loss : 297.178 NLL : -290.311 KLD : 0.000 KLD_attention : 6.867 
iteration : 78000 loss : 304.807 NLL : -298.379 KLD : 0.000 KLD_attention : 6.428 
iteration : 78200 loss : 358.987 NLL : -352.796 KLD : 0.000 KLD_attention : 6.192 
iteration : 78400 loss : 480.070 NLL : -473.278 KLD : 0.000 KLD_attention : 6.793 
iteration : 78600 loss : 489.766 NLL : -483.141 KLD : 0.000 KLD_attention : 6.625 
iteration : 78800 loss : 203.532 NLL : -196.906 KLD : 0.000 KLD_attention : 6.625 
iteration : 79000 loss : 468.210 NLL : -461.273 KLD : 0.000 KLD_attention : 6.937 
iteration : 79200 loss : 374.638 NLL : -368.446 KLD : 0.000 KLD_attention : 6.192 
iteration : 79400 loss : 341.015 NLL : -334.484 KLD : 0.000 KLD_attention : 6.531 
iteration : 79600 loss : 213.584 NLL : -206.717 KLD : 0.000 KLD_attention : 6.867 
iteration : 79800 loss : 349.130 NLL : -343.070 KLD : 0.000 KLD_attention : 6.059 
iteration : 80000 loss : 333.028 NLL : -326.713 KLD : 0.000 KLD_attention : 6.315 
iteration : 80200 loss : 283.743 NLL : -277.813 KLD : 0.000 KLD_attention : 5.930 
iteration : 80400 loss : 247.471 NLL : -240.679 KLD : 0.000 KLD_attention : 6.793 
iteration : 80600 loss : 437.442 NLL : -430.575 KLD : 0.000 KLD_attention : 6.867 
iteration : 80800 loss : 248.273 NLL : -241.481 KLD : 0.000 KLD_attention : 6.793 
iteration : 81000 loss : 307.652 NLL : -300.940 KLD : 0.000 KLD_attention : 6.712 
iteration : 81200 loss : 412.249 NLL : -405.312 KLD : 0.000 KLD_attention : 6.937 
iteration : 81400 loss : 614.620 NLL : -608.305 KLD : 0.000 KLD_attention : 6.315 
iteration : 81600 loss : 358.716 NLL : -351.779 KLD : 0.000 KLD_attention : 6.937 
iteration : 81800 loss : 210.657 NLL : -204.229 KLD : 0.000 KLD_attention : 6.428 
iteration : 82000 loss : 311.057 NLL : -304.432 KLD : 0.000 KLD_attention : 6.625 
iteration : 82200 loss : 402.339 NLL : -395.547 KLD : 0.000 KLD_attention : 6.793 
iteration : 82400 loss : 296.971 NLL : -290.104 KLD : 0.000 KLD_attention : 6.867 
iteration : 82600 loss : 202.972 NLL : -196.260 KLD : 0.000 KLD_attention : 6.712 
iteration : 82800 loss : 323.277 NLL : -316.849 KLD : 0.000 KLD_attention : 6.428 
iteration : 83000 loss : 180.498 NLL : -173.631 KLD : 0.000 KLD_attention : 6.867 
iteration : 83200 loss : 320.217 NLL : -313.424 KLD : 0.000 KLD_attention : 6.793 
iteration : 83400 loss : 227.884 NLL : -221.092 KLD : 0.000 KLD_attention : 6.793 
iteration : 83600 loss : 100.959 NLL : -94.767 KLD : 0.000 KLD_attention : 6.192 
iteration : 83800 loss : 368.993 NLL : -362.200 KLD : 0.000 KLD_attention : 6.793 
iteration : 84000 loss : 363.959 NLL : -358.029 KLD : 0.000 KLD_attention : 5.930 
iteration : 84200 loss : 214.567 NLL : -207.942 KLD : 0.000 KLD_attention : 6.625 
iteration : 84400 loss : 174.502 NLL : -168.443 KLD : 0.000 KLD_attention : 6.059 
iteration : 84600 loss : 300.613 NLL : -293.746 KLD : 0.000 KLD_attention : 6.867 
iteration : 84800 loss : 242.430 NLL : -236.500 KLD : 0.000 KLD_attention : 5.930 
iteration : 85000 loss : 245.606 NLL : -239.676 KLD : 0.000 KLD_attention : 5.930 
iteration : 85200 loss : 280.613 NLL : -273.820 KLD : 0.000 KLD_attention : 6.793 
iteration : 85400 loss : 266.736 NLL : -259.799 KLD : 0.000 KLD_attention : 6.937 
iteration : 85600 loss : 203.895 NLL : -197.580 KLD : 0.000 KLD_attention : 6.315 
iteration : 85800 loss : 331.903 NLL : -325.036 KLD : 0.000 KLD_attention : 6.867 
iteration : 86000 loss : 370.248 NLL : -363.381 KLD : 0.000 KLD_attention : 6.867 
iteration : 86200 loss : 202.509 NLL : -195.716 KLD : 0.000 KLD_attention : 6.793 
iteration : 86400 loss : 510.520 NLL : -504.205 KLD : 0.000 KLD_attention : 6.315 
iteration : 86600 loss : 241.800 NLL : -235.007 KLD : 0.000 KLD_attention : 6.793 
iteration : 86800 loss : 197.229 NLL : -190.362 KLD : 0.000 KLD_attention : 6.867 
iteration : 87000 loss : 125.134 NLL : -118.819 KLD : 0.000 KLD_attention : 6.315 
iteration : 87200 loss : 154.003 NLL : -147.576 KLD : 0.000 KLD_attention : 6.428 
iteration : 87400 loss : 102.236 NLL : -95.808 KLD : 0.000 KLD_attention : 6.428 
iteration : 87600 loss : 180.182 NLL : -174.252 KLD : 0.000 KLD_attention : 5.930 
iteration : 87800 loss : 202.955 NLL : -197.025 KLD : 0.000 KLD_attention : 5.930 
iteration : 88000 loss : 163.264 NLL : -157.205 KLD : 0.000 KLD_attention : 6.059 
iteration : 88200 loss : 233.179 NLL : -226.751 KLD : 0.000 KLD_attention : 6.428 
iteration : 88400 loss : 213.790 NLL : -206.853 KLD : 0.000 KLD_attention : 6.937 
iteration : 88600 loss : 319.301 NLL : -312.364 KLD : 0.000 KLD_attention : 6.937 
iteration : 88800 loss : 148.266 NLL : -141.553 KLD : 0.000 KLD_attention : 6.712 
iteration : 89000 loss : 162.468 NLL : -155.675 KLD : 0.000 KLD_attention : 6.793 
iteration : 89200 loss : 137.114 NLL : -130.686 KLD : 0.000 KLD_attention : 6.428 
iteration : 89400 loss : 144.948 NLL : -138.417 KLD : 0.000 KLD_attention : 6.531 
iteration : 89600 loss : 121.496 NLL : -114.871 KLD : 0.000 KLD_attention : 6.625 
iteration : 89800 loss : 106.599 NLL : -99.973 KLD : 0.000 KLD_attention : 6.625 
iteration : 90000 loss : 188.186 NLL : -181.394 KLD : 0.000 KLD_attention : 6.793 
---------- Training loss 181.522 updated ! and save the model! (step:90000) ----------
---------- Training loss 171.284 updated ! and save the model! (step:90018) ----------
---------- Training loss 147.585 updated ! and save the model! (step:90096) ----------
iteration : 90200 loss : 212.680 NLL : -206.621 KLD : 0.000 KLD_attention : 6.059 
---------- Training loss 145.702 updated ! and save the model! (step:90294) ----------
iteration : 90400 loss : 264.106 NLL : -257.575 KLD : 0.000 KLD_attention : 6.531 
iteration : 90600 loss : 271.240 NLL : -264.812 KLD : 0.000 KLD_attention : 6.428 
iteration : 90800 loss : 158.403 NLL : -152.088 KLD : 0.000 KLD_attention : 6.315 
iteration : 91000 loss : 128.935 NLL : -122.744 KLD : 0.000 KLD_attention : 6.192 
---------- Training loss 143.525 updated ! and save the model! (step:91164) ----------
iteration : 91200 loss : 113.286 NLL : -106.859 KLD : 0.000 KLD_attention : 6.428 
iteration : 91400 loss : 173.996 NLL : -167.129 KLD : 0.000 KLD_attention : 6.867 
---------- Training loss 142.472 updated ! and save the model! (step:91440) ----------
iteration : 91600 loss : 144.491 NLL : -137.866 KLD : 0.000 KLD_attention : 6.625 
iteration : 91800 loss : 178.416 NLL : -172.224 KLD : 0.000 KLD_attention : 6.192 
iteration : 92000 loss : 231.584 NLL : -225.525 KLD : 0.000 KLD_attention : 6.059 
---------- Training loss 139.637 updated ! and save the model! (step:92028) ----------
---------- Training loss 137.491 updated ! and save the model! (step:92106) ----------
iteration : 92200 loss : 200.569 NLL : -193.857 KLD : 0.000 KLD_attention : 6.712 
---------- Training loss 124.345 updated ! and save the model! (step:92388) ----------
iteration : 92400 loss : 187.769 NLL : -180.977 KLD : 0.000 KLD_attention : 6.793 
iteration : 92600 loss : 193.978 NLL : -187.919 KLD : 0.000 KLD_attention : 6.059 
iteration : 92800 loss : 180.569 NLL : -173.943 KLD : 0.000 KLD_attention : 6.625 
iteration : 93000 loss : 232.122 NLL : -225.255 KLD : 0.000 KLD_attention : 6.867 
iteration : 93200 loss : 138.015 NLL : -131.700 KLD : 0.000 KLD_attention : 6.315 
iteration : 93400 loss : 134.134 NLL : -128.204 KLD : 0.000 KLD_attention : 5.930 
iteration : 93600 loss : 221.424 NLL : -214.798 KLD : 0.000 KLD_attention : 6.625 
iteration : 93800 loss : 165.169 NLL : -158.302 KLD : 0.000 KLD_attention : 6.867 
---------- Training loss 122.715 updated ! and save the model! (step:93984) ----------
iteration : 94000 loss : 198.365 NLL : -191.573 KLD : 0.000 KLD_attention : 6.793 
iteration : 94200 loss : 164.673 NLL : -158.143 KLD : 0.000 KLD_attention : 6.531 
iteration : 94400 loss : 264.204 NLL : -257.267 KLD : 0.000 KLD_attention : 6.937 
iteration : 94600 loss : 166.375 NLL : -160.445 KLD : 0.000 KLD_attention : 5.930 
iteration : 94800 loss : 143.088 NLL : -137.158 KLD : 0.000 KLD_attention : 5.930 
iteration : 95000 loss : 120.696 NLL : -114.269 KLD : 0.000 KLD_attention : 6.428 
iteration : 95200 loss : 187.059 NLL : -180.867 KLD : 0.000 KLD_attention : 6.192 
iteration : 95400 loss : 165.415 NLL : -159.100 KLD : 0.000 KLD_attention : 6.315 
iteration : 95600 loss : 235.501 NLL : -229.309 KLD : 0.000 KLD_attention : 6.192 
iteration : 95800 loss : 194.565 NLL : -187.852 KLD : 0.000 KLD_attention : 6.712 
iteration : 96000 loss : 152.848 NLL : -146.317 KLD : 0.000 KLD_attention : 6.531 
iteration : 96200 loss : 169.279 NLL : -162.964 KLD : 0.000 KLD_attention : 6.315 
iteration : 96400 loss : 200.268 NLL : -193.332 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 121.330 updated ! and save the model! (step:96546) ----------
iteration : 96600 loss : 112.388 NLL : -106.458 KLD : 0.000 KLD_attention : 5.930 
---------- Training loss 121.191 updated ! and save the model! (step:96792) ----------
iteration : 96800 loss : 182.343 NLL : -175.406 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 110.266 updated ! and save the model! (step:96996) ----------
iteration : 97000 loss : 62.394 NLL : -56.335 KLD : 0.000 KLD_attention : 6.059 
iteration : 97200 loss : 142.492 NLL : -135.780 KLD : 0.000 KLD_attention : 6.712 
iteration : 97400 loss : 121.676 NLL : -114.884 KLD : 0.000 KLD_attention : 6.793 
iteration : 97600 loss : 164.495 NLL : -157.703 KLD : 0.000 KLD_attention : 6.793 
iteration : 97800 loss : 89.861 NLL : -83.931 KLD : 0.000 KLD_attention : 5.930 
iteration : 98000 loss : 146.845 NLL : -139.908 KLD : 0.000 KLD_attention : 6.937 
iteration : 98200 loss : 114.241 NLL : -107.615 KLD : 0.000 KLD_attention : 6.625 
iteration : 98400 loss : 134.587 NLL : -127.962 KLD : 0.000 KLD_attention : 6.625 
iteration : 98600 loss : 157.350 NLL : -150.413 KLD : 0.000 KLD_attention : 6.937 
iteration : 98800 loss : 172.182 NLL : -165.246 KLD : 0.000 KLD_attention : 6.937 
iteration : 99000 loss : 122.221 NLL : -115.509 KLD : 0.000 KLD_attention : 6.712 
iteration : 99200 loss : 99.561 NLL : -92.849 KLD : 0.000 KLD_attention : 6.712 
iteration : 99400 loss : 81.186 NLL : -75.127 KLD : 0.000 KLD_attention : 6.059 
iteration : 99600 loss : 171.881 NLL : -165.014 KLD : 0.000 KLD_attention : 6.867 
iteration : 99800 loss : 144.348 NLL : -137.480 KLD : 0.000 KLD_attention : 6.867 
iteration : 100000 loss : 184.281 NLL : -177.344 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 99.747 updated ! and save the model! (step:100098) ----------
iteration : 100200 loss : 106.364 NLL : -99.427 KLD : 0.000 KLD_attention : 6.937 
iteration : 100400 loss : 124.146 NLL : -117.433 KLD : 0.000 KLD_attention : 6.712 
iteration : 100600 loss : 171.813 NLL : -165.020 KLD : 0.000 KLD_attention : 6.793 
iteration : 100800 loss : 164.482 NLL : -157.546 KLD : 0.000 KLD_attention : 6.937 
iteration : 101000 loss : 222.538 NLL : -215.601 KLD : 0.000 KLD_attention : 6.937 
iteration : 101200 loss : 133.899 NLL : -127.472 KLD : 0.000 KLD_attention : 6.428 
iteration : 101400 loss : 155.663 NLL : -148.870 KLD : 0.000 KLD_attention : 6.793 
iteration : 101600 loss : 117.427 NLL : -110.560 KLD : 0.000 KLD_attention : 6.867 
iteration : 101800 loss : 200.590 NLL : -194.275 KLD : 0.000 KLD_attention : 6.315 
iteration : 102000 loss : 103.587 NLL : -97.273 KLD : 0.000 KLD_attention : 6.315 
---------- Training loss 87.315 updated ! and save the model! (step:102072) ----------
iteration : 102200 loss : 114.095 NLL : -108.035 KLD : 0.000 KLD_attention : 6.059 
iteration : 102400 loss : 144.949 NLL : -138.323 KLD : 0.000 KLD_attention : 6.625 
iteration : 102600 loss : 94.960 NLL : -88.334 KLD : 0.000 KLD_attention : 6.625 
iteration : 102800 loss : 105.619 NLL : -99.560 KLD : 0.000 KLD_attention : 6.059 
iteration : 103000 loss : 68.330 NLL : -62.400 KLD : 0.000 KLD_attention : 5.930 
iteration : 103200 loss : 190.360 NLL : -184.045 KLD : 0.000 KLD_attention : 6.315 
iteration : 103400 loss : 222.233 NLL : -215.296 KLD : 0.000 KLD_attention : 6.937 
iteration : 103600 loss : 146.736 NLL : -140.676 KLD : 0.000 KLD_attention : 6.059 
iteration : 103800 loss : 115.847 NLL : -109.221 KLD : 0.000 KLD_attention : 6.625 
iteration : 104000 loss : 127.707 NLL : -121.176 KLD : 0.000 KLD_attention : 6.531 
iteration : 104200 loss : 109.815 NLL : -103.284 KLD : 0.000 KLD_attention : 6.531 
iteration : 104400 loss : 146.215 NLL : -139.348 KLD : 0.000 KLD_attention : 6.867 
iteration : 104600 loss : 164.104 NLL : -157.479 KLD : 0.000 KLD_attention : 6.625 
iteration : 104800 loss : 102.089 NLL : -95.464 KLD : 0.000 KLD_attention : 6.625 
iteration : 105000 loss : 130.431 NLL : -124.501 KLD : 0.000 KLD_attention : 5.930 
iteration : 105200 loss : 111.653 NLL : -104.940 KLD : 0.000 KLD_attention : 6.712 
iteration : 105400 loss : 162.799 NLL : -156.371 KLD : 0.000 KLD_attention : 6.428 
iteration : 105600 loss : 108.205 NLL : -101.579 KLD : 0.000 KLD_attention : 6.625 
iteration : 105800 loss : 117.005 NLL : -110.138 KLD : 0.000 KLD_attention : 6.867 
iteration : 106000 loss : 126.589 NLL : -119.722 KLD : 0.000 KLD_attention : 6.867 
iteration : 106200 loss : 202.579 NLL : -196.152 KLD : 0.000 KLD_attention : 6.428 
iteration : 106400 loss : 116.863 NLL : -110.436 KLD : 0.000 KLD_attention : 6.428 
iteration : 106600 loss : 136.167 NLL : -129.454 KLD : 0.000 KLD_attention : 6.712 
iteration : 106800 loss : 151.780 NLL : -144.913 KLD : 0.000 KLD_attention : 6.867 
iteration : 107000 loss : 216.297 NLL : -209.361 KLD : 0.000 KLD_attention : 6.937 
iteration : 107200 loss : 133.393 NLL : -127.334 KLD : 0.000 KLD_attention : 6.059 
iteration : 107400 loss : 46.966 NLL : -41.036 KLD : 0.000 KLD_attention : 5.930 
iteration : 107600 loss : 149.188 NLL : -142.476 KLD : 0.000 KLD_attention : 6.712 
iteration : 107800 loss : 146.101 NLL : -139.308 KLD : 0.000 KLD_attention : 6.793 
iteration : 108000 loss : 108.490 NLL : -101.959 KLD : 0.000 KLD_attention : 6.531 
iteration : 108200 loss : 78.529 NLL : -72.338 KLD : 0.000 KLD_attention : 6.192 
iteration : 108400 loss : 123.510 NLL : -117.195 KLD : 0.000 KLD_attention : 6.315 
iteration : 108600 loss : 137.516 NLL : -130.579 KLD : 0.000 KLD_attention : 6.937 
iteration : 108800 loss : 148.825 NLL : -142.398 KLD : 0.000 KLD_attention : 6.428 
iteration : 109000 loss : 61.571 NLL : -55.641 KLD : 0.000 KLD_attention : 5.930 
---------- Training loss 85.031 updated ! and save the model! (step:109128) ----------
iteration : 109200 loss : 143.079 NLL : -136.142 KLD : 0.000 KLD_attention : 6.937 
iteration : 109400 loss : 184.586 NLL : -178.655 KLD : 0.000 KLD_attention : 5.930 
iteration : 109600 loss : 91.268 NLL : -84.840 KLD : 0.000 KLD_attention : 6.428 
iteration : 109800 loss : 106.734 NLL : -100.108 KLD : 0.000 KLD_attention : 6.625 
iteration : 110000 loss : 93.735 NLL : -87.676 KLD : 0.000 KLD_attention : 6.059 
iteration : 110200 loss : 152.850 NLL : -146.535 KLD : 0.000 KLD_attention : 6.315 
iteration : 110400 loss : 144.458 NLL : -138.030 KLD : 0.000 KLD_attention : 6.428 
iteration : 110600 loss : 170.122 NLL : -163.695 KLD : 0.000 KLD_attention : 6.428 
iteration : 110800 loss : 103.352 NLL : -97.037 KLD : 0.000 KLD_attention : 6.315 
iteration : 111000 loss : 138.506 NLL : -131.639 KLD : 0.000 KLD_attention : 6.867 
iteration : 111200 loss : 145.281 NLL : -138.344 KLD : 0.000 KLD_attention : 6.937 
iteration : 111400 loss : 164.053 NLL : -157.861 KLD : 0.000 KLD_attention : 6.192 
iteration : 111600 loss : 84.360 NLL : -78.300 KLD : 0.000 KLD_attention : 6.059 
---------- Training loss 81.564 updated ! and save the model! (step:111744) ----------
iteration : 111800 loss : 85.955 NLL : -79.896 KLD : 0.000 KLD_attention : 6.059 
iteration : 112000 loss : 117.322 NLL : -111.007 KLD : 0.000 KLD_attention : 6.315 
iteration : 112200 loss : 142.222 NLL : -135.907 KLD : 0.000 KLD_attention : 6.315 
iteration : 112400 loss : 78.113 NLL : -71.685 KLD : 0.000 KLD_attention : 6.428 
iteration : 112600 loss : 111.896 NLL : -105.271 KLD : 0.000 KLD_attention : 6.625 
iteration : 112800 loss : 139.360 NLL : -132.830 KLD : 0.000 KLD_attention : 6.531 
iteration : 113000 loss : 98.274 NLL : -91.743 KLD : 0.000 KLD_attention : 6.531 
iteration : 113200 loss : 119.671 NLL : -113.480 KLD : 0.000 KLD_attention : 6.192 
iteration : 113400 loss : 98.383 NLL : -91.852 KLD : 0.000 KLD_attention : 6.531 
iteration : 113600 loss : 139.264 NLL : -132.836 KLD : 0.000 KLD_attention : 6.428 
iteration : 113800 loss : 95.186 NLL : -88.474 KLD : 0.000 KLD_attention : 6.712 
iteration : 114000 loss : 106.287 NLL : -100.095 KLD : 0.000 KLD_attention : 6.192 
iteration : 114200 loss : 142.321 NLL : -135.384 KLD : 0.000 KLD_attention : 6.937 
iteration : 114400 loss : 124.698 NLL : -117.831 KLD : 0.000 KLD_attention : 6.867 
iteration : 114600 loss : 106.127 NLL : -99.812 KLD : 0.000 KLD_attention : 6.315 
iteration : 114800 loss : 132.767 NLL : -125.900 KLD : 0.000 KLD_attention : 6.867 
iteration : 115000 loss : 165.683 NLL : -158.746 KLD : 0.000 KLD_attention : 6.937 
iteration : 115200 loss : 69.834 NLL : -63.643 KLD : 0.000 KLD_attention : 6.192 
iteration : 115400 loss : 95.458 NLL : -88.591 KLD : 0.000 KLD_attention : 6.867 
iteration : 115600 loss : 104.223 NLL : -98.293 KLD : 0.000 KLD_attention : 5.930 
iteration : 115800 loss : 105.526 NLL : -99.596 KLD : 0.000 KLD_attention : 5.930 
iteration : 116000 loss : 123.176 NLL : -116.861 KLD : 0.000 KLD_attention : 6.315 
iteration : 116200 loss : 70.564 NLL : -64.249 KLD : 0.000 KLD_attention : 6.315 
iteration : 116400 loss : 111.881 NLL : -105.689 KLD : 0.000 KLD_attention : 6.192 
iteration : 116600 loss : 68.462 NLL : -62.147 KLD : 0.000 KLD_attention : 6.315 
iteration : 116800 loss : 139.470 NLL : -133.539 KLD : 0.000 KLD_attention : 5.930 
iteration : 117000 loss : 124.459 NLL : -117.747 KLD : 0.000 KLD_attention : 6.712 
iteration : 117200 loss : 108.178 NLL : -101.386 KLD : 0.000 KLD_attention : 6.793 
iteration : 117400 loss : 128.381 NLL : -121.514 KLD : 0.000 KLD_attention : 6.867 
iteration : 117600 loss : 120.406 NLL : -113.469 KLD : 0.000 KLD_attention : 6.937 
iteration : 117800 loss : 121.935 NLL : -115.068 KLD : 0.000 KLD_attention : 6.867 
iteration : 118000 loss : 93.081 NLL : -86.456 KLD : 0.000 KLD_attention : 6.625 
iteration : 118200 loss : 110.994 NLL : -104.202 KLD : 0.000 KLD_attention : 6.793 
iteration : 118400 loss : 108.498 NLL : -101.631 KLD : 0.000 KLD_attention : 6.867 
---------- Training loss 80.274 updated ! and save the model! (step:118584) ----------
iteration : 118600 loss : 93.660 NLL : -87.130 KLD : 0.000 KLD_attention : 6.531 
iteration : 118800 loss : 156.437 NLL : -149.725 KLD : 0.000 KLD_attention : 6.712 
iteration : 119000 loss : 76.267 NLL : -70.075 KLD : 0.000 KLD_attention : 6.192 
iteration : 119200 loss : 107.104 NLL : -100.912 KLD : 0.000 KLD_attention : 6.192 
iteration : 119400 loss : 120.953 NLL : -114.638 KLD : 0.000 KLD_attention : 6.315 
iteration : 119600 loss : 130.950 NLL : -124.325 KLD : 0.000 KLD_attention : 6.625 
iteration : 119800 loss : 85.077 NLL : -78.762 KLD : 0.000 KLD_attention : 6.315 
iteration : 120000 loss : 109.275 NLL : -102.338 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 106.403 updated ! and save the model! (step:120000) ----------
---------- Training loss 96.773 updated ! and save the model! (step:120012) ----------
---------- Training loss 94.183 updated ! and save the model! (step:120042) ----------
---------- Training loss 92.756 updated ! and save the model! (step:120150) ----------
iteration : 120200 loss : 100.056 NLL : -93.628 KLD : 0.000 KLD_attention : 6.428 
---------- Training loss 84.427 updated ! and save the model! (step:120240) ----------
iteration : 120400 loss : 112.175 NLL : -105.463 KLD : 0.000 KLD_attention : 6.712 
iteration : 120600 loss : 100.667 NLL : -93.874 KLD : 0.000 KLD_attention : 6.793 
iteration : 120800 loss : 129.109 NLL : -123.049 KLD : 0.000 KLD_attention : 6.059 
iteration : 121000 loss : 136.815 NLL : -130.501 KLD : 0.000 KLD_attention : 6.315 
iteration : 121200 loss : 133.525 NLL : -126.812 KLD : 0.000 KLD_attention : 6.712 
iteration : 121400 loss : 90.409 NLL : -84.349 KLD : 0.000 KLD_attention : 6.059 
iteration : 121600 loss : 140.872 NLL : -133.935 KLD : 0.000 KLD_attention : 6.937 
iteration : 121800 loss : 132.579 NLL : -125.642 KLD : 0.000 KLD_attention : 6.937 
iteration : 122000 loss : 123.659 NLL : -116.866 KLD : 0.000 KLD_attention : 6.793 
---------- Training loss 82.532 updated ! and save the model! (step:122040) ----------
iteration : 122200 loss : 79.963 NLL : -73.338 KLD : 0.000 KLD_attention : 6.625 
iteration : 122400 loss : 128.018 NLL : -121.150 KLD : 0.000 KLD_attention : 6.867 
iteration : 122600 loss : 92.444 NLL : -85.913 KLD : 0.000 KLD_attention : 6.531 
iteration : 122800 loss : 121.828 NLL : -115.769 KLD : 0.000 KLD_attention : 6.059 
---------- Training loss 80.835 updated ! and save the model! (step:122832) ----------
iteration : 123000 loss : 109.576 NLL : -102.783 KLD : 0.000 KLD_attention : 6.793 
iteration : 123200 loss : 136.607 NLL : -130.415 KLD : 0.000 KLD_attention : 6.192 
iteration : 123400 loss : 109.771 NLL : -103.240 KLD : 0.000 KLD_attention : 6.531 
iteration : 123600 loss : 147.411 NLL : -141.096 KLD : 0.000 KLD_attention : 6.315 
iteration : 123800 loss : 110.080 NLL : -103.368 KLD : 0.000 KLD_attention : 6.712 
iteration : 124000 loss : 74.720 NLL : -68.661 KLD : 0.000 KLD_attention : 6.059 
iteration : 124200 loss : 102.084 NLL : -95.769 KLD : 0.000 KLD_attention : 6.315 
iteration : 124400 loss : 116.717 NLL : -110.403 KLD : 0.000 KLD_attention : 6.315 
iteration : 124600 loss : 117.181 NLL : -110.556 KLD : 0.000 KLD_attention : 6.625 
iteration : 124800 loss : 119.867 NLL : -113.074 KLD : 0.000 KLD_attention : 6.793 
iteration : 125000 loss : 109.892 NLL : -103.025 KLD : 0.000 KLD_attention : 6.867 
---------- Training loss 74.468 updated ! and save the model! (step:125040) ----------
iteration : 125200 loss : 56.155 NLL : -50.096 KLD : 0.000 KLD_attention : 6.059 
iteration : 125400 loss : 122.822 NLL : -116.030 KLD : 0.000 KLD_attention : 6.793 
iteration : 125600 loss : 72.218 NLL : -65.790 KLD : 0.000 KLD_attention : 6.428 
iteration : 125800 loss : 93.197 NLL : -87.137 KLD : 0.000 KLD_attention : 6.059 
iteration : 126000 loss : 49.869 NLL : -43.809 KLD : 0.000 KLD_attention : 6.059 
iteration : 126200 loss : 120.948 NLL : -114.633 KLD : 0.000 KLD_attention : 6.315 
iteration : 126400 loss : 126.471 NLL : -119.604 KLD : 0.000 KLD_attention : 6.867 
iteration : 126600 loss : 105.602 NLL : -99.542 KLD : 0.000 KLD_attention : 6.059 
iteration : 126800 loss : 79.478 NLL : -73.548 KLD : 0.000 KLD_attention : 5.930 
iteration : 127000 loss : 93.577 NLL : -87.046 KLD : 0.000 KLD_attention : 6.531 
iteration : 127200 loss : 105.987 NLL : -99.927 KLD : 0.000 KLD_attention : 6.059 
iteration : 127400 loss : 81.819 NLL : -75.288 KLD : 0.000 KLD_attention : 6.531 
iteration : 127600 loss : 117.824 NLL : -111.293 KLD : 0.000 KLD_attention : 6.531 
iteration : 127800 loss : 95.088 NLL : -89.158 KLD : 0.000 KLD_attention : 5.930 
iteration : 128000 loss : 86.322 NLL : -79.530 KLD : 0.000 KLD_attention : 6.793 
iteration : 128200 loss : 105.130 NLL : -98.938 KLD : 0.000 KLD_attention : 6.192 
iteration : 128400 loss : 105.009 NLL : -98.478 KLD : 0.000 KLD_attention : 6.531 
iteration : 128600 loss : 115.503 NLL : -108.972 KLD : 0.000 KLD_attention : 6.531 
iteration : 128800 loss : 132.301 NLL : -125.770 KLD : 0.000 KLD_attention : 6.531 
iteration : 129000 loss : 82.284 NLL : -75.856 KLD : 0.000 KLD_attention : 6.428 
iteration : 129200 loss : 122.073 NLL : -115.361 KLD : 0.000 KLD_attention : 6.712 
iteration : 129400 loss : 120.405 NLL : -113.468 KLD : 0.000 KLD_attention : 6.937 
iteration : 129600 loss : 128.436 NLL : -122.121 KLD : 0.000 KLD_attention : 6.315 
iteration : 129800 loss : 107.244 NLL : -101.184 KLD : 0.000 KLD_attention : 6.059 
iteration : 130000 loss : 90.143 NLL : -83.517 KLD : 0.000 KLD_attention : 6.625 
iteration : 130200 loss : 111.828 NLL : -105.202 KLD : 0.000 KLD_attention : 6.625 
iteration : 130400 loss : 49.891 NLL : -43.961 KLD : 0.000 KLD_attention : 5.930 
iteration : 130600 loss : 116.383 NLL : -109.757 KLD : 0.000 KLD_attention : 6.625 
iteration : 130800 loss : 90.558 NLL : -83.765 KLD : 0.000 KLD_attention : 6.793 
iteration : 131000 loss : 111.831 NLL : -104.894 KLD : 0.000 KLD_attention : 6.937 
iteration : 131200 loss : 99.366 NLL : -93.174 KLD : 0.000 KLD_attention : 6.192 
iteration : 131400 loss : 126.116 NLL : -119.404 KLD : 0.000 KLD_attention : 6.712 
iteration : 131600 loss : 84.469 NLL : -78.539 KLD : 0.000 KLD_attention : 5.930 
iteration : 131800 loss : 65.417 NLL : -59.102 KLD : 0.000 KLD_attention : 6.315 
iteration : 132000 loss : 89.409 NLL : -82.981 KLD : 0.000 KLD_attention : 6.428 
iteration : 132200 loss : 81.577 NLL : -75.647 KLD : 0.000 KLD_attention : 5.930 
iteration : 132400 loss : 111.344 NLL : -105.285 KLD : 0.000 KLD_attention : 6.059 
iteration : 132600 loss : 91.939 NLL : -85.408 KLD : 0.000 KLD_attention : 6.531 
iteration : 132800 loss : 108.329 NLL : -101.901 KLD : 0.000 KLD_attention : 6.428 
iteration : 133000 loss : 107.747 NLL : -100.955 KLD : 0.000 KLD_attention : 6.793 
iteration : 133200 loss : 69.073 NLL : -63.143 KLD : 0.000 KLD_attention : 5.930 
iteration : 133400 loss : 90.096 NLL : -84.037 KLD : 0.000 KLD_attention : 6.059 
iteration : 133600 loss : 79.264 NLL : -72.638 KLD : 0.000 KLD_attention : 6.625 
iteration : 133800 loss : 93.789 NLL : -87.258 KLD : 0.000 KLD_attention : 6.531 
iteration : 134000 loss : 124.459 NLL : -117.747 KLD : 0.000 KLD_attention : 6.712 
iteration : 134200 loss : 122.079 NLL : -115.651 KLD : 0.000 KLD_attention : 6.428 
iteration : 134400 loss : 84.620 NLL : -77.908 KLD : 0.000 KLD_attention : 6.712 
iteration : 134600 loss : 114.262 NLL : -107.731 KLD : 0.000 KLD_attention : 6.531 
iteration : 134800 loss : 78.404 NLL : -72.345 KLD : 0.000 KLD_attention : 6.059 
iteration : 135000 loss : 86.443 NLL : -80.252 KLD : 0.000 KLD_attention : 6.192 
iteration : 135200 loss : 113.468 NLL : -106.531 KLD : 0.000 KLD_attention : 6.937 
iteration : 135400 loss : 96.077 NLL : -89.546 KLD : 0.000 KLD_attention : 6.531 
iteration : 135600 loss : 115.486 NLL : -108.549 KLD : 0.000 KLD_attention : 6.937 
iteration : 135800 loss : 107.591 NLL : -100.798 KLD : 0.000 KLD_attention : 6.793 
iteration : 136000 loss : 122.234 NLL : -115.703 KLD : 0.000 KLD_attention : 6.531 
iteration : 136200 loss : 118.051 NLL : -111.338 KLD : 0.000 KLD_attention : 6.712 
iteration : 136400 loss : 104.336 NLL : -97.624 KLD : 0.000 KLD_attention : 6.712 
iteration : 136600 loss : 49.813 NLL : -43.882 KLD : 0.000 KLD_attention : 5.930 
iteration : 136800 loss : 64.430 NLL : -58.499 KLD : 0.000 KLD_attention : 5.930 
iteration : 137000 loss : 110.460 NLL : -104.033 KLD : 0.000 KLD_attention : 6.428 
iteration : 137200 loss : 132.296 NLL : -125.981 KLD : 0.000 KLD_attention : 6.315 
iteration : 137400 loss : 86.793 NLL : -80.478 KLD : 0.000 KLD_attention : 6.315 
iteration : 137600 loss : 101.456 NLL : -95.396 KLD : 0.000 KLD_attention : 6.059 
iteration : 137800 loss : 108.180 NLL : -101.988 KLD : 0.000 KLD_attention : 6.192 
iteration : 138000 loss : 88.256 NLL : -82.065 KLD : 0.000 KLD_attention : 6.192 
iteration : 138200 loss : 84.865 NLL : -78.806 KLD : 0.000 KLD_attention : 6.059 
iteration : 138400 loss : 58.522 NLL : -52.592 KLD : 0.000 KLD_attention : 5.930 
iteration : 138600 loss : 119.307 NLL : -112.681 KLD : 0.000 KLD_attention : 6.625 
iteration : 138800 loss : 111.347 NLL : -104.919 KLD : 0.000 KLD_attention : 6.428 
iteration : 139000 loss : 98.309 NLL : -91.442 KLD : 0.000 KLD_attention : 6.867 
iteration : 139200 loss : 99.449 NLL : -92.582 KLD : 0.000 KLD_attention : 6.867 
iteration : 139400 loss : 99.974 NLL : -93.182 KLD : 0.000 KLD_attention : 6.793 
iteration : 139600 loss : 111.901 NLL : -105.370 KLD : 0.000 KLD_attention : 6.531 
iteration : 139800 loss : 80.630 NLL : -74.439 KLD : 0.000 KLD_attention : 6.192 
iteration : 140000 loss : 132.982 NLL : -126.045 KLD : 0.000 KLD_attention : 6.937 
iteration : 140200 loss : 86.505 NLL : -80.077 KLD : 0.000 KLD_attention : 6.428 
iteration : 140400 loss : 123.790 NLL : -116.997 KLD : 0.000 KLD_attention : 6.793 
iteration : 140600 loss : 68.278 NLL : -61.963 KLD : 0.000 KLD_attention : 6.315 
---------- Training loss 72.118 updated ! and save the model! (step:140694) ----------
iteration : 140800 loss : 60.568 NLL : -54.508 KLD : 0.000 KLD_attention : 6.059 
iteration : 141000 loss : 101.893 NLL : -95.181 KLD : 0.000 KLD_attention : 6.712 
iteration : 141200 loss : 109.961 NLL : -103.168 KLD : 0.000 KLD_attention : 6.793 
iteration : 141400 loss : 91.728 NLL : -85.300 KLD : 0.000 KLD_attention : 6.428 
iteration : 141600 loss : 102.640 NLL : -95.773 KLD : 0.000 KLD_attention : 6.867 
iteration : 141800 loss : 130.970 NLL : -124.102 KLD : 0.000 KLD_attention : 6.867 
iteration : 142000 loss : 124.061 NLL : -117.268 KLD : 0.000 KLD_attention : 6.793 
iteration : 142200 loss : 100.944 NLL : -94.413 KLD : 0.000 KLD_attention : 6.531 
iteration : 142400 loss : 106.460 NLL : -99.667 KLD : 0.000 KLD_attention : 6.793 
iteration : 142600 loss : 111.795 NLL : -105.264 KLD : 0.000 KLD_attention : 6.531 
iteration : 142800 loss : 114.032 NLL : -107.604 KLD : 0.000 KLD_attention : 6.428 
iteration : 143000 loss : 44.974 NLL : -39.044 KLD : 0.000 KLD_attention : 5.930 
iteration : 143200 loss : 77.737 NLL : -71.806 KLD : 0.000 KLD_attention : 5.930 
iteration : 143400 loss : 77.639 NLL : -71.108 KLD : 0.000 KLD_attention : 6.531 
iteration : 143600 loss : 125.409 NLL : -118.472 KLD : 0.000 KLD_attention : 6.937 
iteration : 143800 loss : 94.095 NLL : -87.667 KLD : 0.000 KLD_attention : 6.428 
iteration : 144000 loss : 113.922 NLL : -106.986 KLD : 0.000 KLD_attention : 6.937 
iteration : 144200 loss : 73.586 NLL : -67.055 KLD : 0.000 KLD_attention : 6.531 
iteration : 144400 loss : 96.149 NLL : -89.958 KLD : 0.000 KLD_attention : 6.192 
iteration : 144600 loss : 64.975 NLL : -58.783 KLD : 0.000 KLD_attention : 6.192 
iteration : 144800 loss : 105.083 NLL : -98.370 KLD : 0.000 KLD_attention : 6.712 
iteration : 145000 loss : 100.393 NLL : -93.681 KLD : 0.000 KLD_attention : 6.712 
iteration : 145200 loss : 90.485 NLL : -84.426 KLD : 0.000 KLD_attention : 6.059 
iteration : 145400 loss : 113.621 NLL : -107.193 KLD : 0.000 KLD_attention : 6.428 
iteration : 145600 loss : 82.656 NLL : -75.943 KLD : 0.000 KLD_attention : 6.712 
iteration : 145800 loss : 64.679 NLL : -58.620 KLD : 0.000 KLD_attention : 6.059 
iteration : 146000 loss : 122.364 NLL : -115.571 KLD : 0.000 KLD_attention : 6.793 
iteration : 146200 loss : 88.123 NLL : -81.931 KLD : 0.000 KLD_attention : 6.192 
iteration : 146400 loss : 108.403 NLL : -101.778 KLD : 0.000 KLD_attention : 6.625 
iteration : 146600 loss : 96.737 NLL : -90.025 KLD : 0.000 KLD_attention : 6.712 
iteration : 146800 loss : 109.684 NLL : -102.817 KLD : 0.000 KLD_attention : 6.867 
iteration : 147000 loss : 102.341 NLL : -95.629 KLD : 0.000 KLD_attention : 6.712 
iteration : 147200 loss : 85.760 NLL : -79.229 KLD : 0.000 KLD_attention : 6.531 
iteration : 147400 loss : 84.363 NLL : -77.935 KLD : 0.000 KLD_attention : 6.428 
iteration : 147600 loss : 121.712 NLL : -115.000 KLD : 0.000 KLD_attention : 6.712 
iteration : 147800 loss : 95.787 NLL : -89.596 KLD : 0.000 KLD_attention : 6.192 
iteration : 148000 loss : 66.413 NLL : -60.221 KLD : 0.000 KLD_attention : 6.192 
iteration : 148200 loss : 100.226 NLL : -93.433 KLD : 0.000 KLD_attention : 6.793 
iteration : 148400 loss : 113.432 NLL : -107.117 KLD : 0.000 KLD_attention : 6.315 
iteration : 148600 loss : 65.784 NLL : -59.469 KLD : 0.000 KLD_attention : 6.315 
iteration : 148800 loss : 117.199 NLL : -110.574 KLD : 0.000 KLD_attention : 6.625 
iteration : 149000 loss : 106.703 NLL : -100.644 KLD : 0.000 KLD_attention : 6.060 
iteration : 149200 loss : 116.407 NLL : -110.348 KLD : 0.000 KLD_attention : 6.059 
iteration : 149400 loss : 68.877 NLL : -62.562 KLD : 0.000 KLD_attention : 6.315 
iteration : 149600 loss : 116.683 NLL : -109.971 KLD : 0.000 KLD_attention : 6.712 
iteration : 149800 loss : 106.170 NLL : -99.377 KLD : 0.000 KLD_attention : 6.793 
iteration : 150000 loss : 100.230 NLL : -93.293 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 107.599 updated ! and save the model! (step:150000) ----------
---------- Training loss 90.617 updated ! and save the model! (step:150006) ----------
---------- Training loss 87.737 updated ! and save the model! (step:150042) ----------
---------- Training loss 85.989 updated ! and save the model! (step:150108) ----------
iteration : 150200 loss : 96.912 NLL : -90.484 KLD : 0.000 KLD_attention : 6.428 
---------- Training loss 85.038 updated ! and save the model! (step:150318) ----------
---------- Training loss 79.114 updated ! and save the model! (step:150396) ----------
iteration : 150400 loss : 90.149 NLL : -83.356 KLD : 0.000 KLD_attention : 6.793 
iteration : 150600 loss : 106.776 NLL : -100.845 KLD : 0.000 KLD_attention : 5.930 
iteration : 150800 loss : 51.343 NLL : -45.283 KLD : 0.000 KLD_attention : 6.060 
---------- Training loss 74.203 updated ! and save the model! (step:150918) ----------
iteration : 151000 loss : 103.879 NLL : -97.819 KLD : 0.000 KLD_attention : 6.059 
iteration : 151200 loss : 87.652 NLL : -81.337 KLD : 0.000 KLD_attention : 6.315 
iteration : 151400 loss : 93.148 NLL : -86.523 KLD : 0.000 KLD_attention : 6.625 
iteration : 151600 loss : 102.462 NLL : -95.669 KLD : 0.000 KLD_attention : 6.793 
---------- Training loss 72.577 updated ! and save the model! (step:151668) ----------
iteration : 151800 loss : 112.164 NLL : -105.973 KLD : 0.000 KLD_attention : 6.192 
iteration : 152000 loss : 87.565 NLL : -80.772 KLD : 0.000 KLD_attention : 6.793 
---------- Training loss 70.746 updated ! and save the model! (step:152160) ----------
iteration : 152200 loss : 80.948 NLL : -74.757 KLD : 0.000 KLD_attention : 6.192 
iteration : 152400 loss : 64.551 NLL : -58.492 KLD : 0.000 KLD_attention : 6.059 
iteration : 152600 loss : 97.345 NLL : -90.633 KLD : 0.000 KLD_attention : 6.712 
iteration : 152800 loss : 116.902 NLL : -110.277 KLD : 0.000 KLD_attention : 6.625 
iteration : 153000 loss : 114.971 NLL : -108.104 KLD : 0.000 KLD_attention : 6.867 
iteration : 153200 loss : 111.199 NLL : -104.573 KLD : 0.000 KLD_attention : 6.626 
iteration : 153400 loss : 79.877 NLL : -73.939 KLD : 0.000 KLD_attention : 5.938 
iteration : 153600 loss : 96.439 NLL : -89.572 KLD : 0.000 KLD_attention : 6.867 
iteration : 153800 loss : 47.582 NLL : -41.652 KLD : 0.000 KLD_attention : 5.930 
iteration : 154000 loss : 92.330 NLL : -85.537 KLD : 0.000 KLD_attention : 6.793 
iteration : 154200 loss : 92.376 NLL : -86.446 KLD : 0.000 KLD_attention : 5.931 
iteration : 154400 loss : 123.037 NLL : -116.506 KLD : 0.000 KLD_attention : 6.531 
iteration : 154600 loss : 107.538 NLL : -100.826 KLD : 0.000 KLD_attention : 6.712 
iteration : 154800 loss : 113.476 NLL : -106.608 KLD : 0.000 KLD_attention : 6.867 
iteration : 155000 loss : 87.185 NLL : -80.393 KLD : 0.000 KLD_attention : 6.793 
iteration : 155200 loss : 119.717 NLL : -112.780 KLD : 0.000 KLD_attention : 6.937 
iteration : 155400 loss : 124.316 NLL : -117.449 KLD : 0.000 KLD_attention : 6.867 
iteration : 155600 loss : 72.857 NLL : -66.927 KLD : 0.000 KLD_attention : 5.930 
iteration : 155800 loss : 59.589 NLL : -53.659 KLD : 0.000 KLD_attention : 5.930 
iteration : 156000 loss : 41.828 NLL : -35.898 KLD : 0.000 KLD_attention : 5.930 
iteration : 156200 loss : 91.171 NLL : -84.546 KLD : 0.000 KLD_attention : 6.625 
iteration : 156400 loss : 112.619 NLL : -105.752 KLD : 0.000 KLD_attention : 6.867 
iteration : 156600 loss : 62.530 NLL : -56.215 KLD : 0.000 KLD_attention : 6.315 
iteration : 156800 loss : 121.020 NLL : -114.704 KLD : 0.000 KLD_attention : 6.316 
iteration : 157000 loss : 112.318 NLL : -105.381 KLD : 0.000 KLD_attention : 6.937 
iteration : 157200 loss : 76.335 NLL : -70.021 KLD : 0.000 KLD_attention : 6.315 
iteration : 157400 loss : 100.004 NLL : -93.292 KLD : 0.000 KLD_attention : 6.712 
iteration : 157600 loss : 112.878 NLL : -105.940 KLD : 0.000 KLD_attention : 6.938 
iteration : 157800 loss : 83.315 NLL : -76.602 KLD : 0.000 KLD_attention : 6.712 
iteration : 158000 loss : 68.998 NLL : -63.068 KLD : 0.000 KLD_attention : 5.930 
iteration : 158200 loss : 109.706 NLL : -103.776 KLD : 0.000 KLD_attention : 5.930 
iteration : 158400 loss : 105.568 NLL : -99.036 KLD : 0.000 KLD_attention : 6.532 
iteration : 158600 loss : 121.120 NLL : -114.408 KLD : 0.000 KLD_attention : 6.712 
iteration : 158800 loss : 101.492 NLL : -94.866 KLD : 0.000 KLD_attention : 6.625 
iteration : 159000 loss : 116.268 NLL : -110.076 KLD : 0.000 KLD_attention : 6.192 
iteration : 159200 loss : 111.153 NLL : -105.223 KLD : 0.000 KLD_attention : 5.930 
iteration : 159400 loss : 106.907 NLL : -100.282 KLD : 0.000 KLD_attention : 6.625 
iteration : 159600 loss : 89.350 NLL : -82.922 KLD : 0.000 KLD_attention : 6.428 
iteration : 159800 loss : 115.629 NLL : -109.098 KLD : 0.000 KLD_attention : 6.531 
iteration : 160000 loss : 68.966 NLL : -62.435 KLD : 0.000 KLD_attention : 6.531 
iteration : 160200 loss : 114.393 NLL : -108.463 KLD : 0.000 KLD_attention : 5.930 
iteration : 160400 loss : 115.115 NLL : -108.187 KLD : 0.000 KLD_attention : 6.928 
iteration : 160600 loss : 115.793 NLL : -109.081 KLD : 0.000 KLD_attention : 6.712 
iteration : 160800 loss : 86.550 NLL : -80.019 KLD : 0.000 KLD_attention : 6.531 
iteration : 161000 loss : 110.451 NLL : -103.584 KLD : 0.000 KLD_attention : 6.867 
iteration : 161200 loss : 114.556 NLL : -108.364 KLD : 0.000 KLD_attention : 6.192 
iteration : 161400 loss : 112.052 NLL : -105.115 KLD : 0.000 KLD_attention : 6.937 
---------- Training loss 65.649 updated ! and save the model! (step:161436) ----------
iteration : 161600 loss : 94.499 NLL : -87.787 KLD : 0.000 KLD_attention : 6.712 
iteration : 161800 loss : 89.877 NLL : -83.947 KLD : 0.000 KLD_attention : 5.930 
iteration : 162000 loss : 116.066 NLL : -109.242 KLD : 0.000 KLD_attention : 6.824 
iteration : 162200 loss : 60.412 NLL : -54.221 KLD : 0.000 KLD_attention : 6.192 
iteration : 162400 loss : 68.093 NLL : -62.162 KLD : 0.000 KLD_attention : 5.930 
iteration : 162600 loss : 97.012 NLL : -90.403 KLD : 0.000 KLD_attention : 6.608 
iteration : 162800 loss : 100.835 NLL : -93.899 KLD : 0.000 KLD_attention : 6.937 
iteration : 163000 loss : 119.396 NLL : -112.528 KLD : 0.000 KLD_attention : 6.867 
iteration : 163200 loss : 117.132 NLL : -110.261 KLD : 0.000 KLD_attention : 6.871 
iteration : 163400 loss : 93.288 NLL : -86.576 KLD : 0.000 KLD_attention : 6.712 
iteration : 163600 loss : 72.726 NLL : -66.410 KLD : 0.000 KLD_attention : 6.316 
iteration : 163800 loss : 45.782 NLL : -39.722 KLD : 0.000 KLD_attention : 6.059 
iteration : 164000 loss : 108.359 NLL : -101.828 KLD : 0.000 KLD_attention : 6.531 
iteration : 164200 loss : 133.864 NLL : -127.238 KLD : 0.000 KLD_attention : 6.626 
iteration : 164400 loss : 86.449 NLL : -80.185 KLD : 0.000 KLD_attention : 6.264 
iteration : 164600 loss : 119.153 NLL : -112.962 KLD : 0.000 KLD_attention : 6.192 
iteration : 164800 loss : 112.788 NLL : -106.852 KLD : 0.000 KLD_attention : 5.936 
iteration : 165000 loss : 121.483 NLL : -114.616 KLD : 0.000 KLD_attention : 6.867 
iteration : 165200 loss : 108.417 NLL : -101.624 KLD : 0.000 KLD_attention : 6.793 
iteration : 165400 loss : 91.365 NLL : -84.644 KLD : 0.000 KLD_attention : 6.721 
iteration : 165600 loss : 76.090 NLL : -70.030 KLD : 0.000 KLD_attention : 6.060 
iteration : 165800 loss : 86.810 NLL : -80.098 KLD : 0.000 KLD_attention : 6.712 
iteration : 166000 loss : 104.098 NLL : -97.229 KLD : 0.000 KLD_attention : 6.869 
iteration : 166200 loss : 42.828 NLL : -36.774 KLD : 0.000 KLD_attention : 6.055 
iteration : 166400 loss : 92.431 NLL : -85.638 KLD : 0.000 KLD_attention : 6.793 
iteration : 166600 loss : 50.166 NLL : -44.107 KLD : 0.000 KLD_attention : 6.059 
iteration : 166800 loss : 87.467 NLL : -80.937 KLD : 0.000 KLD_attention : 6.531 
iteration : 167000 loss : 99.206 NLL : -92.675 KLD : 0.000 KLD_attention : 6.531 
iteration : 167200 loss : 69.993 NLL : -63.461 KLD : 0.000 KLD_attention : 6.532 
iteration : 167400 loss : 97.072 NLL : -90.356 KLD : 0.000 KLD_attention : 6.716 
iteration : 167600 loss : 101.865 NLL : -95.239 KLD : 0.000 KLD_attention : 6.625 
iteration : 167800 loss : 110.851 NLL : -104.867 KLD : 0.000 KLD_attention : 5.984 
iteration : 168000 loss : 90.750 NLL : -84.690 KLD : 0.000 KLD_attention : 6.059 
iteration : 168200 loss : 72.750 NLL : -66.322 KLD : 0.000 KLD_attention : 6.428 
iteration : 168400 loss : 87.616 NLL : -81.424 KLD : 0.000 KLD_attention : 6.192 
iteration : 168600 loss : 110.330 NLL : -103.463 KLD : 0.000 KLD_attention : 6.867 
iteration : 168800 loss : 76.316 NLL : -69.785 KLD : 0.000 KLD_attention : 6.531 
iteration : 169000 loss : 79.099 NLL : -72.903 KLD : 0.000 KLD_attention : 6.196 
iteration : 169200 loss : 120.004 NLL : -113.379 KLD : 0.000 KLD_attention : 6.625 
iteration : 169400 loss : 119.727 NLL : -113.668 KLD : 0.000 KLD_attention : 6.060 
iteration : 169600 loss : 59.769 NLL : -53.452 KLD : 0.000 KLD_attention : 6.317 
iteration : 169800 loss : 69.985 NLL : -63.590 KLD : 0.000 KLD_attention : 6.394 
iteration : 170000 loss : 113.186 NLL : -106.319 KLD : 0.000 KLD_attention : 6.867 
iteration : 170200 loss : 90.010 NLL : -83.672 KLD : 0.000 KLD_attention : 6.339 
iteration : 170400 loss : 104.006 NLL : -98.075 KLD : 0.000 KLD_attention : 5.931 
iteration : 170600 loss : 90.896 NLL : -84.010 KLD : 0.000 KLD_attention : 6.886 
iteration : 170800 loss : 80.240 NLL : -74.303 KLD : 0.000 KLD_attention : 5.937 
iteration : 171000 loss : 87.481 NLL : -80.682 KLD : 0.000 KLD_attention : 6.798 
iteration : 171200 loss : 105.347 NLL : -98.635 KLD : 0.000 KLD_attention : 6.712 
iteration : 171400 loss : 117.819 NLL : -111.026 KLD : 0.000 KLD_attention : 6.793 
iteration : 171600 loss : 108.322 NLL : -102.131 KLD : 0.000 KLD_attention : 6.192 
iteration : 171800 loss : 115.060 NLL : -108.122 KLD : 0.000 KLD_attention : 6.938 
iteration : 172000 loss : 76.074 NLL : -69.434 KLD : 0.000 KLD_attention : 6.640 
iteration : 172200 loss : 108.531 NLL : -102.216 KLD : 0.000 KLD_attention : 6.315 
iteration : 172400 loss : 52.832 NLL : -46.902 KLD : 0.000 KLD_attention : 5.930 
iteration : 172600 loss : 91.787 NLL : -85.161 KLD : 0.000 KLD_attention : 6.625 
iteration : 172800 loss : 89.193 NLL : -83.001 KLD : 0.000 KLD_attention : 6.192 
iteration : 173000 loss : 84.354 NLL : -77.723 KLD : 0.000 KLD_attention : 6.632 
iteration : 173200 loss : 100.066 NLL : -93.198 KLD : 0.000 KLD_attention : 6.868 
iteration : 173400 loss : 91.927 NLL : -85.395 KLD : 0.000 KLD_attention : 6.532 
iteration : 173600 loss : 112.641 NLL : -106.213 KLD : 0.000 KLD_attention : 6.428 
iteration : 173800 loss : 86.699 NLL : -79.885 KLD : 0.000 KLD_attention : 6.814 
iteration : 174000 loss : 88.424 NLL : -81.709 KLD : 0.000 KLD_attention : 6.715 
iteration : 174200 loss : 96.302 NLL : -89.761 KLD : 0.000 KLD_attention : 6.540 
iteration : 174400 loss : 86.858 NLL : -79.929 KLD : 0.000 KLD_attention : 6.928 
iteration : 174600 loss : 107.001 NLL : -100.375 KLD : 0.000 KLD_attention : 6.626 
iteration : 174800 loss : 96.470 NLL : -89.600 KLD : 0.000 KLD_attention : 6.870 
iteration : 175000 loss : 47.271 NLL : -41.211 KLD : 0.000 KLD_attention : 6.060 
iteration : 175200 loss : 103.827 NLL : -96.959 KLD : 0.000 KLD_attention : 6.868 
iteration : 175400 loss : 127.030 NLL : -120.838 KLD : 0.000 KLD_attention : 6.192 
iteration : 175600 loss : 112.999 NLL : -106.129 KLD : 0.000 KLD_attention : 6.870 
iteration : 175800 loss : 100.782 NLL : -93.846 KLD : 0.000 KLD_attention : 6.937 
iteration : 176000 loss : 82.839 NLL : -76.214 KLD : 0.000 KLD_attention : 6.625 
iteration : 176200 loss : 86.886 NLL : -80.174 KLD : 0.000 KLD_attention : 6.712 
iteration : 176400 loss : 60.108 NLL : -54.178 KLD : 0.000 KLD_attention : 5.930 
iteration : 176600 loss : 78.053 NLL : -71.986 KLD : 0.000 KLD_attention : 6.067 
iteration : 176800 loss : 108.551 NLL : -102.013 KLD : 0.000 KLD_attention : 6.538 
iteration : 177000 loss : 113.593 NLL : -107.663 KLD : 0.000 KLD_attention : 5.930 
iteration : 177200 loss : 112.954 NLL : -106.203 KLD : 0.000 KLD_attention : 6.751 
iteration : 177400 loss : 76.584 NLL : -70.156 KLD : 0.000 KLD_attention : 6.428 
iteration : 177600 loss : 101.588 NLL : -94.962 KLD : 0.000 KLD_attention : 6.626 
iteration : 177800 loss : 94.078 NLL : -87.512 KLD : 0.000 KLD_attention : 6.566 
iteration : 178000 loss : 119.688 NLL : -112.821 KLD : 0.000 KLD_attention : 6.867 
iteration : 178200 loss : 54.306 NLL : -48.098 KLD : 0.000 KLD_attention : 6.207 
iteration : 178400 loss : 92.409 NLL : -85.897 KLD : 0.000 KLD_attention : 6.512 
iteration : 178600 loss : 59.201 NLL : -53.010 KLD : 0.000 KLD_attention : 6.192 
iteration : 178800 loss : 107.825 NLL : -101.004 KLD : 0.000 KLD_attention : 6.822 
iteration : 179000 loss : 75.248 NLL : -68.972 KLD : 0.000 KLD_attention : 6.276 
iteration : 179200 loss : 65.875 NLL : -59.560 KLD : 0.000 KLD_attention : 6.315 
iteration : 179400 loss : 121.206 NLL : -114.268 KLD : 0.000 KLD_attention : 6.938 
iteration : 179600 loss : 94.566 NLL : -87.699 KLD : 0.000 KLD_attention : 6.867 
iteration : 179800 loss : 115.896 NLL : -108.960 KLD : 0.000 KLD_attention : 6.937 
iteration : 180000 loss : 106.694 NLL : -99.881 KLD : 0.000 KLD_attention : 6.814 
---------- Training loss 94.608 updated ! and save the model! (step:180000) ----------
---------- Training loss 78.113 updated ! and save the model! (step:180006) ----------
---------- Training loss 76.787 updated ! and save the model! (step:180162) ----------
iteration : 180200 loss : 84.112 NLL : -77.683 KLD : 0.000 KLD_attention : 6.429 
iteration : 180400 loss : 123.068 NLL : -115.294 KLD : 0.000 KLD_attention : 7.773 
iteration : 180600 loss : 106.099 NLL : -99.162 KLD : 0.000 KLD_attention : 6.937 
iteration : 180800 loss : 76.110 NLL : -70.180 KLD : 0.000 KLD_attention : 5.930 
iteration : 181000 loss : 121.782 NLL : -114.915 KLD : 0.000 KLD_attention : 6.867 
iteration : 181200 loss : 106.777 NLL : -99.575 KLD : 0.000 KLD_attention : 7.202 
iteration : 181400 loss : 118.948 NLL : -112.417 KLD : 0.000 KLD_attention : 6.531 
iteration : 181600 loss : 116.801 NLL : -109.799 KLD : 0.000 KLD_attention : 7.002 
---------- Training loss 74.994 updated ! and save the model! (step:181746) ----------
iteration : 181800 loss : 99.755 NLL : -92.845 KLD : 0.000 KLD_attention : 6.911 
iteration : 182000 loss : 115.415 NLL : -108.702 KLD : 0.000 KLD_attention : 6.712 
iteration : 182200 loss : 64.582 NLL : -58.154 KLD : 0.000 KLD_attention : 6.428 
---------- Training loss 67.972 updated ! and save the model! (step:182352) ----------
iteration : 182400 loss : 114.632 NLL : -108.021 KLD : 0.000 KLD_attention : 6.611 
iteration : 182600 loss : 125.488 NLL : -118.695 KLD : 0.000 KLD_attention : 6.793 
iteration : 182800 loss : 72.792 NLL : -66.599 KLD : 0.000 KLD_attention : 6.193 
---------- Training loss 62.895 updated ! and save the model! (step:182904) ----------
iteration : 183000 loss : 105.972 NLL : -99.649 KLD : 0.000 KLD_attention : 6.323 
iteration : 183200 loss : 79.084 NLL : -72.769 KLD : 0.000 KLD_attention : 6.315 
iteration : 183400 loss : 121.773 NLL : -115.459 KLD : 0.000 KLD_attention : 6.315  /home/mgyukim/workspaces/AI701/recommend_sys/models/parts/attention.py:668: UserWarning:Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.

iteration : 183600 loss : 97.028 NLL : -90.691 KLD : 0.000 KLD_attention : 6.337 
iteration : 183800 loss : 86.256 NLL : -80.326 KLD : 0.000 KLD_attention : 5.930 
iteration : 184000 loss : 76.420 NLL : -69.545 KLD : 0.000 KLD_attention : 6.876 
iteration : 184200 loss : 104.296 NLL : -97.583 KLD : 0.000 KLD_attention : 6.712 
iteration : 184400 loss : 116.114 NLL : -109.487 KLD : 0.000 KLD_attention : 6.626 
iteration : 184600 loss : 67.367 NLL : -59.640 KLD : 0.000 KLD_attention : 7.727 
iteration : 184800 loss : 123.671 NLL : -116.457 KLD : 0.000 KLD_attention : 7.214 
iteration : 185000 loss : 77.274 NLL : -70.081 KLD : 0.000 KLD_attention : 7.193 
iteration : 185200 loss : 91.707 NLL : -83.427 KLD : 0.000 KLD_attention : 8.280 
iteration : 185400 loss : 61.423 NLL : -54.040 KLD : 0.000 KLD_attention : 7.383 
iteration : 185600 loss : 96.154 NLL : -88.846 KLD : 0.000 KLD_attention : 7.308 
iteration : 185800 loss : 101.353 NLL : -93.995 KLD : 0.000 KLD_attention : 7.357 
iteration : 186000 loss : 106.080 NLL : -98.858 KLD : 0.000 KLD_attention : 7.222 
iteration : 186200 loss : 99.882 NLL : -92.629 KLD : 0.000 KLD_attention : 7.253 
iteration : 186400 loss : 106.814 NLL : -98.842 KLD : 0.000 KLD_attention : 7.972 
iteration : 186600 loss : 104.648 NLL : -97.449 KLD : 0.000 KLD_attention : 7.199 
iteration : 186800 loss : 99.402 NLL : -91.402 KLD : 0.000 KLD_attention : 7.999 
iteration : 187000 loss : 111.900 NLL : -104.127 KLD : 0.000 KLD_attention : 7.773 
iteration : 187200 loss : 75.042 NLL : -67.129 KLD : 0.000 KLD_attention : 7.913 
iteration : 187400 loss : 83.059 NLL : -73.593 KLD : 0.000 KLD_attention : 9.465 
iteration : 187600 loss : 101.986 NLL : -94.023 KLD : 0.000 KLD_attention : 7.963 
iteration : 187800 loss : 90.000 NLL : -82.196 KLD : 0.000 KLD_attention : 7.804 
iteration : 188000 loss : 108.552 NLL : -100.801 KLD : 0.000 KLD_attention : 7.751 
iteration : 188200 loss : 99.464 NLL : -91.711 KLD : 0.000 KLD_attention : 7.753 
iteration : 188400 loss : 95.262 NLL : -87.390 KLD : 0.000 KLD_attention : 7.871 
iteration : 188600 loss : 85.240 NLL : -77.008 KLD : 0.000 KLD_attention : 8.232 
iteration : 188800 loss : 69.341 NLL : -60.713 KLD : 0.000 KLD_attention : 8.627 
iteration : 189000 loss : 108.289 NLL : -100.430 KLD : 0.000 KLD_attention : 7.859 
iteration : 189200 loss : 96.813 NLL : -88.822 KLD : 0.000 KLD_attention : 7.991 
iteration : 189400 loss : 101.214 NLL : -93.461 KLD : 0.000 KLD_attention : 7.753 
iteration : 189600 loss : 86.121 NLL : -77.491 KLD : 0.000 KLD_attention : 8.630 
iteration : 189800 loss : 114.522 NLL : -106.359 KLD : 0.000 KLD_attention : 8.163 
iteration : 190000 loss : 108.240 NLL : -100.367 KLD : 0.000 KLD_attention : 7.873 
iteration : 190200 loss : 85.285 NLL : -76.424 KLD : 0.000 KLD_attention : 8.861 
iteration : 190400 loss : 117.611 NLL : -109.738 KLD : 0.000 KLD_attention : 7.873 
iteration : 190600 loss : 103.435 NLL : -95.654 KLD : 0.000 KLD_attention : 7.781 
iteration : 190800 loss : 107.495 NLL : -99.741 KLD : 0.000 KLD_attention : 7.754 
iteration : 191000 loss : 101.131 NLL : -91.362 KLD : 0.000 KLD_attention : 9.769 
iteration : 191200 loss : 86.837 NLL : -78.126 KLD : 0.000 KLD_attention : 8.711 
iteration : 191400 loss : 69.044 NLL : -60.944 KLD : 0.000 KLD_attention : 8.100 
iteration : 191600 loss : 113.474 NLL : -105.717 KLD : 0.000 KLD_attention : 7.757 
iteration : 191800 loss : 119.441 NLL : -111.349 KLD : 0.000 KLD_attention : 8.091 
iteration : 192000 loss : 85.227 NLL : -76.513 KLD : 0.000 KLD_attention : 8.713 
iteration : 192200 loss : 50.446 NLL : -40.975 KLD : 0.000 KLD_attention : 9.470 
iteration : 192400 loss : 118.183 NLL : -110.421 KLD : 0.000 KLD_attention : 7.762 
iteration : 192600 loss : 118.905 NLL : -111.137 KLD : 0.000 KLD_attention : 7.767 
iteration : 192800 loss : 64.800 NLL : -55.333 KLD : 0.000 KLD_attention : 9.467 
iteration : 193000 loss : 85.731 NLL : -77.596 KLD : 0.000 KLD_attention : 8.135 
iteration : 193200 loss : 66.523 NLL : -58.211 KLD : 0.000 KLD_attention : 8.313 
iteration : 193400 loss : 77.750 NLL : -69.761 KLD : 0.000 KLD_attention : 7.989 
iteration : 193600 loss : 92.878 NLL : -85.098 KLD : 0.000 KLD_attention : 7.780 
iteration : 193800 loss : 83.542 NLL : -75.529 KLD : 0.000 KLD_attention : 8.014 
iteration : 194000 loss : 109.318 NLL : -101.538 KLD : 0.000 KLD_attention : 7.781 
iteration : 194200 loss : 105.935 NLL : -98.067 KLD : 0.000 KLD_attention : 7.868 
iteration : 194400 loss : 81.822 NLL : -73.150 KLD : 0.000 KLD_attention : 8.672 
iteration : 194600 loss : 87.913 NLL : -79.949 KLD : 0.000 KLD_attention : 7.964 
iteration : 194800 loss : 89.865 NLL : -81.215 KLD : 0.000 KLD_attention : 8.651 
iteration : 195000 loss : 119.862 NLL : -111.504 KLD : 0.000 KLD_attention : 8.358 
iteration : 195200 loss : 100.558 NLL : -92.374 KLD : 0.000 KLD_attention : 8.185 
iteration : 195400 loss : 96.862 NLL : -89.089 KLD : 0.000 KLD_attention : 7.773 
iteration : 195600 loss : 113.136 NLL : -105.280 KLD : 0.000 KLD_attention : 7.855 
iteration : 195800 loss : 123.826 NLL : -116.059 KLD : 0.000 KLD_attention : 7.766 
iteration : 196000 loss : 100.721 NLL : -92.713 KLD : 0.000 KLD_attention : 8.008 
iteration : 196200 loss : 75.053 NLL : -66.726 KLD : 0.000 KLD_attention : 8.326 
iteration : 196400 loss : 116.331 NLL : -108.577 KLD : 0.000 KLD_attention : 7.754 
iteration : 196600 loss : 117.319 NLL : -109.559 KLD : 0.000 KLD_attention : 7.760 
iteration : 196800 loss : 115.627 NLL : -107.709 KLD : 0.000 KLD_attention : 7.918 
iteration : 197000 loss : 95.214 NLL : -87.308 KLD : 0.000 KLD_attention : 7.905 
iteration : 197200 loss : 86.188 NLL : -76.718 KLD : 0.000 KLD_attention : 9.470 
iteration : 197400 loss : 124.100 NLL : -116.231 KLD : 0.000 KLD_attention : 7.869 
iteration : 197600 loss : 115.061 NLL : -107.166 KLD : 0.000 KLD_attention : 7.894 
iteration : 197800 loss : 77.718 NLL : -69.703 KLD : 0.000 KLD_attention : 8.016 
iteration : 198000 loss : 85.549 NLL : -76.083 KLD : 0.000 KLD_attention : 9.466 
iteration : 198200 loss : 89.602 NLL : -81.579 KLD : 0.000 KLD_attention : 8.023 
iteration : 198400 loss : 77.384 NLL : -69.512 KLD : 0.000 KLD_attention : 7.873 
iteration : 198600 loss : 105.828 NLL : -97.882 KLD : 0.000 KLD_attention : 7.946 
iteration : 198800 loss : 120.388 NLL : -111.940 KLD : 0.000 KLD_attention : 8.448 
iteration : 199000 loss : 72.838 NLL : -64.095 KLD : 0.000 KLD_attention : 8.743 
iteration : 199200 loss : 104.674 NLL : -96.863 KLD : 0.000 KLD_attention : 7.811 
iteration : 199400 loss : 103.104 NLL : -95.193 KLD : 0.000 KLD_attention : 7.911 
iteration : 199600 loss : 80.289 NLL : -72.192 KLD : 0.000 KLD_attention : 8.097 
iteration : 199800 loss : 104.736 NLL : -96.640 KLD : 0.000 KLD_attention : 8.096 
iteration : 200000 loss : 119.912 NLL : -111.327 KLD : 0.000 KLD_attention : 8.585 
---------- Save the model! (step:None) ----------
