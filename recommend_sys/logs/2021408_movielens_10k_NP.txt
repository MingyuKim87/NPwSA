---------- Training loss 74241.163 updated ! and save the model! (step:6) ----------
---------- Training loss 6190.157 updated ! and save the model! (step:12) ----------
---------- Training loss 1137.977 updated ! and save the model! (step:18) ----------
---------- Training loss 484.512 updated ! and save the model! (step:24) ----------
---------- Training loss 247.482 updated ! and save the model! (step:30) ----------
---------- Training loss 155.057 updated ! and save the model! (step:36) ----------
---------- Training loss 130.697 updated ! and save the model! (step:54) ----------
---------- Training loss 102.527 updated ! and save the model! (step:60) ----------
---------- Training loss 97.785 updated ! and save the model! (step:84) ----------
---------- Training loss 85.621 updated ! and save the model! (step:126) ----------
---------- Training loss 84.961 updated ! and save the model! (step:132) ----------
---------- Training loss 84.923 updated ! and save the model! (step:144) ----------
---------- Training loss 79.229 updated ! and save the model! (step:168) ----------
---------- Training loss 69.884 updated ! and save the model! (step:192) ----------
---------- Training loss 63.142 updated ! and save the model! (step:198) ----------
iteration : 200 loss : 54.699 NLL : -53.874 KLD : 0.824 
---------- Training loss 55.482 updated ! and save the model! (step:258) ----------
---------- Training loss 45.459 updated ! and save the model! (step:366) ----------
iteration : 400 loss : 63.478 NLL : -62.223 KLD : 1.255 
---------- Training loss 44.758 updated ! and save the model! (step:456) ----------
---------- Training loss 43.404 updated ! and save the model! (step:498) ----------
iteration : 600 loss : 85.978 NLL : -85.143 KLD : 0.835 
iteration : 800 loss : 77.848 NLL : -77.329 KLD : 0.519 
---------- Training loss 42.548 updated ! and save the model! (step:858) ----------
---------- Training loss 41.267 updated ! and save the model! (step:888) ----------
iteration : 1000 loss : 72.410 NLL : -70.517 KLD : 1.893 
---------- Training loss 36.827 updated ! and save the model! (step:1116) ----------
iteration : 1200 loss : 56.723 NLL : -55.222 KLD : 1.501 
iteration : 1400 loss : 51.306 NLL : -49.668 KLD : 1.639 
iteration : 1600 loss : 62.295 NLL : -61.289 KLD : 1.006 
iteration : 1800 loss : 45.594 NLL : -42.078 KLD : 3.516 
---------- Training loss 35.407 updated ! and save the model! (step:1980) ----------
iteration : 2000 loss : 51.248 NLL : -50.760 KLD : 0.487 
iteration : 2200 loss : 107.097 NLL : -105.521 KLD : 1.576 
iteration : 2400 loss : 61.756 NLL : -60.720 KLD : 1.036 
iteration : 2600 loss : 113.063 NLL : -110.510 KLD : 2.552 
iteration : 2800 loss : 79.565 NLL : -72.611 KLD : 6.954 
iteration : 3000 loss : 62.017 NLL : -57.514 KLD : 4.504 
iteration : 3200 loss : 41.915 NLL : -41.026 KLD : 0.889 
iteration : 3400 loss : 96.139 NLL : -95.997 KLD : 0.142 
iteration : 3600 loss : 89.388 NLL : -88.880 KLD : 0.508 
iteration : 3800 loss : 53.455 NLL : -51.591 KLD : 1.864 
iteration : 4000 loss : 87.657 NLL : -86.826 KLD : 0.831 
iteration : 4200 loss : 79.494 NLL : -78.606 KLD : 0.887 
iteration : 4400 loss : 43.123 NLL : -42.573 KLD : 0.549 
iteration : 4600 loss : 59.274 NLL : -58.035 KLD : 1.239 
iteration : 4800 loss : 39.288 NLL : -35.392 KLD : 3.897 
iteration : 5000 loss : 47.370 NLL : -46.955 KLD : 0.415 
iteration : 5200 loss : 55.116 NLL : -54.680 KLD : 0.436 
iteration : 5400 loss : 44.991 NLL : -44.769 KLD : 0.222 
iteration : 5600 loss : 79.724 NLL : -79.224 KLD : 0.500 
iteration : 5800 loss : 39.475 NLL : -38.433 KLD : 1.042 
iteration : 6000 loss : 58.477 NLL : -58.156 KLD : 0.321 
iteration : 6200 loss : 46.410 NLL : -46.157 KLD : 0.253 
iteration : 6400 loss : 48.103 NLL : -47.190 KLD : 0.913 
iteration : 6600 loss : 58.026 NLL : -57.635 KLD : 0.392 
iteration : 6800 loss : 61.161 NLL : -60.949 KLD : 0.212 
iteration : 7000 loss : 71.294 NLL : -70.396 KLD : 0.899 
iteration : 7200 loss : 59.937 NLL : -58.861 KLD : 1.076 
iteration : 7400 loss : 39.052 NLL : -38.582 KLD : 0.470 
iteration : 7600 loss : 53.743 NLL : -53.103 KLD : 0.640 
iteration : 7800 loss : 55.934 NLL : -55.486 KLD : 0.448 
iteration : 8000 loss : 46.460 NLL : -46.255 KLD : 0.205 
iteration : 8200 loss : 26.666 NLL : -25.289 KLD : 1.376 
iteration : 8400 loss : 43.628 NLL : -39.554 KLD : 4.074 
iteration : 8600 loss : 40.280 NLL : -39.768 KLD : 0.512 
iteration : 8800 loss : 58.906 NLL : -58.681 KLD : 0.226 
iteration : 9000 loss : 43.990 NLL : -43.690 KLD : 0.300 
iteration : 9200 loss : 75.481 NLL : -74.335 KLD : 1.146 
iteration : 9400 loss : 60.778 NLL : -60.460 KLD : 0.318 
iteration : 9600 loss : 29.565 NLL : -28.041 KLD : 1.524 
iteration : 9800 loss : 45.527 NLL : -44.380 KLD : 1.147 
iteration : 10000 loss : 73.721 NLL : -73.395 KLD : 0.326 
iteration : 10200 loss : 41.632 NLL : -41.473 KLD : 0.159 
iteration : 10400 loss : 37.459 NLL : -37.267 KLD : 0.192 
iteration : 10600 loss : 59.799 NLL : -58.889 KLD : 0.911 
iteration : 10800 loss : 61.227 NLL : -60.174 KLD : 1.054 
iteration : 11000 loss : 45.069 NLL : -44.024 KLD : 1.045 
iteration : 11200 loss : 45.105 NLL : -44.605 KLD : 0.500 
iteration : 11400 loss : 56.623 NLL : -55.537 KLD : 1.086 
---------- Training loss 33.032 updated ! and save the model! (step:11568) ----------
iteration : 11600 loss : 35.724 NLL : -33.744 KLD : 1.980 
iteration : 11800 loss : 49.395 NLL : -46.418 KLD : 2.977 
iteration : 12000 loss : 57.622 NLL : -53.087 KLD : 4.535 
iteration : 12200 loss : 49.078 NLL : -46.720 KLD : 2.358 
iteration : 12400 loss : 58.433 NLL : -58.143 KLD : 0.290 
iteration : 12600 loss : 70.582 NLL : -69.341 KLD : 1.241 
iteration : 12800 loss : 54.481 NLL : -53.477 KLD : 1.004 
iteration : 13000 loss : 67.672 NLL : -67.451 KLD : 0.221 
iteration : 13200 loss : 66.197 NLL : -65.668 KLD : 0.528 
iteration : 13400 loss : 58.379 NLL : -58.178 KLD : 0.201 
iteration : 13600 loss : 51.714 NLL : -51.252 KLD : 0.462 
iteration : 13800 loss : 94.575 NLL : -89.892 KLD : 4.683 
iteration : 14000 loss : 54.442 NLL : -49.939 KLD : 4.503 
iteration : 14200 loss : 97.391 NLL : -90.696 KLD : 6.695 
iteration : 14400 loss : 26.632 NLL : -26.001 KLD : 0.631 
iteration : 14600 loss : 56.202 NLL : -55.923 KLD : 0.279 
iteration : 14800 loss : 57.647 NLL : -57.046 KLD : 0.601 
iteration : 15000 loss : 53.801 NLL : -52.729 KLD : 1.071 
iteration : 15200 loss : 41.998 NLL : -41.754 KLD : 0.244 
iteration : 15400 loss : 76.270 NLL : -75.671 KLD : 0.599 
iteration : 15600 loss : 39.130 NLL : -38.343 KLD : 0.787 
iteration : 15800 loss : 61.578 NLL : -61.228 KLD : 0.350 
iteration : 16000 loss : 36.454 NLL : -36.175 KLD : 0.278 
iteration : 16200 loss : 52.008 NLL : -50.854 KLD : 1.153 
iteration : 16400 loss : 53.826 NLL : -53.509 KLD : 0.317 
iteration : 16600 loss : 36.713 NLL : -35.578 KLD : 1.135 
iteration : 16800 loss : 31.125 NLL : -30.877 KLD : 0.248 
iteration : 17000 loss : 109.391 NLL : -101.527 KLD : 7.864 
iteration : 17200 loss : 63.937 NLL : -57.161 KLD : 6.776 
iteration : 17400 loss : 43.937 NLL : -42.480 KLD : 1.457 
iteration : 17600 loss : 57.893 NLL : -57.093 KLD : 0.800 
iteration : 17800 loss : 37.639 NLL : -36.421 KLD : 1.218 
iteration : 18000 loss : 72.943 NLL : -67.150 KLD : 5.794 
iteration : 18200 loss : 55.952 NLL : -55.692 KLD : 0.260 
iteration : 18400 loss : 54.276 NLL : -52.858 KLD : 1.419 
iteration : 18600 loss : 44.840 NLL : -43.205 KLD : 1.635 
iteration : 18800 loss : 33.702 NLL : -32.610 KLD : 1.093 
iteration : 19000 loss : 36.311 NLL : -35.797 KLD : 0.513 
iteration : 19200 loss : 51.453 NLL : -50.833 KLD : 0.620 
iteration : 19400 loss : 39.888 NLL : -39.537 KLD : 0.351 
iteration : 19600 loss : 68.496 NLL : -66.704 KLD : 1.792 
iteration : 19800 loss : 51.135 NLL : -50.763 KLD : 0.372 
iteration : 20000 loss : 46.243 NLL : -45.499 KLD : 0.744 
iteration : 20200 loss : 74.381 NLL : -74.104 KLD : 0.277 
iteration : 20400 loss : 19.479 NLL : -17.871 KLD : 1.608 
iteration : 20600 loss : 43.972 NLL : -43.363 KLD : 0.609 
iteration : 20800 loss : 56.350 NLL : -55.309 KLD : 1.041 
iteration : 21000 loss : 59.821 NLL : -59.590 KLD : 0.231 
iteration : 21200 loss : 44.757 NLL : -44.570 KLD : 0.187 
iteration : 21400 loss : 50.981 NLL : -50.278 KLD : 0.703 
iteration : 21600 loss : 45.812 NLL : -45.320 KLD : 0.492 
iteration : 21800 loss : 53.503 NLL : -53.097 KLD : 0.406 
iteration : 22000 loss : 67.395 NLL : -64.046 KLD : 3.349 
iteration : 22200 loss : 55.416 NLL : -55.205 KLD : 0.211 
iteration : 22400 loss : 65.704 NLL : -65.496 KLD : 0.208 
iteration : 22600 loss : 61.589 NLL : -61.499 KLD : 0.090 
iteration : 22800 loss : 22.538 NLL : -21.513 KLD : 1.025 
iteration : 23000 loss : 43.996 NLL : -41.857 KLD : 2.138 
iteration : 23200 loss : 41.755 NLL : -40.513 KLD : 1.242 
---------- Training loss 32.069 updated ! and save the model! (step:23232) ----------
iteration : 23400 loss : 36.671 NLL : -36.479 KLD : 0.192 
iteration : 23600 loss : 26.573 NLL : -25.946 KLD : 0.627 
iteration : 23800 loss : 56.589 NLL : -53.316 KLD : 3.273 
iteration : 24000 loss : 67.992 NLL : -67.713 KLD : 0.278 
iteration : 24200 loss : 47.876 NLL : -46.916 KLD : 0.960 
iteration : 24400 loss : 56.107 NLL : -55.895 KLD : 0.212 
iteration : 24600 loss : 46.944 NLL : -46.200 KLD : 0.743 
iteration : 24800 loss : 72.313 NLL : -71.913 KLD : 0.400 
iteration : 25000 loss : 62.498 NLL : -60.562 KLD : 1.936 
iteration : 25200 loss : 53.173 NLL : -52.360 KLD : 0.813 
iteration : 25400 loss : 57.990 NLL : -57.601 KLD : 0.390 
iteration : 25600 loss : 41.864 NLL : -40.308 KLD : 1.557 
iteration : 25800 loss : 56.522 NLL : -56.228 KLD : 0.294 
iteration : 26000 loss : 56.982 NLL : -56.560 KLD : 0.423 
iteration : 26200 loss : 31.436 NLL : -29.861 KLD : 1.575 
iteration : 26400 loss : 24.644 NLL : -23.829 KLD : 0.815 
iteration : 26600 loss : 45.102 NLL : -44.662 KLD : 0.440 
iteration : 26800 loss : 43.700 NLL : -43.349 KLD : 0.351 
iteration : 27000 loss : 41.940 NLL : -41.782 KLD : 0.158 
iteration : 27200 loss : 59.202 NLL : -58.872 KLD : 0.330 
iteration : 27400 loss : 57.186 NLL : -56.760 KLD : 0.426 
iteration : 27600 loss : 77.263 NLL : -76.869 KLD : 0.395 
iteration : 27800 loss : 53.650 NLL : -52.904 KLD : 0.746 
iteration : 28000 loss : 51.052 NLL : -50.732 KLD : 0.320 
iteration : 28200 loss : 30.725 NLL : -29.396 KLD : 1.328 
iteration : 28400 loss : 61.216 NLL : -60.266 KLD : 0.951 
iteration : 28600 loss : 57.425 NLL : -56.929 KLD : 0.496 
iteration : 28800 loss : 31.488 NLL : -29.740 KLD : 1.749 
iteration : 29000 loss : 51.651 NLL : -50.902 KLD : 0.749 
iteration : 29200 loss : 44.136 NLL : -43.312 KLD : 0.824 
iteration : 29400 loss : 49.168 NLL : -48.290 KLD : 0.878 
iteration : 29600 loss : 62.133 NLL : -61.061 KLD : 1.072 
iteration : 29800 loss : 52.033 NLL : -51.363 KLD : 0.670 
iteration : 30000 loss : 83.359 NLL : -80.959 KLD : 2.401 
---------- Training loss 55.010 updated ! and save the model! (step:30000) ----------
---------- Training loss 50.986 updated ! and save the model! (step:30006) ----------
---------- Training loss 45.756 updated ! and save the model! (step:30012) ----------
---------- Training loss 43.621 updated ! and save the model! (step:30030) ----------
---------- Training loss 38.367 updated ! and save the model! (step:30036) ----------
iteration : 30200 loss : 56.014 NLL : -55.539 KLD : 0.475 
iteration : 30400 loss : 60.457 NLL : -59.605 KLD : 0.852 
iteration : 30600 loss : 60.715 NLL : -60.284 KLD : 0.431 
iteration : 30800 loss : 55.512 NLL : -55.056 KLD : 0.456 
iteration : 31000 loss : 51.368 NLL : -49.515 KLD : 1.852 
---------- Training loss 37.359 updated ! and save the model! (step:31086) ----------
iteration : 31200 loss : 63.756 NLL : -63.190 KLD : 0.566 
---------- Training loss 33.836 updated ! and save the model! (step:31296) ----------
iteration : 31400 loss : 61.239 NLL : -61.109 KLD : 0.130 
iteration : 31600 loss : 51.044 NLL : -50.766 KLD : 0.277 
iteration : 31800 loss : 37.011 NLL : -36.789 KLD : 0.222 
iteration : 32000 loss : 65.177 NLL : -64.977 KLD : 0.200 
---------- Training loss 33.562 updated ! and save the model! (step:32082) ----------
iteration : 32200 loss : 63.667 NLL : -63.207 KLD : 0.460 
iteration : 32400 loss : 35.360 NLL : -35.013 KLD : 0.347 
iteration : 32600 loss : 59.312 NLL : -58.667 KLD : 0.646 
iteration : 32800 loss : 63.529 NLL : -63.047 KLD : 0.482 
iteration : 33000 loss : 54.343 NLL : -51.379 KLD : 2.965 
iteration : 33200 loss : 77.126 NLL : -76.885 KLD : 0.241 
iteration : 33400 loss : 48.697 NLL : -48.427 KLD : 0.269 
iteration : 33600 loss : 43.955 NLL : -43.612 KLD : 0.343 
iteration : 33800 loss : 60.262 NLL : -59.231 KLD : 1.031 
iteration : 34000 loss : 43.510 NLL : -41.772 KLD : 1.739 
iteration : 34200 loss : 26.351 NLL : -25.843 KLD : 0.508 
iteration : 34400 loss : 56.470 NLL : -55.299 KLD : 1.170 
iteration : 34600 loss : 39.924 NLL : -39.575 KLD : 0.349 
iteration : 34800 loss : 55.076 NLL : -53.435 KLD : 1.641 
iteration : 35000 loss : 47.712 NLL : -46.236 KLD : 1.476 
iteration : 35200 loss : 53.821 NLL : -53.511 KLD : 0.310 
iteration : 35400 loss : 51.001 NLL : -50.806 KLD : 0.195 
iteration : 35600 loss : 45.584 NLL : -44.635 KLD : 0.949 
iteration : 35800 loss : 44.235 NLL : -44.129 KLD : 0.106 
iteration : 36000 loss : 52.607 NLL : -52.478 KLD : 0.129 
iteration : 36200 loss : 53.650 NLL : -51.662 KLD : 1.988 
---------- Training loss 33.321 updated ! and save the model! (step:36282) ----------
iteration : 36400 loss : 33.592 NLL : -32.654 KLD : 0.938 
iteration : 36600 loss : 40.309 NLL : -38.717 KLD : 1.592 
iteration : 36800 loss : 53.428 NLL : -52.048 KLD : 1.380 
iteration : 37000 loss : 74.643 NLL : -71.042 KLD : 3.601 
---------- Training loss 33.258 updated ! and save the model! (step:37110) ----------
iteration : 37200 loss : 45.686 NLL : -45.266 KLD : 0.420 
iteration : 37400 loss : 53.559 NLL : -52.868 KLD : 0.691 
iteration : 37600 loss : 76.671 NLL : -76.022 KLD : 0.649 
iteration : 37800 loss : 50.478 NLL : -50.150 KLD : 0.328 
iteration : 38000 loss : 59.133 NLL : -56.410 KLD : 2.723 
iteration : 38200 loss : 37.406 NLL : -36.406 KLD : 1.000 
iteration : 38400 loss : 67.790 NLL : -67.558 KLD : 0.232 
iteration : 38600 loss : 42.855 NLL : -41.514 KLD : 1.341 
iteration : 38800 loss : 39.916 NLL : -39.668 KLD : 0.247 
iteration : 39000 loss : 41.790 NLL : -41.263 KLD : 0.528 
iteration : 39200 loss : 48.988 NLL : -48.648 KLD : 0.340 
iteration : 39400 loss : 77.100 NLL : -76.635 KLD : 0.465 
iteration : 39600 loss : 57.685 NLL : -57.481 KLD : 0.204 
iteration : 39800 loss : 40.106 NLL : -39.437 KLD : 0.669 
iteration : 40000 loss : 53.936 NLL : -53.760 KLD : 0.176 
iteration : 40200 loss : 40.184 NLL : -39.556 KLD : 0.627 
iteration : 40400 loss : 66.065 NLL : -65.848 KLD : 0.217 
iteration : 40600 loss : 56.535 NLL : -53.562 KLD : 2.973 
---------- Training loss 32.510 updated ! and save the model! (step:40620) ----------
iteration : 40800 loss : 57.426 NLL : -56.730 KLD : 0.696 
iteration : 41000 loss : 53.128 NLL : -52.203 KLD : 0.925 
iteration : 41200 loss : 44.771 NLL : -44.313 KLD : 0.458 
iteration : 41400 loss : 36.098 NLL : -35.475 KLD : 0.623 
iteration : 41600 loss : 69.417 NLL : -68.962 KLD : 0.455 
iteration : 41800 loss : 54.345 NLL : -52.399 KLD : 1.946 
iteration : 42000 loss : 44.479 NLL : -44.165 KLD : 0.314 
iteration : 42200 loss : 60.529 NLL : -58.970 KLD : 1.560 
iteration : 42400 loss : 39.492 NLL : -38.871 KLD : 0.621 
iteration : 42600 loss : 50.063 NLL : -49.836 KLD : 0.227 
iteration : 42800 loss : 48.316 NLL : -47.816 KLD : 0.500 
iteration : 43000 loss : 38.421 NLL : -38.051 KLD : 0.370 
iteration : 43200 loss : 47.587 NLL : -47.347 KLD : 0.240 
iteration : 43400 loss : 20.847 NLL : -18.616 KLD : 2.231 
iteration : 43600 loss : 79.720 NLL : -77.829 KLD : 1.891 
iteration : 43800 loss : 38.157 NLL : -37.045 KLD : 1.112 
iteration : 44000 loss : 53.733 NLL : -52.157 KLD : 1.576 
iteration : 44200 loss : 40.105 NLL : -39.925 KLD : 0.181 
iteration : 44400 loss : 50.440 NLL : -49.446 KLD : 0.994 
iteration : 44600 loss : 61.421 NLL : -61.093 KLD : 0.328 
iteration : 44800 loss : 47.785 NLL : -45.312 KLD : 2.473 
iteration : 45000 loss : 49.642 NLL : -49.471 KLD : 0.171 
iteration : 45200 loss : 43.953 NLL : -43.357 KLD : 0.596 
iteration : 45400 loss : 47.921 NLL : -47.797 KLD : 0.125 
iteration : 45600 loss : 17.925 NLL : -17.040 KLD : 0.885 
iteration : 45800 loss : 58.414 NLL : -56.322 KLD : 2.092 
iteration : 46000 loss : 34.495 NLL : -32.782 KLD : 1.712 
iteration : 46200 loss : 34.337 NLL : -33.860 KLD : 0.477 
iteration : 46400 loss : 46.223 NLL : -46.025 KLD : 0.198 
iteration : 46600 loss : 39.358 NLL : -38.171 KLD : 1.187 
iteration : 46800 loss : 39.559 NLL : -39.358 KLD : 0.201 
iteration : 47000 loss : 37.702 NLL : -37.036 KLD : 0.666 
iteration : 47200 loss : 53.203 NLL : -52.088 KLD : 1.115 
iteration : 47400 loss : 47.334 NLL : -46.504 KLD : 0.830 
iteration : 47600 loss : 40.518 NLL : -40.060 KLD : 0.458 
iteration : 47800 loss : 47.792 NLL : -47.236 KLD : 0.556 
iteration : 48000 loss : 59.564 NLL : -57.324 KLD : 2.240 
iteration : 48200 loss : 59.558 NLL : -59.315 KLD : 0.243 
iteration : 48400 loss : 55.683 NLL : -55.336 KLD : 0.346 
iteration : 48600 loss : 49.634 NLL : -49.470 KLD : 0.165 
iteration : 48800 loss : 53.585 NLL : -51.524 KLD : 2.061 
iteration : 49000 loss : 49.646 NLL : -49.530 KLD : 0.116 
iteration : 49200 loss : 50.418 NLL : -50.189 KLD : 0.229 
iteration : 49400 loss : 55.788 NLL : -55.039 KLD : 0.749 
iteration : 49600 loss : 53.090 NLL : -52.755 KLD : 0.335 
iteration : 49800 loss : 38.369 NLL : -37.459 KLD : 0.910 
iteration : 50000 loss : 55.750 NLL : -55.433 KLD : 0.317 
iteration : 50200 loss : 62.377 NLL : -62.130 KLD : 0.246 
iteration : 50400 loss : 52.585 NLL : -52.444 KLD : 0.141 
iteration : 50600 loss : 41.036 NLL : -38.046 KLD : 2.990 
iteration : 50800 loss : 32.849 NLL : -31.931 KLD : 0.918 
iteration : 51000 loss : 38.709 NLL : -38.382 KLD : 0.327 
iteration : 51200 loss : 37.802 NLL : -37.230 KLD : 0.572 
iteration : 51400 loss : 48.766 NLL : -48.576 KLD : 0.190 
iteration : 51600 loss : 27.478 NLL : -26.772 KLD : 0.706 
iteration : 51800 loss : 47.768 NLL : -46.700 KLD : 1.069 
iteration : 52000 loss : 34.779 NLL : -34.433 KLD : 0.346 
iteration : 52200 loss : 58.673 NLL : -58.392 KLD : 0.282 
iteration : 52400 loss : 49.366 NLL : -48.714 KLD : 0.652 
iteration : 52600 loss : 47.824 NLL : -47.730 KLD : 0.094 
iteration : 52800 loss : 33.906 NLL : -33.414 KLD : 0.492 
iteration : 53000 loss : 33.999 NLL : -33.000 KLD : 1.000 
iteration : 53200 loss : 57.393 NLL : -55.419 KLD : 1.974 
iteration : 53400 loss : 59.133 NLL : -55.261 KLD : 3.872 
iteration : 53600 loss : 30.155 NLL : -28.206 KLD : 1.949 
iteration : 53800 loss : 55.819 NLL : -54.828 KLD : 0.991 
iteration : 54000 loss : 48.307 NLL : -47.593 KLD : 0.714 
iteration : 54200 loss : 51.990 NLL : -51.243 KLD : 0.747 
iteration : 54400 loss : 59.997 NLL : -59.379 KLD : 0.618 
iteration : 54600 loss : 52.853 NLL : -52.391 KLD : 0.462 
iteration : 54800 loss : 64.867 NLL : -60.901 KLD : 3.966 
iteration : 55000 loss : 44.829 NLL : -44.606 KLD : 0.223 
iteration : 55200 loss : 60.172 NLL : -59.182 KLD : 0.989 
iteration : 55400 loss : 55.059 NLL : -52.686 KLD : 2.373 
iteration : 55600 loss : 61.938 NLL : -61.520 KLD : 0.418 
iteration : 55800 loss : 55.850 NLL : -55.684 KLD : 0.166 
iteration : 56000 loss : 48.375 NLL : -48.227 KLD : 0.148 
iteration : 56200 loss : 61.965 NLL : -61.743 KLD : 0.222 
iteration : 56400 loss : 42.067 NLL : -41.643 KLD : 0.424 
iteration : 56600 loss : 51.126 NLL : -50.610 KLD : 0.516 
iteration : 56800 loss : 52.677 NLL : -52.405 KLD : 0.272 
iteration : 57000 loss : 40.026 NLL : -39.228 KLD : 0.798 
iteration : 57200 loss : 40.345 NLL : -39.820 KLD : 0.525 
iteration : 57400 loss : 55.613 NLL : -53.847 KLD : 1.765 
iteration : 57600 loss : 44.095 NLL : -43.465 KLD : 0.630 
iteration : 57800 loss : 55.872 NLL : -55.474 KLD : 0.398 
iteration : 58000 loss : 33.499 NLL : -32.936 KLD : 0.564 
iteration : 58200 loss : 46.991 NLL : -46.744 KLD : 0.248 
iteration : 58400 loss : 52.027 NLL : -51.520 KLD : 0.507 
iteration : 58600 loss : 34.429 NLL : -33.575 KLD : 0.854 
iteration : 58800 loss : 55.912 NLL : -55.510 KLD : 0.402 
iteration : 59000 loss : 56.710 NLL : -56.595 KLD : 0.115 
iteration : 59200 loss : 40.705 NLL : -40.486 KLD : 0.220 
iteration : 59400 loss : 51.224 NLL : -50.877 KLD : 0.347 
iteration : 59600 loss : 38.974 NLL : -32.845 KLD : 6.129 
iteration : 59800 loss : 80.940 NLL : -79.473 KLD : 1.467 
iteration : 60000 loss : 48.293 NLL : -48.098 KLD : 0.195 
---------- Training loss 44.626 updated ! and save the model! (step:60000) ----------
---------- Training loss 41.995 updated ! and save the model! (step:60066) ----------
---------- Training loss 41.785 updated ! and save the model! (step:60180) ----------
iteration : 60200 loss : 25.165 NLL : -23.585 KLD : 1.580 
---------- Training loss 36.865 updated ! and save the model! (step:60282) ----------
iteration : 60400 loss : 55.370 NLL : -54.386 KLD : 0.983 
iteration : 60600 loss : 58.433 NLL : -57.625 KLD : 0.807 
iteration : 60800 loss : 46.378 NLL : -46.201 KLD : 0.177 
iteration : 61000 loss : 52.851 NLL : -52.107 KLD : 0.744 
iteration : 61200 loss : 52.609 NLL : -39.447 KLD : 13.162 
iteration : 61400 loss : 48.038 NLL : -47.783 KLD : 0.255 
iteration : 61600 loss : 34.160 NLL : -32.572 KLD : 1.588 
iteration : 61800 loss : 60.178 NLL : -57.978 KLD : 2.200 
iteration : 62000 loss : 50.725 NLL : -49.389 KLD : 1.336 
iteration : 62200 loss : 54.260 NLL : -53.561 KLD : 0.699 
iteration : 62400 loss : 29.300 NLL : -28.896 KLD : 0.404 
---------- Training loss 35.567 updated ! and save the model! (step:62580) ----------
iteration : 62600 loss : 57.068 NLL : -56.739 KLD : 0.329 
iteration : 62800 loss : 35.245 NLL : -34.422 KLD : 0.823 
iteration : 63000 loss : 62.875 NLL : -60.028 KLD : 2.847 
iteration : 63200 loss : 53.685 NLL : -53.549 KLD : 0.136 
iteration : 63400 loss : 27.339 NLL : -26.422 KLD : 0.918 
---------- Training loss 35.496 updated ! and save the model! (step:63510) ----------
iteration : 63600 loss : 48.515 NLL : -48.137 KLD : 0.378 
iteration : 63800 loss : 71.726 NLL : -66.206 KLD : 5.520 
iteration : 64000 loss : 61.297 NLL : -60.112 KLD : 1.185 
iteration : 64200 loss : 49.568 NLL : -47.016 KLD : 2.552 
iteration : 64400 loss : 47.141 NLL : -46.925 KLD : 0.216 
iteration : 64600 loss : 63.195 NLL : -62.127 KLD : 1.068 
iteration : 64800 loss : 27.498 NLL : -26.226 KLD : 1.273 
iteration : 65000 loss : 55.864 NLL : -55.163 KLD : 0.702 
iteration : 65200 loss : 53.883 NLL : -52.648 KLD : 1.235 
---------- Training loss 32.050 updated ! and save the model! (step:65220) ----------
iteration : 65400 loss : 43.429 NLL : -42.990 KLD : 0.439 
iteration : 65600 loss : 54.616 NLL : -52.994 KLD : 1.623 
iteration : 65800 loss : 53.306 NLL : -51.814 KLD : 1.492 
iteration : 66000 loss : 25.308 NLL : -22.110 KLD : 3.197 
iteration : 66200 loss : 44.326 NLL : -43.306 KLD : 1.020 
iteration : 66400 loss : 59.333 NLL : -58.870 KLD : 0.464 
iteration : 66600 loss : 56.252 NLL : -56.116 KLD : 0.135 
iteration : 66800 loss : 48.354 NLL : -47.348 KLD : 1.006 
iteration : 67000 loss : 57.739 NLL : -54.695 KLD : 3.043 
iteration : 67200 loss : 49.727 NLL : -48.808 KLD : 0.919 
iteration : 67400 loss : 65.624 NLL : -52.660 KLD : 12.964 
iteration : 67600 loss : 25.715 NLL : -25.018 KLD : 0.697 
iteration : 67800 loss : 40.153 NLL : -38.882 KLD : 1.271 
iteration : 68000 loss : 52.753 NLL : -51.185 KLD : 1.568 
iteration : 68200 loss : 46.778 NLL : -44.407 KLD : 2.371 
iteration : 68400 loss : 64.484 NLL : -59.124 KLD : 5.360 
iteration : 68600 loss : 23.872 NLL : -23.428 KLD : 0.444 
iteration : 68800 loss : 32.507 NLL : -31.586 KLD : 0.921 
iteration : 69000 loss : 29.759 NLL : -29.352 KLD : 0.407 
iteration : 69200 loss : 72.056 NLL : -71.352 KLD : 0.704 
iteration : 69400 loss : 52.625 NLL : -52.134 KLD : 0.490 
iteration : 69600 loss : 43.162 NLL : -33.294 KLD : 9.868 
iteration : 69800 loss : 48.473 NLL : -47.041 KLD : 1.432 
iteration : 70000 loss : 26.148 NLL : -25.636 KLD : 0.512 
iteration : 70200 loss : 81.768 NLL : -51.199 KLD : 30.569 
iteration : 70400 loss : 50.562 NLL : -49.872 KLD : 0.690 
iteration : 70600 loss : 34.010 NLL : -32.583 KLD : 1.427 
iteration : 70800 loss : 47.888 NLL : -47.558 KLD : 0.330 
iteration : 71000 loss : 35.320 NLL : -34.458 KLD : 0.863 
iteration : 71200 loss : 56.563 NLL : -56.378 KLD : 0.185 
iteration : 71400 loss : 42.219 NLL : -41.977 KLD : 0.243 
iteration : 71600 loss : 50.498 NLL : -48.434 KLD : 2.065 
iteration : 71800 loss : 39.348 NLL : -39.174 KLD : 0.174 
iteration : 72000 loss : 57.573 NLL : -54.699 KLD : 2.874 
iteration : 72200 loss : 53.113 NLL : -51.557 KLD : 1.556 
iteration : 72400 loss : 53.681 NLL : -53.427 KLD : 0.254 
iteration : 72600 loss : 56.069 NLL : -53.670 KLD : 2.399 
iteration : 72800 loss : 51.909 NLL : -51.736 KLD : 0.172 
iteration : 73000 loss : 62.460 NLL : -62.128 KLD : 0.332 
iteration : 73200 loss : 34.612 NLL : -33.406 KLD : 1.206 
iteration : 73400 loss : 47.121 NLL : -46.422 KLD : 0.699 
iteration : 73600 loss : 54.254 NLL : -53.925 KLD : 0.329 
iteration : 73800 loss : 59.355 NLL : -58.431 KLD : 0.924 
iteration : 74000 loss : 67.536 NLL : -65.467 KLD : 2.068 
iteration : 74200 loss : 33.210 NLL : -32.017 KLD : 1.194 
iteration : 74400 loss : 60.732 NLL : -59.918 KLD : 0.814 
iteration : 74600 loss : 49.229 NLL : -48.624 KLD : 0.605 
iteration : 74800 loss : 35.529 NLL : -33.922 KLD : 1.607 
iteration : 75000 loss : 33.644 NLL : -32.123 KLD : 1.520 
iteration : 75200 loss : 38.138 NLL : -37.789 KLD : 0.349 
iteration : 75400 loss : 38.415 NLL : -37.893 KLD : 0.522 
iteration : 75600 loss : 33.201 NLL : -31.192 KLD : 2.009 
iteration : 75800 loss : 53.058 NLL : -52.162 KLD : 0.896 
iteration : 76000 loss : 37.611 NLL : -36.906 KLD : 0.704 
iteration : 76200 loss : 59.613 NLL : -59.379 KLD : 0.234 
iteration : 76400 loss : 37.462 NLL : -37.276 KLD : 0.186 
iteration : 76600 loss : 60.435 NLL : -59.946 KLD : 0.489 
iteration : 76800 loss : 48.553 NLL : -48.107 KLD : 0.446 
iteration : 77000 loss : 63.294 NLL : -62.430 KLD : 0.864 
iteration : 77200 loss : 36.027 NLL : -35.872 KLD : 0.155 
iteration : 77400 loss : 50.088 NLL : -49.527 KLD : 0.562 
iteration : 77600 loss : 36.213 NLL : -35.752 KLD : 0.461 
iteration : 77800 loss : 71.391 NLL : -70.763 KLD : 0.629 
iteration : 78000 loss : 49.598 NLL : -49.347 KLD : 0.251 
iteration : 78200 loss : 62.858 NLL : -62.500 KLD : 0.358 
iteration : 78400 loss : 24.520 NLL : -23.783 KLD : 0.737 
iteration : 78600 loss : 63.904 NLL : -63.064 KLD : 0.840 
iteration : 78800 loss : 31.175 NLL : -27.509 KLD : 3.666 
iteration : 79000 loss : 51.080 NLL : -47.912 KLD : 3.168 
iteration : 79200 loss : 30.309 NLL : -28.909 KLD : 1.400 
iteration : 79400 loss : 55.967 NLL : -55.643 KLD : 0.323 
---------- Training loss 31.849 updated ! and save the model! (step:79452) ----------
iteration : 79600 loss : 231.874 NLL : -230.002 KLD : 1.872 
iteration : 79800 loss : 63.522 NLL : -62.641 KLD : 0.881 
iteration : 80000 loss : 57.850 NLL : -57.572 KLD : 0.278 
iteration : 80200 loss : 39.467 NLL : -38.699 KLD : 0.768 
iteration : 80400 loss : 49.159 NLL : -48.674 KLD : 0.485 
iteration : 80600 loss : 58.038 NLL : -57.849 KLD : 0.189 
iteration : 80800 loss : 48.361 NLL : -47.983 KLD : 0.378 
iteration : 81000 loss : 98.499 NLL : -56.517 KLD : 41.982 
iteration : 81200 loss : 56.511 NLL : -53.280 KLD : 3.231 
iteration : 81400 loss : 53.035 NLL : -52.709 KLD : 0.326 
iteration : 81600 loss : 31.266 NLL : -31.010 KLD : 0.256 
iteration : 81800 loss : 48.915 NLL : -48.624 KLD : 0.291 
iteration : 82000 loss : 50.473 NLL : -49.964 KLD : 0.509 
iteration : 82200 loss : 57.907 NLL : -57.051 KLD : 0.856 
iteration : 82400 loss : 49.468 NLL : -48.309 KLD : 1.158 
iteration : 82600 loss : 52.702 NLL : -50.436 KLD : 2.265 
iteration : 82800 loss : 37.558 NLL : -35.482 KLD : 2.075 
iteration : 83000 loss : 62.215 NLL : -60.629 KLD : 1.585 
iteration : 83200 loss : 50.594 NLL : -50.191 KLD : 0.402 
iteration : 83400 loss : 45.159 NLL : -42.176 KLD : 2.984 
iteration : 83600 loss : 55.774 NLL : -55.540 KLD : 0.235 
iteration : 83800 loss : 55.936 NLL : -55.762 KLD : 0.174 
iteration : 84000 loss : 50.856 NLL : -50.466 KLD : 0.390 
iteration : 84200 loss : 27.106 NLL : -25.640 KLD : 1.466 
iteration : 84400 loss : 44.366 NLL : -44.130 KLD : 0.236 
iteration : 84600 loss : 46.391 NLL : -45.897 KLD : 0.495 
iteration : 84800 loss : 54.142 NLL : -52.966 KLD : 1.176 
iteration : 85000 loss : 50.313 NLL : -48.764 KLD : 1.549 
iteration : 85200 loss : 48.780 NLL : -48.361 KLD : 0.419 
iteration : 85400 loss : 53.419 NLL : -53.059 KLD : 0.360 
iteration : 85600 loss : 45.794 NLL : -45.576 KLD : 0.218 
iteration : 85800 loss : 26.953 NLL : -26.717 KLD : 0.236 
iteration : 86000 loss : 92.089 NLL : -84.873 KLD : 7.216 
iteration : 86200 loss : 31.906 NLL : -29.206 KLD : 2.700 
iteration : 86400 loss : 51.444 NLL : -50.785 KLD : 0.659 
iteration : 86600 loss : 41.929 NLL : -41.349 KLD : 0.580 
iteration : 86800 loss : 51.063 NLL : -50.942 KLD : 0.121 
iteration : 87000 loss : 55.993 NLL : -55.685 KLD : 0.308 
iteration : 87200 loss : 46.114 NLL : -46.022 KLD : 0.092 
iteration : 87400 loss : 28.692 NLL : -28.279 KLD : 0.413 
iteration : 87600 loss : 37.284 NLL : -36.970 KLD : 0.314 
iteration : 87800 loss : 50.455 NLL : -49.606 KLD : 0.849 
iteration : 88000 loss : 62.168 NLL : -61.243 KLD : 0.926 
iteration : 88200 loss : 54.909 NLL : -54.602 KLD : 0.306 
iteration : 88400 loss : 36.525 NLL : -36.297 KLD : 0.229 
iteration : 88600 loss : 46.248 NLL : -44.579 KLD : 1.669 
iteration : 88800 loss : 57.773 NLL : -55.459 KLD : 2.314 
iteration : 89000 loss : 35.404 NLL : -34.037 KLD : 1.367 
iteration : 89200 loss : 48.900 NLL : -48.208 KLD : 0.692 
iteration : 89400 loss : 43.666 NLL : -42.774 KLD : 0.892 
iteration : 89600 loss : 33.366 NLL : -33.222 KLD : 0.145 
iteration : 89800 loss : 50.891 NLL : -49.434 KLD : 1.457 
iteration : 90000 loss : 42.340 NLL : -42.093 KLD : 0.248 
---------- Training loss 39.152 updated ! and save the model! (step:90000) ----------
iteration : 90200 loss : 43.907 NLL : -42.143 KLD : 1.764 
---------- Training loss 36.868 updated ! and save the model! (step:90222) ----------
iteration : 90400 loss : 51.647 NLL : -50.944 KLD : 0.703 
iteration : 90600 loss : 34.588 NLL : -33.340 KLD : 1.248 
iteration : 90800 loss : 54.053 NLL : -53.764 KLD : 0.289 
iteration : 91000 loss : 53.194 NLL : -52.946 KLD : 0.248 
iteration : 91200 loss : 34.026 NLL : -31.070 KLD : 2.956 
iteration : 91400 loss : 47.586 NLL : -47.106 KLD : 0.481 
iteration : 91600 loss : 76.181 NLL : -75.380 KLD : 0.801 
iteration : 91800 loss : 52.648 NLL : -52.462 KLD : 0.186 
---------- Training loss 36.040 updated ! and save the model! (step:91920) ----------
iteration : 92000 loss : 54.461 NLL : -53.860 KLD : 0.601 
iteration : 92200 loss : 64.894 NLL : -63.923 KLD : 0.971 
iteration : 92400 loss : 34.903 NLL : -34.495 KLD : 0.407 
---------- Training loss 33.957 updated ! and save the model! (step:92460) ----------
iteration : 92600 loss : 33.650 NLL : -33.390 KLD : 0.260 
iteration : 92800 loss : 51.876 NLL : -51.755 KLD : 0.121 
iteration : 93000 loss : 58.073 NLL : -57.651 KLD : 0.422 
iteration : 93200 loss : 63.950 NLL : -62.519 KLD : 1.431 
iteration : 93400 loss : 41.826 NLL : -41.423 KLD : 0.403 
iteration : 93600 loss : 33.310 NLL : -32.874 KLD : 0.436 
iteration : 93800 loss : 72.261 NLL : -70.919 KLD : 1.341 
iteration : 94000 loss : 41.682 NLL : -41.348 KLD : 0.334 
iteration : 94200 loss : 41.893 NLL : -39.993 KLD : 1.901 
iteration : 94400 loss : 59.209 NLL : -58.965 KLD : 0.244 
iteration : 94600 loss : 34.864 NLL : -34.052 KLD : 0.813 
iteration : 94800 loss : 28.839 NLL : -28.489 KLD : 0.350 
iteration : 95000 loss : 46.023 NLL : -45.890 KLD : 0.133 
iteration : 95200 loss : 41.960 NLL : -41.584 KLD : 0.377 
iteration : 95400 loss : 60.446 NLL : -57.310 KLD : 3.136 
---------- Training loss 32.733 updated ! and save the model! (step:95538) ----------
iteration : 95600 loss : 68.958 NLL : -68.742 KLD : 0.216 
iteration : 95800 loss : 50.551 NLL : -45.898 KLD : 4.654 
iteration : 96000 loss : 56.346 NLL : -56.243 KLD : 0.103 
iteration : 96200 loss : 45.588 NLL : -45.347 KLD : 0.241 
iteration : 96400 loss : 50.021 NLL : -49.826 KLD : 0.195 
iteration : 96600 loss : 42.901 NLL : -42.589 KLD : 0.312 
iteration : 96800 loss : 49.605 NLL : -49.383 KLD : 0.222 
iteration : 97000 loss : 65.590 NLL : -64.427 KLD : 1.163 
iteration : 97200 loss : 44.561 NLL : -44.425 KLD : 0.137 
iteration : 97400 loss : 54.852 NLL : -54.626 KLD : 0.226 
iteration : 97600 loss : 56.678 NLL : -55.246 KLD : 1.432 
iteration : 97800 loss : 36.032 NLL : -35.257 KLD : 0.775 
iteration : 98000 loss : 60.458 NLL : -59.466 KLD : 0.992 
iteration : 98200 loss : 45.273 NLL : -44.820 KLD : 0.453 
iteration : 98400 loss : 42.417 NLL : -39.382 KLD : 3.035 
iteration : 98600 loss : 36.169 NLL : -32.042 KLD : 4.128 
iteration : 98800 loss : 40.111 NLL : -39.253 KLD : 0.857 
iteration : 99000 loss : 68.802 NLL : -65.565 KLD : 3.237 
iteration : 99200 loss : 49.453 NLL : -43.168 KLD : 6.285 
iteration : 99400 loss : 21.453 NLL : -18.621 KLD : 2.832 
iteration : 99600 loss : 77.510 NLL : -74.190 KLD : 3.320 
iteration : 99800 loss : 48.242 NLL : -47.556 KLD : 0.685 
iteration : 100000 loss : 47.720 NLL : -44.289 KLD : 3.431 
iteration : 100200 loss : 43.916 NLL : -42.951 KLD : 0.965 
iteration : 100400 loss : 55.441 NLL : -54.743 KLD : 0.699 
iteration : 100600 loss : 55.819 NLL : -53.583 KLD : 2.236 
iteration : 100800 loss : 55.332 NLL : -53.757 KLD : 1.576 
iteration : 101000 loss : 66.593 NLL : -66.177 KLD : 0.416 
iteration : 101200 loss : 51.483 NLL : -50.946 KLD : 0.537 
iteration : 101400 loss : 50.789 NLL : -50.577 KLD : 0.212 
iteration : 101600 loss : 51.169 NLL : -50.717 KLD : 0.451 
iteration : 101800 loss : 41.453 NLL : -29.784 KLD : 11.669 
iteration : 102000 loss : 48.540 NLL : -48.262 KLD : 0.278 
iteration : 102200 loss : 58.875 NLL : -58.011 KLD : 0.864 
iteration : 102400 loss : 76.282 NLL : -72.344 KLD : 3.938 
iteration : 102600 loss : 53.357 NLL : -53.241 KLD : 0.116 
iteration : 102800 loss : 41.333 NLL : -38.261 KLD : 3.071 
iteration : 103000 loss : 33.680 NLL : -32.871 KLD : 0.808 
iteration : 103200 loss : 52.258 NLL : -51.748 KLD : 0.510 
iteration : 103400 loss : 62.729 NLL : -58.187 KLD : 4.542 
iteration : 103600 loss : 33.600 NLL : -32.123 KLD : 1.478 
iteration : 103800 loss : 31.066 NLL : -30.371 KLD : 0.696 
iteration : 104000 loss : 48.069 NLL : -47.738 KLD : 0.331 
iteration : 104200 loss : 50.025 NLL : -49.345 KLD : 0.680 
iteration : 104400 loss : 57.423 NLL : -57.142 KLD : 0.281 
iteration : 104600 loss : 51.217 NLL : -50.697 KLD : 0.520 
iteration : 104800 loss : 44.853 NLL : -44.059 KLD : 0.794 
iteration : 105000 loss : 63.359 NLL : -61.141 KLD : 2.217 
iteration : 105200 loss : 47.166 NLL : -46.610 KLD : 0.556 
iteration : 105400 loss : 39.652 NLL : -39.391 KLD : 0.261 
iteration : 105600 loss : 65.555 NLL : -64.906 KLD : 0.648 
iteration : 105800 loss : 55.983 NLL : -55.530 KLD : 0.453 
iteration : 106000 loss : 41.768 NLL : -41.545 KLD : 0.224 
iteration : 106200 loss : 57.071 NLL : -56.728 KLD : 0.343 
iteration : 106400 loss : 45.122 NLL : -43.317 KLD : 1.805 
iteration : 106600 loss : 59.804 NLL : -59.314 KLD : 0.491 
---------- Training loss 31.600 updated ! and save the model! (step:106776) ----------
iteration : 106800 loss : 46.855 NLL : -46.683 KLD : 0.172 
iteration : 107000 loss : 62.306 NLL : -61.698 KLD : 0.608 
iteration : 107200 loss : 49.750 NLL : -49.130 KLD : 0.620 
iteration : 107400 loss : 54.865 NLL : -54.709 KLD : 0.157 
iteration : 107600 loss : 45.390 NLL : -44.886 KLD : 0.504 
iteration : 107800 loss : 47.456 NLL : -46.255 KLD : 1.201 
iteration : 108000 loss : 62.516 NLL : -62.344 KLD : 0.172 
iteration : 108200 loss : 57.739 NLL : -57.334 KLD : 0.405 
iteration : 108400 loss : 30.611 NLL : -27.671 KLD : 2.940 
iteration : 108600 loss : 35.633 NLL : -35.213 KLD : 0.421 
iteration : 108800 loss : 29.596 NLL : -29.337 KLD : 0.259 
iteration : 109000 loss : 59.884 NLL : -59.333 KLD : 0.551 
iteration : 109200 loss : 32.608 NLL : -31.907 KLD : 0.701 
iteration : 109400 loss : 28.769 NLL : -28.390 KLD : 0.379 
iteration : 109600 loss : 58.660 NLL : -57.215 KLD : 1.445 
iteration : 109800 loss : 42.978 NLL : -42.807 KLD : 0.172 
iteration : 110000 loss : 56.548 NLL : -56.176 KLD : 0.372 
iteration : 110200 loss : 33.361 NLL : -32.703 KLD : 0.657 
iteration : 110400 loss : 65.797 NLL : -64.827 KLD : 0.970 
iteration : 110600 loss : 52.766 NLL : -51.561 KLD : 1.206 
iteration : 110800 loss : 42.802 NLL : -42.240 KLD : 0.563 
iteration : 111000 loss : 24.850 NLL : -23.250 KLD : 1.600 
iteration : 111200 loss : 43.428 NLL : -42.095 KLD : 1.333 
iteration : 111400 loss : 43.101 NLL : -42.965 KLD : 0.137 
iteration : 111600 loss : 20.884 NLL : -19.837 KLD : 1.048 
iteration : 111800 loss : 55.857 NLL : -55.522 KLD : 0.335 
iteration : 112000 loss : 36.535 NLL : -34.856 KLD : 1.679 
iteration : 112200 loss : 47.805 NLL : -47.437 KLD : 0.368 
iteration : 112400 loss : 40.430 NLL : -38.752 KLD : 1.679 
iteration : 112600 loss : 39.491 NLL : -39.367 KLD : 0.124 
iteration : 112800 loss : 77.074 NLL : -74.458 KLD : 2.616 
iteration : 113000 loss : 56.849 NLL : -56.591 KLD : 0.259 
iteration : 113200 loss : 46.205 NLL : -46.081 KLD : 0.124 
iteration : 113400 loss : 63.659 NLL : -52.840 KLD : 10.820 
iteration : 113600 loss : 59.205 NLL : -58.768 KLD : 0.437 
iteration : 113800 loss : 45.790 NLL : -45.006 KLD : 0.784 
iteration : 114000 loss : 75.988 NLL : -72.887 KLD : 3.101 
iteration : 114200 loss : 74.343 NLL : -54.022 KLD : 20.321 
iteration : 114400 loss : 54.780 NLL : -54.128 KLD : 0.652 
iteration : 114600 loss : 53.958 NLL : -53.016 KLD : 0.942 
iteration : 114800 loss : 63.527 NLL : -63.194 KLD : 0.333 
iteration : 115000 loss : 42.481 NLL : -42.318 KLD : 0.162 
iteration : 115200 loss : 63.615 NLL : -63.235 KLD : 0.380 
iteration : 115400 loss : 54.578 NLL : -54.184 KLD : 0.394 
iteration : 115600 loss : 56.482 NLL : -55.776 KLD : 0.706 
iteration : 115800 loss : 74.323 NLL : -71.515 KLD : 2.808 
iteration : 116000 loss : 55.283 NLL : -54.992 KLD : 0.291 
iteration : 116200 loss : 66.352 NLL : -66.057 KLD : 0.294 
iteration : 116400 loss : 31.488 NLL : -30.625 KLD : 0.864 
iteration : 116600 loss : 44.196 NLL : -43.938 KLD : 0.258 
iteration : 116800 loss : 68.220 NLL : -65.816 KLD : 2.404 
iteration : 117000 loss : 57.052 NLL : -56.521 KLD : 0.531 
iteration : 117200 loss : 35.983 NLL : -33.912 KLD : 2.071 
iteration : 117400 loss : 40.643 NLL : -40.115 KLD : 0.527 
iteration : 117600 loss : 43.747 NLL : -43.257 KLD : 0.489 
iteration : 117800 loss : 52.236 NLL : -52.156 KLD : 0.080 
iteration : 118000 loss : 48.974 NLL : -48.743 KLD : 0.231 
iteration : 118200 loss : 43.387 NLL : -43.076 KLD : 0.311 
iteration : 118400 loss : 40.031 NLL : -38.953 KLD : 1.079 
iteration : 118600 loss : 54.815 NLL : -54.557 KLD : 0.258 
iteration : 118800 loss : 52.012 NLL : -51.706 KLD : 0.305 
iteration : 119000 loss : 53.740 NLL : -53.292 KLD : 0.448 
iteration : 119200 loss : 44.299 NLL : -44.091 KLD : 0.208 
iteration : 119400 loss : 38.082 NLL : -37.795 KLD : 0.286 
iteration : 119600 loss : 42.796 NLL : -42.650 KLD : 0.146 
iteration : 119800 loss : 62.243 NLL : -61.020 KLD : 1.223 
iteration : 120000 loss : 62.684 NLL : -62.050 KLD : 0.634 
---------- Training loss 61.981 updated ! and save the model! (step:120000) ----------
---------- Training loss 61.278 updated ! and save the model! (step:120006) ----------
---------- Training loss 48.215 updated ! and save the model! (step:120012) ----------
---------- Training loss 44.343 updated ! and save the model! (step:120018) ----------
---------- Training loss 39.093 updated ! and save the model! (step:120036) ----------
---------- Training loss 37.829 updated ! and save the model! (step:120054) ----------
iteration : 120200 loss : 42.751 NLL : -41.216 KLD : 1.535 
iteration : 120400 loss : 31.690 NLL : -30.606 KLD : 1.084 
iteration : 120600 loss : 28.262 NLL : -27.824 KLD : 0.438 
iteration : 120800 loss : 37.738 NLL : -36.719 KLD : 1.019 
---------- Training loss 32.393 updated ! and save the model! (step:120834) ----------
iteration : 121000 loss : 56.510 NLL : -56.133 KLD : 0.377 
iteration : 121200 loss : 48.579 NLL : -48.287 KLD : 0.292 
iteration : 121400 loss : 63.891 NLL : -62.779 KLD : 1.112 
iteration : 121600 loss : 35.004 NLL : -34.441 KLD : 0.563 
iteration : 121800 loss : 48.990 NLL : -48.804 KLD : 0.186 
iteration : 122000 loss : 53.000 NLL : -52.642 KLD : 0.358 
iteration : 122200 loss : 59.196 NLL : -58.480 KLD : 0.716 
iteration : 122400 loss : 53.714 NLL : -53.401 KLD : 0.313 
iteration : 122600 loss : 37.638 NLL : -36.932 KLD : 0.706 
iteration : 122800 loss : 60.254 NLL : -59.928 KLD : 0.326 
iteration : 123000 loss : 63.022 NLL : -62.705 KLD : 0.317 
iteration : 123200 loss : 37.448 NLL : -35.646 KLD : 1.802 
iteration : 123400 loss : 52.925 NLL : -51.926 KLD : 0.999 
iteration : 123600 loss : 49.147 NLL : -48.538 KLD : 0.609 
iteration : 123800 loss : 34.999 NLL : -33.657 KLD : 1.342 
iteration : 124000 loss : 47.730 NLL : -45.503 KLD : 2.227 
iteration : 124200 loss : 46.963 NLL : -46.516 KLD : 0.447 
iteration : 124400 loss : 43.435 NLL : -43.044 KLD : 0.391 
iteration : 124600 loss : 22.538 NLL : -20.321 KLD : 2.217 
iteration : 124800 loss : 40.911 NLL : -40.776 KLD : 0.135 
iteration : 125000 loss : 54.879 NLL : -54.359 KLD : 0.520 
iteration : 125200 loss : 42.781 NLL : -40.814 KLD : 1.967 
iteration : 125400 loss : 47.645 NLL : -44.615 KLD : 3.030 
iteration : 125600 loss : 47.998 NLL : -44.633 KLD : 3.365 
iteration : 125800 loss : 42.354 NLL : -36.778 KLD : 5.576 
iteration : 126000 loss : 36.752 NLL : -36.570 KLD : 0.182 
iteration : 126200 loss : 39.491 NLL : -39.234 KLD : 0.257 
iteration : 126400 loss : 50.896 NLL : -50.749 KLD : 0.146 
---------- Training loss 31.190 updated ! and save the model! (step:126402) ----------
iteration : 126600 loss : 42.013 NLL : -41.643 KLD : 0.370 
iteration : 126800 loss : 44.166 NLL : -43.728 KLD : 0.438 
iteration : 127000 loss : 60.941 NLL : -60.780 KLD : 0.161 
iteration : 127200 loss : 44.814 NLL : -43.942 KLD : 0.872 
iteration : 127400 loss : 69.557 NLL : -67.922 KLD : 1.634 
iteration : 127600 loss : 40.194 NLL : -39.973 KLD : 0.221 
iteration : 127800 loss : 47.977 NLL : -47.621 KLD : 0.356 
iteration : 128000 loss : 47.670 NLL : -47.421 KLD : 0.249 
iteration : 128200 loss : 62.851 NLL : -59.734 KLD : 3.118 
iteration : 128400 loss : 34.433 NLL : -32.822 KLD : 1.611 
iteration : 128600 loss : 42.047 NLL : -41.247 KLD : 0.800 
iteration : 128800 loss : 29.571 NLL : -27.524 KLD : 2.047 
iteration : 129000 loss : 41.916 NLL : -41.736 KLD : 0.180 
iteration : 129200 loss : 52.027 NLL : -51.736 KLD : 0.291 
iteration : 129400 loss : 51.950 NLL : -51.237 KLD : 0.713 
iteration : 129600 loss : 45.055 NLL : -44.932 KLD : 0.122 
iteration : 129800 loss : 51.728 NLL : -46.745 KLD : 4.983 
iteration : 130000 loss : 55.585 NLL : -54.482 KLD : 1.103 
iteration : 130200 loss : 49.509 NLL : -49.128 KLD : 0.381 
iteration : 130400 loss : 70.149 NLL : -69.984 KLD : 0.165 
iteration : 130600 loss : 41.721 NLL : -41.162 KLD : 0.559 
iteration : 130800 loss : 46.172 NLL : -44.829 KLD : 1.344 
iteration : 131000 loss : 45.231 NLL : -43.713 KLD : 1.519 
iteration : 131200 loss : 28.362 NLL : -27.071 KLD : 1.291 
iteration : 131400 loss : 49.768 NLL : -49.535 KLD : 0.233 
iteration : 131600 loss : 38.437 NLL : -37.803 KLD : 0.634 
iteration : 131800 loss : 43.353 NLL : -42.269 KLD : 1.084 
iteration : 132000 loss : 38.007 NLL : -37.564 KLD : 0.444 
iteration : 132200 loss : 53.702 NLL : -51.853 KLD : 1.849 
iteration : 132400 loss : 32.107 NLL : -30.804 KLD : 1.303 
iteration : 132600 loss : 39.683 NLL : -38.585 KLD : 1.098 
iteration : 132800 loss : 20.663 NLL : -19.211 KLD : 1.452 
iteration : 133000 loss : 45.146 NLL : -44.967 KLD : 0.179 
iteration : 133200 loss : 48.848 NLL : -44.980 KLD : 3.868 
iteration : 133400 loss : 46.023 NLL : -45.752 KLD : 0.271 
iteration : 133600 loss : 47.899 NLL : -47.624 KLD : 0.275 
iteration : 133800 loss : 57.002 NLL : -56.735 KLD : 0.267 
iteration : 134000 loss : 56.286 NLL : -53.248 KLD : 3.038 
iteration : 134200 loss : 53.306 NLL : -51.605 KLD : 1.701 
iteration : 134400 loss : 38.058 NLL : -35.736 KLD : 2.322 
iteration : 134600 loss : 47.957 NLL : -47.013 KLD : 0.944 
iteration : 134800 loss : 59.224 NLL : -47.855 KLD : 11.369 
iteration : 135000 loss : 53.850 NLL : -52.905 KLD : 0.945 
iteration : 135200 loss : 51.730 NLL : -51.262 KLD : 0.468 
iteration : 135400 loss : 38.586 NLL : -37.516 KLD : 1.069 
iteration : 135600 loss : 40.251 NLL : -39.411 KLD : 0.839 
iteration : 135800 loss : 45.581 NLL : -45.417 KLD : 0.165 
iteration : 136000 loss : 46.525 NLL : -46.169 KLD : 0.356 
iteration : 136200 loss : 39.454 NLL : -39.032 KLD : 0.422 
iteration : 136400 loss : 49.025 NLL : -48.909 KLD : 0.116 
iteration : 136600 loss : 33.877 NLL : -33.601 KLD : 0.275 
iteration : 136800 loss : 68.528 NLL : -64.797 KLD : 3.731 
iteration : 137000 loss : 50.336 NLL : -43.893 KLD : 6.444 
iteration : 137200 loss : 50.138 NLL : -49.911 KLD : 0.227 
iteration : 137400 loss : 59.949 NLL : -54.593 KLD : 5.356 
iteration : 137600 loss : 27.426 NLL : -22.914 KLD : 4.512 
iteration : 137800 loss : 51.186 NLL : -50.886 KLD : 0.300 
iteration : 138000 loss : 70.350 NLL : -69.558 KLD : 0.792 
iteration : 138200 loss : 40.679 NLL : -40.396 KLD : 0.283 
iteration : 138400 loss : 44.135 NLL : -43.404 KLD : 0.732 
iteration : 138600 loss : 35.612 NLL : -35.374 KLD : 0.238 
iteration : 138800 loss : 56.553 NLL : -56.421 KLD : 0.132 
iteration : 139000 loss : 47.462 NLL : -46.691 KLD : 0.771 
iteration : 139200 loss : 45.931 NLL : -45.851 KLD : 0.080 
iteration : 139400 loss : 51.694 NLL : -51.498 KLD : 0.196 
iteration : 139600 loss : 53.700 NLL : -52.155 KLD : 1.545 
iteration : 139800 loss : 47.531 NLL : -46.439 KLD : 1.092 
iteration : 140000 loss : 48.798 NLL : -47.087 KLD : 1.711 
iteration : 140200 loss : 34.528 NLL : -32.799 KLD : 1.729 
iteration : 140400 loss : 89.003 NLL : -88.635 KLD : 0.368 
iteration : 140600 loss : 51.168 NLL : -51.047 KLD : 0.121 
iteration : 140800 loss : 47.723 NLL : -46.887 KLD : 0.836 
iteration : 141000 loss : 31.819 NLL : -31.394 KLD : 0.425 
iteration : 141200 loss : 54.506 NLL : -52.596 KLD : 1.910 
iteration : 141400 loss : 57.412 NLL : -56.714 KLD : 0.698 
iteration : 141600 loss : 53.211 NLL : -52.814 KLD : 0.397 
iteration : 141800 loss : 59.044 NLL : -58.846 KLD : 0.197 
iteration : 142000 loss : 38.959 NLL : -38.109 KLD : 0.850 
iteration : 142200 loss : 52.140 NLL : -51.860 KLD : 0.281 
iteration : 142400 loss : 222.813 NLL : -220.219 KLD : 2.594 
iteration : 142600 loss : 52.066 NLL : -51.536 KLD : 0.530 
iteration : 142800 loss : 50.980 NLL : -50.899 KLD : 0.081 
iteration : 143000 loss : 52.308 NLL : -51.735 KLD : 0.573 
iteration : 143200 loss : 34.983 NLL : -34.210 KLD : 0.773 
iteration : 143400 loss : 46.103 NLL : -45.938 KLD : 0.165 
iteration : 143600 loss : 37.626 NLL : -35.491 KLD : 2.136 
iteration : 143800 loss : 62.141 NLL : -61.422 KLD : 0.719 
iteration : 144000 loss : 37.716 NLL : -36.270 KLD : 1.446 
iteration : 144200 loss : 35.198 NLL : -32.071 KLD : 3.127 
iteration : 144400 loss : 26.354 NLL : -24.281 KLD : 2.073 
iteration : 144600 loss : 37.824 NLL : -37.110 KLD : 0.715 
iteration : 144800 loss : 31.712 NLL : -30.360 KLD : 1.353 
iteration : 145000 loss : 50.637 NLL : -50.204 KLD : 0.432 
iteration : 145200 loss : 48.336 NLL : -46.780 KLD : 1.556 
iteration : 145400 loss : 56.571 NLL : -44.298 KLD : 12.273 
iteration : 145600 loss : 52.346 NLL : -51.903 KLD : 0.444 
iteration : 145800 loss : 38.140 NLL : -37.035 KLD : 1.105 
iteration : 146000 loss : 46.606 NLL : -46.097 KLD : 0.509 
iteration : 146200 loss : 35.575 NLL : -34.701 KLD : 0.874 
iteration : 146400 loss : 33.322 NLL : -32.302 KLD : 1.020 
iteration : 146600 loss : 56.161 NLL : -55.723 KLD : 0.439 
iteration : 146800 loss : 49.645 NLL : -49.130 KLD : 0.515 
iteration : 147000 loss : 45.233 NLL : -44.751 KLD : 0.482 
iteration : 147200 loss : 49.709 NLL : -49.500 KLD : 0.209 
iteration : 147400 loss : 52.004 NLL : -51.832 KLD : 0.172 
iteration : 147600 loss : 45.630 NLL : -45.061 KLD : 0.568 
iteration : 147800 loss : 48.347 NLL : -48.017 KLD : 0.330 
iteration : 148000 loss : 53.615 NLL : -51.715 KLD : 1.901 
iteration : 148200 loss : 55.743 NLL : -55.512 KLD : 0.231 
iteration : 148400 loss : 54.466 NLL : -54.160 KLD : 0.306 
iteration : 148600 loss : 43.760 NLL : -43.625 KLD : 0.136 
iteration : 148800 loss : 33.129 NLL : -32.791 KLD : 0.338 
iteration : 149000 loss : 53.233 NLL : -53.057 KLD : 0.176 
iteration : 149200 loss : 49.533 NLL : -49.266 KLD : 0.267 
iteration : 149400 loss : 41.416 NLL : -41.147 KLD : 0.270 
iteration : 149600 loss : 59.526 NLL : -58.770 KLD : 0.756 
iteration : 149800 loss : 34.773 NLL : -34.545 KLD : 0.228 
iteration : 150000 loss : 74.438 NLL : -72.302 KLD : 2.136 
---------- Training loss 51.594 updated ! and save the model! (step:150000) ----------
---------- Training loss 48.978 updated ! and save the model! (step:150006) ----------
---------- Training loss 46.022 updated ! and save the model! (step:150018) ----------
---------- Training loss 40.713 updated ! and save the model! (step:150030) ----------
---------- Training loss 40.509 updated ! and save the model! (step:150168) ----------
iteration : 150200 loss : 53.020 NLL : -52.692 KLD : 0.328 
---------- Training loss 39.046 updated ! and save the model! (step:150354) ----------
iteration : 150400 loss : 55.260 NLL : -54.356 KLD : 0.904 
iteration : 150600 loss : 52.794 NLL : -52.636 KLD : 0.158 
iteration : 150800 loss : 31.201 NLL : -30.298 KLD : 0.903 
iteration : 151000 loss : 53.680 NLL : -53.307 KLD : 0.373 
---------- Training loss 38.793 updated ! and save the model! (step:151164) ----------
iteration : 151200 loss : 49.575 NLL : -49.311 KLD : 0.264 
---------- Training loss 38.340 updated ! and save the model! (step:151218) ----------
---------- Training loss 38.041 updated ! and save the model! (step:151242) ----------
iteration : 151400 loss : 46.749 NLL : -46.618 KLD : 0.131 
iteration : 151600 loss : 54.475 NLL : -53.235 KLD : 1.240 
iteration : 151800 loss : 46.558 NLL : -45.856 KLD : 0.702 
iteration : 152000 loss : 33.624 NLL : -33.407 KLD : 0.217 
iteration : 152200 loss : 52.910 NLL : -46.404 KLD : 6.506 
---------- Training loss 37.523 updated ! and save the model! (step:152250) ----------
iteration : 152400 loss : 37.881 NLL : -35.024 KLD : 2.857 
iteration : 152600 loss : 22.724 NLL : -21.214 KLD : 1.510 
iteration : 152800 loss : 41.743 NLL : -41.006 KLD : 0.737 
---------- Training loss 35.820 updated ! and save the model! (step:152940) ----------
iteration : 153000 loss : 49.887 NLL : -47.881 KLD : 2.006 
iteration : 153200 loss : 48.738 NLL : -48.495 KLD : 0.243 
iteration : 153400 loss : 37.363 NLL : -36.674 KLD : 0.689 
iteration : 153600 loss : 66.346 NLL : -63.221 KLD : 3.126 
---------- Training loss 35.546 updated ! and save the model! (step:153684) ----------
---------- Training loss 33.298 updated ! and save the model! (step:153774) ----------
iteration : 153800 loss : 23.284 NLL : -22.236 KLD : 1.048 
iteration : 154000 loss : 52.117 NLL : -48.273 KLD : 3.844 
iteration : 154200 loss : 49.906 NLL : -48.362 KLD : 1.544 
iteration : 154400 loss : 44.114 NLL : -43.052 KLD : 1.063 
iteration : 154600 loss : 46.885 NLL : -46.602 KLD : 0.283 
iteration : 154800 loss : 52.786 NLL : -52.115 KLD : 0.672 
iteration : 155000 loss : 54.411 NLL : -54.146 KLD : 0.265 
iteration : 155200 loss : 60.980 NLL : -60.385 KLD : 0.595 
iteration : 155400 loss : 49.950 NLL : -49.669 KLD : 0.281 
iteration : 155600 loss : 70.872 NLL : -70.044 KLD : 0.827 
iteration : 155800 loss : 116.943 NLL : -75.773 KLD : 41.171 
iteration : 156000 loss : 53.145 NLL : -52.755 KLD : 0.390 
iteration : 156200 loss : 60.166 NLL : -59.065 KLD : 1.101 
iteration : 156400 loss : 48.942 NLL : -48.784 KLD : 0.158 
iteration : 156600 loss : 46.734 NLL : -46.392 KLD : 0.342 
iteration : 156800 loss : 42.704 NLL : -42.459 KLD : 0.245 
iteration : 157000 loss : 48.011 NLL : -47.671 KLD : 0.340 
iteration : 157200 loss : 58.661 NLL : -56.727 KLD : 1.934 
iteration : 157400 loss : 53.938 NLL : -53.720 KLD : 0.218 
iteration : 157600 loss : 59.832 NLL : -57.918 KLD : 1.913 
iteration : 157800 loss : 60.879 NLL : -56.509 KLD : 4.370 
iteration : 158000 loss : 39.946 NLL : -39.754 KLD : 0.192 
iteration : 158200 loss : 27.646 NLL : -26.653 KLD : 0.993 
iteration : 158400 loss : 48.691 NLL : -48.555 KLD : 0.136 
iteration : 158600 loss : 60.559 NLL : -59.526 KLD : 1.033 
iteration : 158800 loss : 41.804 NLL : -41.590 KLD : 0.214 
iteration : 159000 loss : 58.231 NLL : -56.815 KLD : 1.416 
iteration : 159200 loss : 39.556 NLL : -39.209 KLD : 0.348 
iteration : 159400 loss : 35.564 NLL : -34.778 KLD : 0.786 
iteration : 159600 loss : 45.503 NLL : -45.231 KLD : 0.272 
iteration : 159800 loss : 37.241 NLL : -36.774 KLD : 0.467 
iteration : 160000 loss : 57.772 NLL : -57.019 KLD : 0.752 
iteration : 160200 loss : 50.501 NLL : -50.351 KLD : 0.150 
iteration : 160400 loss : 39.595 NLL : -39.227 KLD : 0.367 
iteration : 160600 loss : 24.604 NLL : -24.142 KLD : 0.462 
iteration : 160800 loss : 27.106 NLL : -26.581 KLD : 0.525 
iteration : 161000 loss : 61.882 NLL : -61.741 KLD : 0.141 
iteration : 161200 loss : 52.758 NLL : -49.176 KLD : 3.582 
iteration : 161400 loss : 42.233 NLL : -42.073 KLD : 0.160 
iteration : 161600 loss : 47.296 NLL : -46.799 KLD : 0.498 
---------- Training loss 33.205 updated ! and save the model! (step:161616) ----------
iteration : 161800 loss : 55.765 NLL : -55.677 KLD : 0.088 
iteration : 162000 loss : 43.215 NLL : -42.417 KLD : 0.798 
iteration : 162200 loss : 48.633 NLL : -48.405 KLD : 0.228 
iteration : 162400 loss : 34.945 NLL : -32.128 KLD : 2.817 
iteration : 162600 loss : 41.561 NLL : -40.785 KLD : 0.776 
iteration : 162800 loss : 39.344 NLL : -38.858 KLD : 0.486 
iteration : 163000 loss : 27.026 NLL : -24.011 KLD : 3.015 
iteration : 163200 loss : 46.361 NLL : -46.079 KLD : 0.282 
iteration : 163400 loss : 47.916 NLL : -47.219 KLD : 0.696 
iteration : 163600 loss : 34.306 NLL : -32.472 KLD : 1.833 
iteration : 163800 loss : 43.982 NLL : -43.838 KLD : 0.144 
iteration : 164000 loss : 53.575 NLL : -52.699 KLD : 0.877 
iteration : 164200 loss : 46.589 NLL : -46.152 KLD : 0.437 
iteration : 164400 loss : 69.165 NLL : -44.409 KLD : 24.756 
iteration : 164600 loss : 38.771 NLL : -38.488 KLD : 0.283 
iteration : 164800 loss : 55.287 NLL : -55.066 KLD : 0.221 
iteration : 165000 loss : 46.060 NLL : -45.838 KLD : 0.222 
iteration : 165200 loss : 44.771 NLL : -44.162 KLD : 0.610 
iteration : 165400 loss : 38.992 NLL : -38.108 KLD : 0.884 
iteration : 165600 loss : 51.677 NLL : -51.259 KLD : 0.418 
iteration : 165800 loss : 38.520 NLL : -37.110 KLD : 1.411 
iteration : 166000 loss : 69.713 NLL : -69.280 KLD : 0.432 
iteration : 166200 loss : 55.983 NLL : -53.178 KLD : 2.805 
iteration : 166400 loss : 37.094 NLL : -36.677 KLD : 0.417 
iteration : 166600 loss : 53.419 NLL : -52.096 KLD : 1.324 
iteration : 166800 loss : 42.690 NLL : -42.388 KLD : 0.302 
iteration : 167000 loss : 64.671 NLL : -64.231 KLD : 0.440 
iteration : 167200 loss : 46.768 NLL : -46.042 KLD : 0.726 
iteration : 167400 loss : 46.605 NLL : -46.507 KLD : 0.098 
iteration : 167600 loss : 38.535 NLL : -38.422 KLD : 0.113 
iteration : 167800 loss : 55.568 NLL : -54.677 KLD : 0.891 
iteration : 168000 loss : 51.312 NLL : -51.221 KLD : 0.091 
iteration : 168200 loss : 30.739 NLL : -25.917 KLD : 4.823 
iteration : 168400 loss : 47.374 NLL : -46.110 KLD : 1.264 
iteration : 168600 loss : 36.015 NLL : -33.842 KLD : 2.173 
iteration : 168800 loss : 56.811 NLL : -56.568 KLD : 0.243 
iteration : 169000 loss : 35.988 NLL : -35.052 KLD : 0.936 
iteration : 169200 loss : 30.077 NLL : -28.006 KLD : 2.071 
---------- Training loss 31.422 updated ! and save the model! (step:169200) ----------
iteration : 169400 loss : 46.426 NLL : -46.194 KLD : 0.232 
iteration : 169600 loss : 38.990 NLL : -38.596 KLD : 0.394 
iteration : 169800 loss : 38.472 NLL : -37.365 KLD : 1.107 
iteration : 170000 loss : 57.875 NLL : -56.991 KLD : 0.884 
iteration : 170200 loss : 45.094 NLL : -43.265 KLD : 1.829 
iteration : 170400 loss : 59.014 NLL : -55.686 KLD : 3.328 
iteration : 170600 loss : 30.311 NLL : -29.587 KLD : 0.724 
iteration : 170800 loss : 58.381 NLL : -56.038 KLD : 2.343 
iteration : 171000 loss : 45.052 NLL : -36.755 KLD : 8.297 
iteration : 171200 loss : 32.562 NLL : -32.270 KLD : 0.292 
iteration : 171400 loss : 47.567 NLL : -47.431 KLD : 0.136 
iteration : 171600 loss : 44.156 NLL : -43.847 KLD : 0.309 
iteration : 171800 loss : 36.960 NLL : -36.799 KLD : 0.161 
iteration : 172000 loss : 72.957 NLL : -72.525 KLD : 0.432 
iteration : 172200 loss : 50.839 NLL : -50.371 KLD : 0.468 
iteration : 172400 loss : 44.309 NLL : -44.153 KLD : 0.156 
iteration : 172600 loss : 53.438 NLL : -53.193 KLD : 0.244 
iteration : 172800 loss : 37.962 NLL : -37.017 KLD : 0.945 
iteration : 173000 loss : 47.288 NLL : -43.264 KLD : 4.025 
iteration : 173200 loss : 61.885 NLL : -46.470 KLD : 15.415 
iteration : 173400 loss : 48.867 NLL : -48.089 KLD : 0.778 
iteration : 173600 loss : 54.010 NLL : -53.616 KLD : 0.394 
iteration : 173800 loss : 51.253 NLL : -49.969 KLD : 1.284 
iteration : 174000 loss : 51.524 NLL : -49.564 KLD : 1.960 
iteration : 174200 loss : 38.729 NLL : -36.470 KLD : 2.259 
iteration : 174400 loss : 44.600 NLL : -41.026 KLD : 3.573 
iteration : 174600 loss : 49.874 NLL : -48.853 KLD : 1.021 
iteration : 174800 loss : 58.699 NLL : -57.983 KLD : 0.716 
iteration : 175000 loss : 76.133 NLL : -74.779 KLD : 1.354 
iteration : 175200 loss : 56.042 NLL : -55.835 KLD : 0.207 
iteration : 175400 loss : 59.812 NLL : -58.218 KLD : 1.594 
iteration : 175600 loss : 36.230 NLL : -36.000 KLD : 0.230 
iteration : 175800 loss : 30.660 NLL : -28.061 KLD : 2.599 
iteration : 176000 loss : 48.100 NLL : -47.779 KLD : 0.321 
iteration : 176200 loss : 26.267 NLL : -24.172 KLD : 2.095 
iteration : 176400 loss : 54.321 NLL : -53.377 KLD : 0.945 
iteration : 176600 loss : 56.485 NLL : -55.500 KLD : 0.985 
iteration : 176800 loss : 61.098 NLL : -53.465 KLD : 7.633 
iteration : 177000 loss : 61.639 NLL : -56.597 KLD : 5.042 
iteration : 177200 loss : 87.833 NLL : -76.767 KLD : 11.066 
iteration : 177400 loss : 96.824 NLL : -75.135 KLD : 21.689 
iteration : 177600 loss : 41.043 NLL : -39.716 KLD : 1.327 
iteration : 177800 loss : 93.492 NLL : -92.188 KLD : 1.304 
iteration : 178000 loss : 54.344 NLL : -54.025 KLD : 0.319 
iteration : 178200 loss : 55.012 NLL : -54.625 KLD : 0.387 
iteration : 178400 loss : 52.731 NLL : -50.749 KLD : 1.982 
iteration : 178600 loss : 66.565 NLL : -65.891 KLD : 0.674 
iteration : 178800 loss : 59.878 NLL : -59.319 KLD : 0.559 
iteration : 179000 loss : 59.544 NLL : -59.054 KLD : 0.490 
iteration : 179200 loss : 71.933 NLL : -66.770 KLD : 5.164 
iteration : 179400 loss : 60.088 NLL : -59.545 KLD : 0.543 
iteration : 179600 loss : 63.535 NLL : -62.901 KLD : 0.634 
iteration : 179800 loss : 68.173 NLL : -67.080 KLD : 1.093 
iteration : 180000 loss : 54.470 NLL : -53.180 KLD : 1.290 
---------- Training loss 45.714 updated ! and save the model! (step:180000) ----------
---------- Training loss 45.435 updated ! and save the model! (step:180012) ----------
---------- Training loss 43.180 updated ! and save the model! (step:180102) ----------
---------- Training loss 42.075 updated ! and save the model! (step:180186) ----------
iteration : 180200 loss : 45.292 NLL : -44.760 KLD : 0.531 
---------- Training loss 40.911 updated ! and save the model! (step:180360) ----------
---------- Training loss 40.101 updated ! and save the model! (step:180378) ----------
iteration : 180400 loss : 64.269 NLL : -63.753 KLD : 0.516 
---------- Training loss 38.205 updated ! and save the model! (step:180426) ----------
---------- Training loss 36.798 updated ! and save the model! (step:180534) ----------
iteration : 180600 loss : 49.514 NLL : -49.356 KLD : 0.157 
iteration : 180800 loss : 63.809 NLL : -63.052 KLD : 0.757 
iteration : 181000 loss : 84.132 NLL : -81.764 KLD : 2.368 
---------- Training loss 36.784 updated ! and save the model! (step:181164) ----------
iteration : 181200 loss : 179.773 NLL : -178.088 KLD : 1.685 
iteration : 181400 loss : 45.903 NLL : -45.356 KLD : 0.547 
iteration : 181600 loss : 41.098 NLL : -40.444 KLD : 0.654 
iteration : 181800 loss : 38.671 NLL : -37.951 KLD : 0.721 
iteration : 182000 loss : 48.897 NLL : -48.055 KLD : 0.843 
---------- Training loss 35.140 updated ! and save the model! (step:182130) ----------
iteration : 182200 loss : 22.164 NLL : -20.386 KLD : 1.779 
iteration : 182400 loss : 48.014 NLL : -46.675 KLD : 1.338 
iteration : 182600 loss : 56.263 NLL : -55.979 KLD : 0.284 
iteration : 182800 loss : 46.240 NLL : -46.106 KLD : 0.135 
iteration : 183000 loss : 48.144 NLL : -47.509 KLD : 0.635 
iteration : 183200 loss : 39.568 NLL : -38.835 KLD : 0.733 
---------- Training loss 34.399 updated ! and save the model! (step:183378) ----------
iteration : 183400 loss : 60.042 NLL : -59.757 KLD : 0.285 
iteration : 183600 loss : 60.159 NLL : -57.702 KLD : 2.457 
iteration : 183800 loss : 57.356 NLL : -56.002 KLD : 1.354 
iteration : 184000 loss : 58.532 NLL : -58.216 KLD : 0.317 
iteration : 184200 loss : 49.541 NLL : -49.296 KLD : 0.245 
iteration : 184400 loss : 25.273 NLL : -23.083 KLD : 2.189 
iteration : 184600 loss : 54.194 NLL : -53.872 KLD : 0.323 
iteration : 184800 loss : 59.839 NLL : -59.687 KLD : 0.152 
iteration : 185000 loss : 50.706 NLL : -50.269 KLD : 0.436 
iteration : 185200 loss : 46.253 NLL : -45.815 KLD : 0.438 
iteration : 185400 loss : 51.846 NLL : -51.011 KLD : 0.835 
iteration : 185600 loss : 56.410 NLL : -55.958 KLD : 0.452 
iteration : 185800 loss : 52.455 NLL : -52.097 KLD : 0.358 
iteration : 186000 loss : 56.378 NLL : -55.379 KLD : 0.999 
iteration : 186200 loss : 42.650 NLL : -42.472 KLD : 0.178 
iteration : 186400 loss : 60.994 NLL : -55.447 KLD : 5.547 
iteration : 186600 loss : 37.746 NLL : -36.758 KLD : 0.989 
iteration : 186800 loss : 43.435 NLL : -43.081 KLD : 0.354 
iteration : 187000 loss : 39.145 NLL : -38.820 KLD : 0.326 
iteration : 187200 loss : 46.551 NLL : -46.418 KLD : 0.133 
iteration : 187400 loss : 49.530 NLL : -47.269 KLD : 2.261 
iteration : 187600 loss : 55.380 NLL : -55.228 KLD : 0.152 
iteration : 187800 loss : 43.297 NLL : -41.844 KLD : 1.453 
iteration : 188000 loss : 44.240 NLL : -43.873 KLD : 0.367 
---------- Training loss 32.884 updated ! and save the model! (step:188028) ----------
iteration : 188200 loss : 40.560 NLL : -40.211 KLD : 0.349 
iteration : 188400 loss : 47.095 NLL : -45.985 KLD : 1.110 
iteration : 188600 loss : 35.526 NLL : -34.592 KLD : 0.933 
---------- Training loss 32.675 updated ! and save the model! (step:188796) ----------
iteration : 188800 loss : 45.297 NLL : -45.161 KLD : 0.136 
iteration : 189000 loss : 61.709 NLL : -61.342 KLD : 0.368 
iteration : 189200 loss : 51.642 NLL : -51.466 KLD : 0.176 
iteration : 189400 loss : 40.363 NLL : -39.285 KLD : 1.078 
iteration : 189600 loss : 57.105 NLL : -56.519 KLD : 0.586 
iteration : 189800 loss : 44.386 NLL : -42.771 KLD : 1.615 
iteration : 190000 loss : 44.652 NLL : -44.371 KLD : 0.281 
iteration : 190200 loss : 55.483 NLL : -54.064 KLD : 1.419 
iteration : 190400 loss : 51.841 NLL : -51.583 KLD : 0.259 
iteration : 190600 loss : 66.349 NLL : -64.498 KLD : 1.852 
iteration : 190800 loss : 57.818 NLL : -56.409 KLD : 1.409 
iteration : 191000 loss : 48.559 NLL : -47.592 KLD : 0.968 
iteration : 191200 loss : 27.601 NLL : -27.128 KLD : 0.473 
iteration : 191400 loss : 45.799 NLL : -45.521 KLD : 0.278 
iteration : 191600 loss : 49.722 NLL : -49.179 KLD : 0.543 
iteration : 191800 loss : 43.932 NLL : -43.655 KLD : 0.278 
iteration : 192000 loss : 33.674 NLL : -32.421 KLD : 1.252 
iteration : 192200 loss : 59.874 NLL : -59.564 KLD : 0.310 
iteration : 192400 loss : 59.773 NLL : -55.906 KLD : 3.866 
iteration : 192600 loss : 47.313 NLL : -47.171 KLD : 0.142 
iteration : 192800 loss : 59.585 NLL : -58.238 KLD : 1.348 
iteration : 193000 loss : 53.830 NLL : -53.622 KLD : 0.207 
iteration : 193200 loss : 35.855 NLL : -35.246 KLD : 0.609 
iteration : 193400 loss : 58.190 NLL : -57.612 KLD : 0.579 
iteration : 193600 loss : 48.552 NLL : -48.236 KLD : 0.316 
iteration : 193800 loss : 55.398 NLL : -53.836 KLD : 1.562 
iteration : 194000 loss : 52.069 NLL : -51.316 KLD : 0.753 
iteration : 194200 loss : 98.284 NLL : -90.752 KLD : 7.532 
iteration : 194400 loss : 50.976 NLL : -49.414 KLD : 1.562 
iteration : 194600 loss : 42.901 NLL : -42.739 KLD : 0.162 
iteration : 194800 loss : 56.527 NLL : -56.099 KLD : 0.428 
iteration : 195000 loss : 52.568 NLL : -51.580 KLD : 0.987 
iteration : 195200 loss : 63.284 NLL : -63.021 KLD : 0.262 
iteration : 195400 loss : 46.037 NLL : -45.774 KLD : 0.263 
iteration : 195600 loss : 55.590 NLL : -55.038 KLD : 0.552 
iteration : 195800 loss : 38.279 NLL : -37.791 KLD : 0.488 
iteration : 196000 loss : 44.439 NLL : -43.942 KLD : 0.497 
iteration : 196200 loss : 52.739 NLL : -52.475 KLD : 0.264 
iteration : 196400 loss : 56.486 NLL : -56.203 KLD : 0.283 
iteration : 196600 loss : 54.185 NLL : -54.062 KLD : 0.124 
iteration : 196800 loss : 52.838 NLL : -52.327 KLD : 0.511 
iteration : 197000 loss : 26.848 NLL : -26.345 KLD : 0.503 
iteration : 197200 loss : 67.071 NLL : -64.698 KLD : 2.373 
iteration : 197400 loss : 33.209 NLL : -32.124 KLD : 1.085 
iteration : 197600 loss : 40.779 NLL : -39.096 KLD : 1.683 
iteration : 197800 loss : 53.246 NLL : -52.716 KLD : 0.530 
iteration : 198000 loss : 47.031 NLL : -46.915 KLD : 0.116 
iteration : 198200 loss : 44.952 NLL : -44.780 KLD : 0.172 
iteration : 198400 loss : 53.625 NLL : -51.802 KLD : 1.823 
iteration : 198600 loss : 55.751 NLL : -55.390 KLD : 0.362 
iteration : 198800 loss : 51.436 NLL : -51.201 KLD : 0.235 
iteration : 199000 loss : 29.206 NLL : -28.467 KLD : 0.739 
iteration : 199200 loss : 51.557 NLL : -51.131 KLD : 0.426 
iteration : 199400 loss : 49.617 NLL : -36.554 KLD : 13.064 
iteration : 199600 loss : 58.483 NLL : -58.118 KLD : 0.365 
iteration : 199800 loss : 24.674 NLL : -24.234 KLD : 0.440 
iteration : 200000 loss : 24.022 NLL : -22.818 KLD : 1.205 
---------- Save the model! (step:None) ----------
